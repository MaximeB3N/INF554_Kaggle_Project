{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory : d:\\Documents\\Info\\INF554\\INF554_Kaggle_Project\\Exploration, Project directory : d:\\Documents\\Info\\INF554\\INF554_Kaggle_Project\n"
     ]
    }
   ],
   "source": [
    "project_path = str(Path(os.getcwd()).parent.absolute())\n",
    "print(\"Current directory : \" + os.getcwd() + \", Project directory : \" + project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 217801\n",
      "Number of edges: 1718164\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read training data\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]\n",
    "\n",
    "# load the graph    \n",
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges() \n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)\n",
    "\n",
    "\n",
    "# computes structural features for each node\n",
    "core_number = nx.core_number(G)\n",
    "\n",
    "# create the training matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number \n",
    "X_train = np.zeros((n_train, 2))\n",
    "y_train = np.zeros(n_train)\n",
    "for i,row in df_train.iterrows():\n",
    "    node = row['author']\n",
    "    X_train[i,0] = G.degree(node)\n",
    "    X_train[i,1] = core_number[node]\n",
    "    y_train[i] = row['hindex']\n",
    "\n",
    "# create the test matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number\n",
    "X_test = np.zeros((n_test, 2))\n",
    "for i,row in df_test.iterrows():\n",
    "    node = row['author']\n",
    "    X_test[i,0] = G.degree(node)\n",
    "    X_test[i,1] = core_number[node]\n",
    "    \n",
    "# train a regression model and make predictions\n",
    "reg = Lasso(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# write the predictions to file\n",
    "df_test['hindex'] = pd.Series(np.round_(y_pred, decimals=3))\n",
    "\n",
    "\n",
    "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(G, node_size=10)\n",
    "plt.title(\"Raw graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/abstracts.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11092/457375096.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/abstracts.txt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/abstracts.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"data/abstracts.txt\") as file:\n",
    "    texts = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624181"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3603----{\"IndexLength\":122,\"InvertedIndex\":{\"In\":[0],\"this\":[1],\"paper,\":[2],\"we\":[3,98],\"describe\":[4],\"a\":[5,16,41,58,62,67,74,108],\"new\":[6,17],\"bitmap\":[7,42,54,69,76],\"indexing\":[8,43],\"technique\":[9],\"to\":[10,73],\"cluster\":[11],\"XML\":[12,14,34,63],\"documents.\":[13],\"is\":[15,71],\"standard\":[18,89],\"for\":[19,60],\"exchanging\":[20],\"and\":[21,38,49,56,91,104,115],\"representing\":[22],\"information\":[23],\"on\":[24,95,107],\"the\":[25,47,85,101,117],\"Internet.\":[26],\"Documents\":[27],\"can\":[28,111],\"be\":[29,112],\"hierarchically\":[30],\"represented\":[31,37],\"by\":[32],\"XML-elements.\":[33],\"documents\":[35],\"are\":[36],\"indexed\":[39],\"using\":[40],\"technique.\":[44],\"We\":[45,80],\"define\":[46,81,100],\"similarity\":[48],\"popularity\":[50],\"operations\":[51,106],\"available\":[52],\"in\":[53,84],\"indexes\":[55],\"propose\":[57],\"method\":[59],\"partitioning\":[61],\"document\":[64,120],\"set.\":[65],\"Furthermore,\":[66],\"2-dimensional\":[68],\"index\":[70],\"extended\":[72],\"3dimensional\":[75],\"index,\":[77],\"called\":[78],\"BitCube.\":[79,109],\"statistical\":[82],\"measurements\":[83],\"BitCube:\":[86],\"mean,\":[87],\"mode,\":[88],\"derivation,\":[90],\"correlation\":[92],\"coefficient.\":[93],\"Based\":[94],\"these\":[96],\"measurements,\":[97],\"also\":[99],\"slice,\":[102],\"project,\":[103],\"dice\":[105],\"BitCube\":[110],\"manipulated\":[113],\"efficiently\":[114],\"improves\":[116],\"performance\":[118],\"of\":[119],\"retrieval.\":[121]}}\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_str = texts[0]\n",
    "print(type(file_str))\n",
    "file_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(file_str):\n",
    "    return int(file_str.split(\"----\")[0])\n",
    "\n",
    "def get_json(file_str):\n",
    "    new_file_str = file_str.replace(\"-\",\"\")\n",
    "    json_str = new_file_str.split(str(get_id(file_str)))[-1]\n",
    "\n",
    "    return json.loads(json_str)\n",
    "\n",
    "def get_descritpion(file_str):\n",
    "\n",
    "    json_file = get_json(file_str)\n",
    "    words = [\"\"]*int(json_file[\"IndexLength\"])\n",
    "\n",
    "    for word in json_file[\"InvertedIndex\"].keys():\n",
    "        indexes = json_file[\"InvertedIndex\"][word]\n",
    "\n",
    "        for index in indexes:\n",
    "            words[int(index)] = word\n",
    "        \n",
    "        \n",
    "    return words #' '.join(words).replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def get_line(file_str):\n",
    "    \n",
    "    words = get_descritpion(file_str)\n",
    "\n",
    "    return ' '.join(words).replace(\"\\n\", \" \")\n",
    "\n",
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in tqdm(list_of_docs):\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = word_tokenize\n",
    "stpwds = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9162d927d94f24898cf6bca676d7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[ get_id(file_str), get_line(file_str)] for i,file_str in tqdm(enumerate(texts))]#[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, columns=[\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] =  df[\"text\"].map(lambda x: clean_text(x, tokenizer, stpwds))\n",
    "tokenized_docs = df[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [paper, describe, new, bitmap, indexing, techn...\n",
       "1         [paper, starts, observation, inclusionbased, a...\n",
       "2         [contribution, describes, approach, integrate,...\n",
       "3         [cleaneval, shared, task, competitive, evaluat...\n",
       "4         [xax, browser, plugin, model, enables, develop...\n",
       "                                ...                        \n",
       "624176    [xray, polarimetry, sometimes, alone, sometime...\n",
       "624177    [recent, years, underwater, wireless, sensor, ...\n",
       "624178    [todays, cyber, physical, systems, cps, well, ...\n",
       "624179    [software, service, cloud, computing, model, f...\n",
       "624180    [penetration, testing, wellestablished, practi...\n",
       "Name: tokens, Length: 624181, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.6594e-01,  6.6349e-01,  2.6309e-01, -5.4793e-01,  1.0673e+00,\n",
       "       -9.0648e-02, -1.8309e-01,  4.4575e-01, -4.4378e-01,  2.6572e-01,\n",
       "       -6.3668e-02,  3.0498e-01, -4.1506e-01, -2.9739e-01, -1.8309e-01,\n",
       "        8.4494e-02,  3.8472e-01, -9.2856e-01,  4.4563e-01,  3.2183e-01,\n",
       "        3.8717e-01,  1.6766e+00,  3.8729e-01,  3.9076e-01,  6.2466e-01,\n",
       "        1.4462e-01, -6.5783e-02, -4.3507e-01,  8.1413e-01,  5.2090e-02,\n",
       "        2.5516e-01, -3.9658e-02, -5.0672e-01, -2.9341e-01,  2.3707e-01,\n",
       "        1.2230e-01, -1.1576e-01, -2.7695e-01,  4.1394e-01,  4.3311e-02,\n",
       "        4.8973e-01, -4.4549e-01, -6.1259e-02,  1.9547e-02,  5.2209e-01,\n",
       "        9.1658e-01,  3.5733e-01, -4.9494e-01, -9.8224e-01,  2.0034e-01,\n",
       "       -3.0868e-02, -1.7801e-02, -2.3831e-01,  2.8774e-01,  7.4100e-02,\n",
       "        1.5456e-01,  3.3661e-01,  3.4376e-01, -5.9647e-01, -9.4255e-02,\n",
       "       -3.4176e-01,  2.5309e-02, -2.3511e-02,  1.2011e+00, -2.1605e-01,\n",
       "       -4.8390e-01,  4.3511e-01,  5.4461e-01,  1.2633e+00,  2.5917e-01,\n",
       "       -1.1421e-01,  5.3720e-01, -5.4331e-01, -3.6742e-01, -5.7027e-01,\n",
       "       -1.7175e-01, -6.8733e-01,  9.8460e-01,  2.4515e-01, -2.2371e-01,\n",
       "        9.7174e-01, -4.1807e-02, -1.1784e+00,  1.3977e-01,  4.6078e-01,\n",
       "       -1.9632e-01, -7.4474e-01,  2.5423e-02,  8.0462e-01, -3.3450e-01,\n",
       "       -2.1865e-01, -6.5424e-01,  2.1113e-01,  1.8232e-01,  4.2931e-01,\n",
       "       -8.7038e-01,  4.1972e-01, -4.1389e-01,  7.1248e-01,  2.2949e-01,\n",
       "       -1.2032e-01, -8.5783e-01, -8.0654e-01,  8.9839e-01,  4.8607e-01,\n",
       "        1.3298e-01, -1.2310e+00,  3.1057e-01, -5.6629e-02,  5.4886e-01,\n",
       "        6.1032e-01,  7.4684e-01,  3.4069e-01, -2.3033e-02, -1.2998e+00,\n",
       "        1.6497e-01, -1.8836e-01, -1.3293e-01, -2.5130e-01, -1.8927e-01,\n",
       "       -1.5952e-01,  3.7192e-04, -9.2012e-01,  3.6893e-02, -5.1207e-01,\n",
       "       -3.4395e-01, -2.7056e-01, -8.3500e-02,  3.0654e-01, -1.5095e-01,\n",
       "       -4.2247e-02, -8.6109e-01, -7.7691e-01,  1.9575e-01, -2.8952e-01,\n",
       "        3.4186e-01, -1.3374e-01,  4.6298e-01,  1.9810e-01,  6.7440e-02,\n",
       "       -2.0761e-01, -3.5577e-02, -2.9432e-01, -2.1170e-01,  3.6431e-01,\n",
       "        8.5816e-03, -1.0175e-02, -1.6175e-01, -2.2964e-01, -6.4495e-01,\n",
       "        9.1279e-01, -9.2916e-02, -1.1923e-01, -2.3787e-01,  1.5188e-01,\n",
       "       -2.8293e-02, -1.1462e-01, -8.6337e-02,  4.8884e-01, -2.5865e-01,\n",
       "        4.4624e-01, -4.1583e-01,  1.3673e-01,  2.5084e-02, -1.7666e-01,\n",
       "       -7.1640e-02, -3.2530e-01, -4.0709e-02, -8.5925e-01, -6.6533e-02,\n",
       "        3.9017e-01,  9.4013e-02,  2.3947e-01,  3.3033e-01,  1.0883e+00,\n",
       "       -1.3063e-01, -7.3562e-01,  4.3786e-01,  2.6336e-01,  1.0088e+00,\n",
       "        1.9219e-01,  5.0724e-01, -1.6973e-01,  9.2276e-01, -1.7674e-02,\n",
       "       -3.0621e-01, -1.5001e-01, -6.2389e-01, -1.6403e-01, -5.3258e-01,\n",
       "        2.1697e-01,  1.2172e-01,  3.5392e-01,  5.4564e-01,  1.8090e-01,\n",
       "       -6.6867e-01,  4.1631e-01,  1.8434e-01,  2.0702e-01, -4.7242e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector(\"math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c0d0>,\n",
       " ',': <gensim.models.keyedvectors.Vocab at 0x7fb767b67e10>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c110>,\n",
       " 'of': <gensim.models.keyedvectors.Vocab at 0x7fb767b67e90>,\n",
       " 'to': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c190>,\n",
       " 'and': <gensim.models.keyedvectors.Vocab at 0x7fb767b67dd0>,\n",
       " 'in': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c250>,\n",
       " 'a': <gensim.models.keyedvectors.Vocab at 0x7fb781ca8450>,\n",
       " '\"': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c290>,\n",
       " \"'s\": <gensim.models.keyedvectors.Vocab at 0x7fb781ca8a90>,\n",
       " 'for': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c2d0>,\n",
       " '-': <gensim.models.keyedvectors.Vocab at 0x7fb781ca8490>,\n",
       " 'that': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c310>,\n",
       " 'on': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c090>,\n",
       " 'is': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c390>,\n",
       " 'was': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d410>,\n",
       " 'said': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c410>,\n",
       " 'with': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d490>,\n",
       " 'he': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c490>,\n",
       " 'as': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d510>,\n",
       " 'it': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c510>,\n",
       " 'by': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d590>,\n",
       " 'at': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c590>,\n",
       " '(': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d610>,\n",
       " ')': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c5d0>,\n",
       " 'from': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d650>,\n",
       " 'his': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c650>,\n",
       " \"''\": <gensim.models.keyedvectors.Vocab at 0x7fb744d2d6d0>,\n",
       " '``': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c6d0>,\n",
       " 'an': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d750>,\n",
       " 'be': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c750>,\n",
       " 'has': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d7d0>,\n",
       " 'are': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c7d0>,\n",
       " 'have': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d850>,\n",
       " 'but': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c850>,\n",
       " 'were': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d8d0>,\n",
       " 'not': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c8d0>,\n",
       " 'this': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d950>,\n",
       " 'who': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c950>,\n",
       " 'they': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d9d0>,\n",
       " 'had': <gensim.models.keyedvectors.Vocab at 0x7fb767b7c9d0>,\n",
       " 'i': <gensim.models.keyedvectors.Vocab at 0x7fb744d2da50>,\n",
       " 'which': <gensim.models.keyedvectors.Vocab at 0x7fb767b7ca10>,\n",
       " 'will': <gensim.models.keyedvectors.Vocab at 0x7fb744d2dad0>,\n",
       " 'their': <gensim.models.keyedvectors.Vocab at 0x7fb767b7ca90>,\n",
       " ':': <gensim.models.keyedvectors.Vocab at 0x7fb744d2db50>,\n",
       " 'or': <gensim.models.keyedvectors.Vocab at 0x7fb767b7cad0>,\n",
       " 'its': <gensim.models.keyedvectors.Vocab at 0x7fb744d2dbd0>,\n",
       " 'one': <gensim.models.keyedvectors.Vocab at 0x7fb767b7cb50>,\n",
       " 'after': <gensim.models.keyedvectors.Vocab at 0x7fb744d2dc50>,\n",
       " 'new': <gensim.models.keyedvectors.Vocab at 0x7fb767b7cbd0>,\n",
       " 'been': <gensim.models.keyedvectors.Vocab at 0x7fb744d2dcd0>,\n",
       " 'also': <gensim.models.keyedvectors.Vocab at 0x7fb767b7cc50>,\n",
       " 'we': <gensim.models.keyedvectors.Vocab at 0x7fb744d2dd50>,\n",
       " 'would': <gensim.models.keyedvectors.Vocab at 0x7fb767b7ccd0>,\n",
       " 'two': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ddd0>,\n",
       " 'more': <gensim.models.keyedvectors.Vocab at 0x7fb767b7cd50>,\n",
       " \"'\": <gensim.models.keyedvectors.Vocab at 0x7fb744d2de50>,\n",
       " 'first': <gensim.models.keyedvectors.Vocab at 0x7fb767b7cd90>,\n",
       " 'about': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ded0>,\n",
       " 'up': <gensim.models.keyedvectors.Vocab at 0x7fb767b7ce10>,\n",
       " 'when': <gensim.models.keyedvectors.Vocab at 0x7fb744d2df50>,\n",
       " 'year': <gensim.models.keyedvectors.Vocab at 0x7fb767b7ce90>,\n",
       " 'there': <gensim.models.keyedvectors.Vocab at 0x7fb744d2dfd0>,\n",
       " 'all': <gensim.models.keyedvectors.Vocab at 0x7fb767b7cf10>,\n",
       " '--': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e090>,\n",
       " 'out': <gensim.models.keyedvectors.Vocab at 0x7fb767b7cf90>,\n",
       " 'she': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e110>,\n",
       " 'other': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d050>,\n",
       " 'people': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e190>,\n",
       " \"n't\": <gensim.models.keyedvectors.Vocab at 0x7fb744d2d0d0>,\n",
       " 'her': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e210>,\n",
       " 'percent': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d150>,\n",
       " 'than': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e290>,\n",
       " 'over': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d1d0>,\n",
       " 'into': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e310>,\n",
       " 'last': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d250>,\n",
       " 'some': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e390>,\n",
       " 'government': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d2d0>,\n",
       " 'time': <gensim.models.keyedvectors.Vocab at 0x7fb767b84490>,\n",
       " '$': <gensim.models.keyedvectors.Vocab at 0x7fb744d2d350>,\n",
       " 'you': <gensim.models.keyedvectors.Vocab at 0x7fb767b844d0>,\n",
       " 'years': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e410>,\n",
       " 'if': <gensim.models.keyedvectors.Vocab at 0x7fb767b84550>,\n",
       " 'no': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e490>,\n",
       " 'world': <gensim.models.keyedvectors.Vocab at 0x7fb767b845d0>,\n",
       " 'can': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e510>,\n",
       " 'three': <gensim.models.keyedvectors.Vocab at 0x7fb767b84650>,\n",
       " 'do': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e590>,\n",
       " ';': <gensim.models.keyedvectors.Vocab at 0x7fb767b846d0>,\n",
       " 'president': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e5d0>,\n",
       " 'only': <gensim.models.keyedvectors.Vocab at 0x7fb767b84750>,\n",
       " 'state': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e650>,\n",
       " 'million': <gensim.models.keyedvectors.Vocab at 0x7fb767b847d0>,\n",
       " 'could': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e6d0>,\n",
       " 'us': <gensim.models.keyedvectors.Vocab at 0x7fb767b84850>,\n",
       " 'most': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e750>,\n",
       " '_': <gensim.models.keyedvectors.Vocab at 0x7fb767b848d0>,\n",
       " 'against': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e790>,\n",
       " 'u.s.': <gensim.models.keyedvectors.Vocab at 0x7fb767b84950>,\n",
       " 'so': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e810>,\n",
       " 'them': <gensim.models.keyedvectors.Vocab at 0x7fb767b849d0>,\n",
       " 'what': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e890>,\n",
       " 'him': <gensim.models.keyedvectors.Vocab at 0x7fb767b84a50>,\n",
       " 'united': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e910>,\n",
       " 'during': <gensim.models.keyedvectors.Vocab at 0x7fb767b84ad0>,\n",
       " 'before': <gensim.models.keyedvectors.Vocab at 0x7fb744d2e990>,\n",
       " 'may': <gensim.models.keyedvectors.Vocab at 0x7fb767b84b50>,\n",
       " 'since': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ea10>,\n",
       " 'many': <gensim.models.keyedvectors.Vocab at 0x7fb767b84bd0>,\n",
       " 'while': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ea90>,\n",
       " 'where': <gensim.models.keyedvectors.Vocab at 0x7fb767b84c50>,\n",
       " 'states': <gensim.models.keyedvectors.Vocab at 0x7fb744d2eb10>,\n",
       " 'because': <gensim.models.keyedvectors.Vocab at 0x7fb767b84cd0>,\n",
       " 'now': <gensim.models.keyedvectors.Vocab at 0x7fb744d2eb90>,\n",
       " 'city': <gensim.models.keyedvectors.Vocab at 0x7fb767b84d50>,\n",
       " 'made': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ec10>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x7fb767b84dd0>,\n",
       " 'between': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ec90>,\n",
       " 'did': <gensim.models.keyedvectors.Vocab at 0x7fb767b84e50>,\n",
       " 'just': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ed10>,\n",
       " 'national': <gensim.models.keyedvectors.Vocab at 0x7fb767b84ed0>,\n",
       " 'day': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ed90>,\n",
       " 'country': <gensim.models.keyedvectors.Vocab at 0x7fb767b84f50>,\n",
       " 'under': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ee10>,\n",
       " 'such': <gensim.models.keyedvectors.Vocab at 0x7fb767b84fd0>,\n",
       " 'second': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ee90>,\n",
       " 'then': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d090>,\n",
       " 'company': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ef10>,\n",
       " 'group': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d110>,\n",
       " 'any': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ef90>,\n",
       " 'through': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d190>,\n",
       " 'china': <gensim.models.keyedvectors.Vocab at 0x7fb767b84050>,\n",
       " 'four': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d210>,\n",
       " 'being': <gensim.models.keyedvectors.Vocab at 0x7fb767b840d0>,\n",
       " 'down': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d290>,\n",
       " 'war': <gensim.models.keyedvectors.Vocab at 0x7fb767b84150>,\n",
       " 'back': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d310>,\n",
       " 'off': <gensim.models.keyedvectors.Vocab at 0x7fb767b841d0>,\n",
       " 'south': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d390>,\n",
       " 'american': <gensim.models.keyedvectors.Vocab at 0x7fb767b84250>,\n",
       " 'minister': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d410>,\n",
       " 'police': <gensim.models.keyedvectors.Vocab at 0x7fb767b842d0>,\n",
       " 'well': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d490>,\n",
       " 'including': <gensim.models.keyedvectors.Vocab at 0x7fb767b84350>,\n",
       " 'team': <gensim.models.keyedvectors.Vocab at 0x7fb767b855d0>,\n",
       " 'international': <gensim.models.keyedvectors.Vocab at 0x7fb767b843d0>,\n",
       " 'week': <gensim.models.keyedvectors.Vocab at 0x7fb767b85650>,\n",
       " 'officials': <gensim.models.keyedvectors.Vocab at 0x7fb767b84450>,\n",
       " 'still': <gensim.models.keyedvectors.Vocab at 0x7fb767b856d0>,\n",
       " 'both': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d550>,\n",
       " 'even': <gensim.models.keyedvectors.Vocab at 0x7fb767b85750>,\n",
       " 'high': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d5d0>,\n",
       " 'part': <gensim.models.keyedvectors.Vocab at 0x7fb767b857d0>,\n",
       " 'told': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d650>,\n",
       " 'those': <gensim.models.keyedvectors.Vocab at 0x7fb767b85850>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d6d0>,\n",
       " 'former': <gensim.models.keyedvectors.Vocab at 0x7fb767b858d0>,\n",
       " 'these': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d750>,\n",
       " 'make': <gensim.models.keyedvectors.Vocab at 0x7fb767b85950>,\n",
       " 'billion': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d7d0>,\n",
       " 'work': <gensim.models.keyedvectors.Vocab at 0x7fb767b859d0>,\n",
       " 'our': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d850>,\n",
       " 'home': <gensim.models.keyedvectors.Vocab at 0x7fb767b85a50>,\n",
       " 'school': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d8d0>,\n",
       " 'party': <gensim.models.keyedvectors.Vocab at 0x7fb767b85ad0>,\n",
       " 'house': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d950>,\n",
       " 'old': <gensim.models.keyedvectors.Vocab at 0x7fb767b85b50>,\n",
       " 'later': <gensim.models.keyedvectors.Vocab at 0x7fb767b7d9d0>,\n",
       " 'get': <gensim.models.keyedvectors.Vocab at 0x7fb767b85bd0>,\n",
       " 'another': <gensim.models.keyedvectors.Vocab at 0x7fb767b7da50>,\n",
       " 'tuesday': <gensim.models.keyedvectors.Vocab at 0x7fb767b85c50>,\n",
       " 'news': <gensim.models.keyedvectors.Vocab at 0x7fb767b7dad0>,\n",
       " 'long': <gensim.models.keyedvectors.Vocab at 0x7fb767b85cd0>,\n",
       " 'five': <gensim.models.keyedvectors.Vocab at 0x7fb767b7db50>,\n",
       " 'called': <gensim.models.keyedvectors.Vocab at 0x7fb767b85d50>,\n",
       " '1': <gensim.models.keyedvectors.Vocab at 0x7fb767b7dbd0>,\n",
       " 'wednesday': <gensim.models.keyedvectors.Vocab at 0x7fb767b85d90>,\n",
       " 'military': <gensim.models.keyedvectors.Vocab at 0x7fb767b7dc50>,\n",
       " 'way': <gensim.models.keyedvectors.Vocab at 0x7fb767b85e10>,\n",
       " 'used': <gensim.models.keyedvectors.Vocab at 0x7fb767b7dcd0>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x7fb767b85e90>,\n",
       " 'next': <gensim.models.keyedvectors.Vocab at 0x7fb767b7dd50>,\n",
       " 'monday': <gensim.models.keyedvectors.Vocab at 0x7fb767b85f10>,\n",
       " 'thursday': <gensim.models.keyedvectors.Vocab at 0x7fb767b7ddd0>,\n",
       " 'friday': <gensim.models.keyedvectors.Vocab at 0x7fb767b85f90>,\n",
       " 'game': <gensim.models.keyedvectors.Vocab at 0x7fb767b7de50>,\n",
       " 'here': <gensim.models.keyedvectors.Vocab at 0x7fb767b80050>,\n",
       " '?': <gensim.models.keyedvectors.Vocab at 0x7fb767b7ded0>,\n",
       " 'should': <gensim.models.keyedvectors.Vocab at 0x7fb767b80090>,\n",
       " 'take': <gensim.models.keyedvectors.Vocab at 0x7fb767b7df50>,\n",
       " 'very': <gensim.models.keyedvectors.Vocab at 0x7fb767b80110>,\n",
       " 'my': <gensim.models.keyedvectors.Vocab at 0x7fb767b7dfd0>,\n",
       " 'north': <gensim.models.keyedvectors.Vocab at 0x7fb767b80190>,\n",
       " 'security': <gensim.models.keyedvectors.Vocab at 0x7fb767b85090>,\n",
       " 'season': <gensim.models.keyedvectors.Vocab at 0x7fb767b80210>,\n",
       " 'york': <gensim.models.keyedvectors.Vocab at 0x7fb767b85110>,\n",
       " 'how': <gensim.models.keyedvectors.Vocab at 0x7fb767b80290>,\n",
       " 'public': <gensim.models.keyedvectors.Vocab at 0x7fb767b85190>,\n",
       " 'early': <gensim.models.keyedvectors.Vocab at 0x7fb767b80310>,\n",
       " 'according': <gensim.models.keyedvectors.Vocab at 0x7fb767b85210>,\n",
       " 'several': <gensim.models.keyedvectors.Vocab at 0x7fb767b80390>,\n",
       " 'court': <gensim.models.keyedvectors.Vocab at 0x7fb767b85290>,\n",
       " 'say': <gensim.models.keyedvectors.Vocab at 0x7fb767b80410>,\n",
       " 'around': <gensim.models.keyedvectors.Vocab at 0x7fb767b85310>,\n",
       " 'foreign': <gensim.models.keyedvectors.Vocab at 0x7fb767b80490>,\n",
       " '10': <gensim.models.keyedvectors.Vocab at 0x7fb767b85390>,\n",
       " 'until': <gensim.models.keyedvectors.Vocab at 0x7fb767b80510>,\n",
       " 'set': <gensim.models.keyedvectors.Vocab at 0x7fb767b85410>,\n",
       " 'political': <gensim.models.keyedvectors.Vocab at 0x7fb767b80590>,\n",
       " 'says': <gensim.models.keyedvectors.Vocab at 0x7fb767b85490>,\n",
       " 'market': <gensim.models.keyedvectors.Vocab at 0x7fb767b80610>,\n",
       " 'however': <gensim.models.keyedvectors.Vocab at 0x7fb767b85510>,\n",
       " 'family': <gensim.models.keyedvectors.Vocab at 0x7fb767b80710>,\n",
       " 'life': <gensim.models.keyedvectors.Vocab at 0x7fb767b85590>,\n",
       " 'same': <gensim.models.keyedvectors.Vocab at 0x7fb767b807d0>,\n",
       " 'general': <gensim.models.keyedvectors.Vocab at 0x7fb767b806d0>,\n",
       " '–': <gensim.models.keyedvectors.Vocab at 0x7fb767b808d0>,\n",
       " 'left': <gensim.models.keyedvectors.Vocab at 0x7fb767b80790>,\n",
       " 'good': <gensim.models.keyedvectors.Vocab at 0x7fb767b809d0>,\n",
       " 'top': <gensim.models.keyedvectors.Vocab at 0x7fb767b80890>,\n",
       " 'university': <gensim.models.keyedvectors.Vocab at 0x7fb767b80ad0>,\n",
       " 'going': <gensim.models.keyedvectors.Vocab at 0x7fb767b80990>,\n",
       " 'number': <gensim.models.keyedvectors.Vocab at 0x7fb767b80bd0>,\n",
       " 'major': <gensim.models.keyedvectors.Vocab at 0x7fb767b80a90>,\n",
       " 'known': <gensim.models.keyedvectors.Vocab at 0x7fb767b80cd0>,\n",
       " 'points': <gensim.models.keyedvectors.Vocab at 0x7fb767b80b90>,\n",
       " 'won': <gensim.models.keyedvectors.Vocab at 0x7fb767b80d90>,\n",
       " 'six': <gensim.models.keyedvectors.Vocab at 0x7fb767b80c90>,\n",
       " 'month': <gensim.models.keyedvectors.Vocab at 0x7fb767b80e90>,\n",
       " 'dollars': <gensim.models.keyedvectors.Vocab at 0x7fb767b80dd0>,\n",
       " 'bank': <gensim.models.keyedvectors.Vocab at 0x7fb767b80f90>,\n",
       " '2': <gensim.models.keyedvectors.Vocab at 0x7fb767b80ed0>,\n",
       " 'iraq': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f090>,\n",
       " 'use': <gensim.models.keyedvectors.Vocab at 0x7fb767b80fd0>,\n",
       " 'members': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f190>,\n",
       " 'each': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f110>,\n",
       " 'area': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f290>,\n",
       " 'found': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f210>,\n",
       " 'official': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f390>,\n",
       " 'sunday': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f310>,\n",
       " 'place': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f490>,\n",
       " 'go': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f410>,\n",
       " 'based': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f590>,\n",
       " 'among': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f510>,\n",
       " 'third': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f650>,\n",
       " 'times': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f610>,\n",
       " 'took': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f750>,\n",
       " 'right': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f710>,\n",
       " 'days': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f810>,\n",
       " 'local': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f890>,\n",
       " 'economic': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f910>,\n",
       " 'countries': <gensim.models.keyedvectors.Vocab at 0x7fb744d2f990>,\n",
       " 'see': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fa10>,\n",
       " 'best': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fa90>,\n",
       " 'report': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fb10>,\n",
       " 'killed': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fb90>,\n",
       " 'held': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fc10>,\n",
       " 'business': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fc90>,\n",
       " 'west': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fd10>,\n",
       " 'does': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fd90>,\n",
       " 'own': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fdd0>,\n",
       " '%': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fe90>,\n",
       " 'came': <gensim.models.keyedvectors.Vocab at 0x7fb744d2fe10>,\n",
       " 'law': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ff90>,\n",
       " 'months': <gensim.models.keyedvectors.Vocab at 0x7fb744d2ff10>,\n",
       " 'women': <gensim.models.keyedvectors.Vocab at 0x7fb744d320d0>,\n",
       " \"'re\": <gensim.models.keyedvectors.Vocab at 0x7fb7d5b965d0>,\n",
       " 'power': <gensim.models.keyedvectors.Vocab at 0x7fb744d321d0>,\n",
       " 'think': <gensim.models.keyedvectors.Vocab at 0x7fb7d5b96550>,\n",
       " 'service': <gensim.models.keyedvectors.Vocab at 0x7fb744d322d0>,\n",
       " 'children': <gensim.models.keyedvectors.Vocab at 0x7fb7d5b961d0>,\n",
       " 'bush': <gensim.models.keyedvectors.Vocab at 0x7fb744d323d0>,\n",
       " 'show': <gensim.models.keyedvectors.Vocab at 0x7fb7da916090>,\n",
       " '/': <gensim.models.keyedvectors.Vocab at 0x7fb744d324d0>,\n",
       " 'help': <gensim.models.keyedvectors.Vocab at 0x7fb7da916690>,\n",
       " 'chief': <gensim.models.keyedvectors.Vocab at 0x7fb744d325d0>,\n",
       " 'saturday': <gensim.models.keyedvectors.Vocab at 0x7fb7da916590>,\n",
       " 'system': <gensim.models.keyedvectors.Vocab at 0x7fb744d326d0>,\n",
       " 'john': <gensim.models.keyedvectors.Vocab at 0x7fb7d5b8a190>,\n",
       " 'support': <gensim.models.keyedvectors.Vocab at 0x7fb744d327d0>,\n",
       " 'series': <gensim.models.keyedvectors.Vocab at 0x7fb7d5b50490>,\n",
       " 'play': <gensim.models.keyedvectors.Vocab at 0x7fb744d328d0>,\n",
       " 'office': <gensim.models.keyedvectors.Vocab at 0x7fb744d32110>,\n",
       " 'following': <gensim.models.keyedvectors.Vocab at 0x7fb744d32990>,\n",
       " 'me': <gensim.models.keyedvectors.Vocab at 0x7fb744d32210>,\n",
       " 'meeting': <gensim.models.keyedvectors.Vocab at 0x7fb744d32a90>,\n",
       " 'expected': <gensim.models.keyedvectors.Vocab at 0x7fb744d32310>,\n",
       " 'late': <gensim.models.keyedvectors.Vocab at 0x7fb744d32b90>,\n",
       " 'washington': <gensim.models.keyedvectors.Vocab at 0x7fb744d32410>,\n",
       " 'games': <gensim.models.keyedvectors.Vocab at 0x7fb744d32c90>,\n",
       " 'european': <gensim.models.keyedvectors.Vocab at 0x7fb744d32510>,\n",
       " 'league': <gensim.models.keyedvectors.Vocab at 0x7fb744d32e10>,\n",
       " 'reported': <gensim.models.keyedvectors.Vocab at 0x7fb744d32610>,\n",
       " 'final': <gensim.models.keyedvectors.Vocab at 0x7fb744d32f10>,\n",
       " 'added': <gensim.models.keyedvectors.Vocab at 0x7fb744d32710>,\n",
       " 'without': <gensim.models.keyedvectors.Vocab at 0x7fb744d33050>,\n",
       " 'british': <gensim.models.keyedvectors.Vocab at 0x7fb744d32810>,\n",
       " 'white': <gensim.models.keyedvectors.Vocab at 0x7fb744d33150>,\n",
       " 'history': <gensim.models.keyedvectors.Vocab at 0x7fb744d32910>,\n",
       " 'man': <gensim.models.keyedvectors.Vocab at 0x7fb744d33250>,\n",
       " 'men': <gensim.models.keyedvectors.Vocab at 0x7fb744d32a10>,\n",
       " 'became': <gensim.models.keyedvectors.Vocab at 0x7fb744d33350>,\n",
       " 'want': <gensim.models.keyedvectors.Vocab at 0x7fb744d32b10>,\n",
       " 'march': <gensim.models.keyedvectors.Vocab at 0x7fb744d33450>,\n",
       " 'case': <gensim.models.keyedvectors.Vocab at 0x7fb744d32c10>,\n",
       " 'few': <gensim.models.keyedvectors.Vocab at 0x7fb744d33550>,\n",
       " 'run': <gensim.models.keyedvectors.Vocab at 0x7fb744d32d10>,\n",
       " 'money': <gensim.models.keyedvectors.Vocab at 0x7fb744d33650>,\n",
       " 'began': <gensim.models.keyedvectors.Vocab at 0x7fb744d32dd0>,\n",
       " 'open': <gensim.models.keyedvectors.Vocab at 0x7fb744d33750>,\n",
       " 'name': <gensim.models.keyedvectors.Vocab at 0x7fb744d32ed0>,\n",
       " 'trade': <gensim.models.keyedvectors.Vocab at 0x7fb744d33850>,\n",
       " 'center': <gensim.models.keyedvectors.Vocab at 0x7fb744d32fd0>,\n",
       " '3': <gensim.models.keyedvectors.Vocab at 0x7fb744d33950>,\n",
       " 'israel': <gensim.models.keyedvectors.Vocab at 0x7fb744d330d0>,\n",
       " 'oil': <gensim.models.keyedvectors.Vocab at 0x7fb744d33a50>,\n",
       " 'too': <gensim.models.keyedvectors.Vocab at 0x7fb744d33190>,\n",
       " 'al': <gensim.models.keyedvectors.Vocab at 0x7fb744d33b50>,\n",
       " 'film': <gensim.models.keyedvectors.Vocab at 0x7fb744d33290>,\n",
       " 'win': <gensim.models.keyedvectors.Vocab at 0x7fb744d33c50>,\n",
       " 'led': <gensim.models.keyedvectors.Vocab at 0x7fb744d33390>,\n",
       " 'east': <gensim.models.keyedvectors.Vocab at 0x7fb744d33d50>,\n",
       " 'central': <gensim.models.keyedvectors.Vocab at 0x7fb744d33490>,\n",
       " '20': <gensim.models.keyedvectors.Vocab at 0x7fb744d33e50>,\n",
       " 'air': <gensim.models.keyedvectors.Vocab at 0x7fb744d33590>,\n",
       " 'come': <gensim.models.keyedvectors.Vocab at 0x7fb744d33f50>,\n",
       " 'chinese': <gensim.models.keyedvectors.Vocab at 0x7fb744d33690>,\n",
       " 'town': <gensim.models.keyedvectors.Vocab at 0x7fb744d35090>,\n",
       " 'leader': <gensim.models.keyedvectors.Vocab at 0x7fb744d33790>,\n",
       " 'army': <gensim.models.keyedvectors.Vocab at 0x7fb744d35190>,\n",
       " 'line': <gensim.models.keyedvectors.Vocab at 0x7fb744d33890>,\n",
       " 'never': <gensim.models.keyedvectors.Vocab at 0x7fb744d35290>,\n",
       " 'little': <gensim.models.keyedvectors.Vocab at 0x7fb744d33990>,\n",
       " 'played': <gensim.models.keyedvectors.Vocab at 0x7fb744d35390>,\n",
       " 'prime': <gensim.models.keyedvectors.Vocab at 0x7fb744d33a90>,\n",
       " 'death': <gensim.models.keyedvectors.Vocab at 0x7fb744d35490>,\n",
       " 'companies': <gensim.models.keyedvectors.Vocab at 0x7fb744d33b90>,\n",
       " 'least': <gensim.models.keyedvectors.Vocab at 0x7fb744d35590>,\n",
       " 'put': <gensim.models.keyedvectors.Vocab at 0x7fb744d33c90>,\n",
       " 'forces': <gensim.models.keyedvectors.Vocab at 0x7fb744d35690>,\n",
       " 'past': <gensim.models.keyedvectors.Vocab at 0x7fb744d33d90>,\n",
       " 'de': <gensim.models.keyedvectors.Vocab at 0x7fb744d35790>,\n",
       " 'half': <gensim.models.keyedvectors.Vocab at 0x7fb744d33e90>,\n",
       " 'june': <gensim.models.keyedvectors.Vocab at 0x7fb744d35890>,\n",
       " 'saying': <gensim.models.keyedvectors.Vocab at 0x7fb744d33f90>,\n",
       " 'know': <gensim.models.keyedvectors.Vocab at 0x7fb744d35990>,\n",
       " 'federal': <gensim.models.keyedvectors.Vocab at 0x7fb744d350d0>,\n",
       " 'french': <gensim.models.keyedvectors.Vocab at 0x7fb744d35a90>,\n",
       " 'peace': <gensim.models.keyedvectors.Vocab at 0x7fb744d351d0>,\n",
       " 'earlier': <gensim.models.keyedvectors.Vocab at 0x7fb744d35b90>,\n",
       " 'capital': <gensim.models.keyedvectors.Vocab at 0x7fb744d352d0>,\n",
       " 'force': <gensim.models.keyedvectors.Vocab at 0x7fb744d35c90>,\n",
       " 'great': <gensim.models.keyedvectors.Vocab at 0x7fb744d353d0>,\n",
       " 'union': <gensim.models.keyedvectors.Vocab at 0x7fb744d35d90>,\n",
       " 'near': <gensim.models.keyedvectors.Vocab at 0x7fb744d354d0>,\n",
       " 'released': <gensim.models.keyedvectors.Vocab at 0x7fb744d35e90>,\n",
       " 'small': <gensim.models.keyedvectors.Vocab at 0x7fb744d355d0>,\n",
       " 'department': <gensim.models.keyedvectors.Vocab at 0x7fb744d35f90>,\n",
       " 'every': <gensim.models.keyedvectors.Vocab at 0x7fb744d356d0>,\n",
       " 'health': <gensim.models.keyedvectors.Vocab at 0x7fb744d370d0>,\n",
       " 'japan': <gensim.models.keyedvectors.Vocab at 0x7fb744d357d0>,\n",
       " 'head': <gensim.models.keyedvectors.Vocab at 0x7fb744d371d0>,\n",
       " 'ago': <gensim.models.keyedvectors.Vocab at 0x7fb744d358d0>,\n",
       " 'night': <gensim.models.keyedvectors.Vocab at 0x7fb744d372d0>,\n",
       " 'big': <gensim.models.keyedvectors.Vocab at 0x7fb744d359d0>,\n",
       " 'cup': <gensim.models.keyedvectors.Vocab at 0x7fb744d373d0>,\n",
       " 'election': <gensim.models.keyedvectors.Vocab at 0x7fb744d35ad0>,\n",
       " 'region': <gensim.models.keyedvectors.Vocab at 0x7fb744d374d0>,\n",
       " 'director': <gensim.models.keyedvectors.Vocab at 0x7fb744d35bd0>,\n",
       " 'talks': <gensim.models.keyedvectors.Vocab at 0x7fb744d375d0>,\n",
       " 'program': <gensim.models.keyedvectors.Vocab at 0x7fb744d35cd0>,\n",
       " 'far': <gensim.models.keyedvectors.Vocab at 0x7fb744d376d0>,\n",
       " 'today': <gensim.models.keyedvectors.Vocab at 0x7fb744d35dd0>,\n",
       " 'statement': <gensim.models.keyedvectors.Vocab at 0x7fb744d377d0>,\n",
       " 'july': <gensim.models.keyedvectors.Vocab at 0x7fb744d35ed0>,\n",
       " 'although': <gensim.models.keyedvectors.Vocab at 0x7fb744d378d0>,\n",
       " 'district': <gensim.models.keyedvectors.Vocab at 0x7fb744d35fd0>,\n",
       " 'again': <gensim.models.keyedvectors.Vocab at 0x7fb744d37990>,\n",
       " 'born': <gensim.models.keyedvectors.Vocab at 0x7fb744d37110>,\n",
       " 'development': <gensim.models.keyedvectors.Vocab at 0x7fb744d37a90>,\n",
       " 'leaders': <gensim.models.keyedvectors.Vocab at 0x7fb744d37210>,\n",
       " 'council': <gensim.models.keyedvectors.Vocab at 0x7fb744d37b90>,\n",
       " 'close': <gensim.models.keyedvectors.Vocab at 0x7fb744d37310>,\n",
       " 'record': <gensim.models.keyedvectors.Vocab at 0x7fb744d37c90>,\n",
       " 'along': <gensim.models.keyedvectors.Vocab at 0x7fb744d37410>,\n",
       " 'county': <gensim.models.keyedvectors.Vocab at 0x7fb744d37d90>,\n",
       " 'france': <gensim.models.keyedvectors.Vocab at 0x7fb744d37510>,\n",
       " 'went': <gensim.models.keyedvectors.Vocab at 0x7fb744d37e90>,\n",
       " 'point': <gensim.models.keyedvectors.Vocab at 0x7fb744d37610>,\n",
       " 'must': <gensim.models.keyedvectors.Vocab at 0x7fb744d37f50>,\n",
       " 'spokesman': <gensim.models.keyedvectors.Vocab at 0x7fb744d37710>,\n",
       " 'your': <gensim.models.keyedvectors.Vocab at 0x7fb744d39090>,\n",
       " 'member': <gensim.models.keyedvectors.Vocab at 0x7fb744d37810>,\n",
       " 'plan': <gensim.models.keyedvectors.Vocab at 0x7fb744d39190>,\n",
       " 'financial': <gensim.models.keyedvectors.Vocab at 0x7fb744d37910>,\n",
       " 'april': <gensim.models.keyedvectors.Vocab at 0x7fb744d39290>,\n",
       " 'recent': <gensim.models.keyedvectors.Vocab at 0x7fb744d37a10>,\n",
       " 'campaign': <gensim.models.keyedvectors.Vocab at 0x7fb744d39390>,\n",
       " 'become': <gensim.models.keyedvectors.Vocab at 0x7fb744d37b10>,\n",
       " 'troops': <gensim.models.keyedvectors.Vocab at 0x7fb744d39490>,\n",
       " 'whether': <gensim.models.keyedvectors.Vocab at 0x7fb744d37c10>,\n",
       " 'lost': <gensim.models.keyedvectors.Vocab at 0x7fb744d39590>,\n",
       " 'music': <gensim.models.keyedvectors.Vocab at 0x7fb744d37d10>,\n",
       " '15': <gensim.models.keyedvectors.Vocab at 0x7fb744d39690>,\n",
       " 'got': <gensim.models.keyedvectors.Vocab at 0x7fb744d37e10>,\n",
       " 'israeli': <gensim.models.keyedvectors.Vocab at 0x7fb744d39790>,\n",
       " '30': <gensim.models.keyedvectors.Vocab at 0x7fb744d37f90>,\n",
       " 'need': <gensim.models.keyedvectors.Vocab at 0x7fb744d39890>,\n",
       " '4': <gensim.models.keyedvectors.Vocab at 0x7fb744d390d0>,\n",
       " 'lead': <gensim.models.keyedvectors.Vocab at 0x7fb744d39950>,\n",
       " 'already': <gensim.models.keyedvectors.Vocab at 0x7fb744d391d0>,\n",
       " 'russia': <gensim.models.keyedvectors.Vocab at 0x7fb744d39a50>,\n",
       " 'though': <gensim.models.keyedvectors.Vocab at 0x7fb744d392d0>,\n",
       " 'might': <gensim.models.keyedvectors.Vocab at 0x7fb744d39b50>,\n",
       " 'free': <gensim.models.keyedvectors.Vocab at 0x7fb744d393d0>,\n",
       " 'hit': <gensim.models.keyedvectors.Vocab at 0x7fb744d39c50>,\n",
       " 'rights': <gensim.models.keyedvectors.Vocab at 0x7fb744d394d0>,\n",
       " '11': <gensim.models.keyedvectors.Vocab at 0x7fb744d39d50>,\n",
       " 'information': <gensim.models.keyedvectors.Vocab at 0x7fb744d395d0>,\n",
       " 'away': <gensim.models.keyedvectors.Vocab at 0x7fb744d39e50>,\n",
       " '12': <gensim.models.keyedvectors.Vocab at 0x7fb744d396d0>,\n",
       " '5': <gensim.models.keyedvectors.Vocab at 0x7fb744d39f50>,\n",
       " 'others': <gensim.models.keyedvectors.Vocab at 0x7fb744d39710>,\n",
       " 'control': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c090>,\n",
       " 'within': <gensim.models.keyedvectors.Vocab at 0x7fb744d39810>,\n",
       " 'large': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c190>,\n",
       " 'economy': <gensim.models.keyedvectors.Vocab at 0x7fb744d39910>,\n",
       " 'press': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c290>,\n",
       " 'agency': <gensim.models.keyedvectors.Vocab at 0x7fb744d39a10>,\n",
       " 'water': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c390>,\n",
       " 'died': <gensim.models.keyedvectors.Vocab at 0x7fb744d39b10>,\n",
       " 'career': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c490>,\n",
       " 'making': <gensim.models.keyedvectors.Vocab at 0x7fb744d39c10>,\n",
       " '...': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c550>,\n",
       " 'deal': <gensim.models.keyedvectors.Vocab at 0x7fb744d39d10>,\n",
       " 'attack': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c650>,\n",
       " 'side': <gensim.models.keyedvectors.Vocab at 0x7fb744d39dd0>,\n",
       " 'seven': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c750>,\n",
       " 'better': <gensim.models.keyedvectors.Vocab at 0x7fb744d39ed0>,\n",
       " 'less': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c850>,\n",
       " 'september': <gensim.models.keyedvectors.Vocab at 0x7fb744d39fd0>,\n",
       " 'once': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c950>,\n",
       " 'clinton': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c110>,\n",
       " 'main': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ca50>,\n",
       " 'due': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c210>,\n",
       " 'committee': <gensim.models.keyedvectors.Vocab at 0x7fb744d3cb50>,\n",
       " 'building': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c310>,\n",
       " 'conference': <gensim.models.keyedvectors.Vocab at 0x7fb744d3cc50>,\n",
       " 'club': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c410>,\n",
       " 'january': <gensim.models.keyedvectors.Vocab at 0x7fb744d3cd50>,\n",
       " 'decision': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c590>,\n",
       " 'stock': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ce50>,\n",
       " 'america': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c690>,\n",
       " 'given': <gensim.models.keyedvectors.Vocab at 0x7fb744d3cf50>,\n",
       " 'give': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c790>,\n",
       " 'often': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e050>,\n",
       " 'announced': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c890>,\n",
       " 'television': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e0d0>,\n",
       " 'industry': <gensim.models.keyedvectors.Vocab at 0x7fb744d3c990>,\n",
       " 'order': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e150>,\n",
       " 'young': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ca90>,\n",
       " \"'ve\": <gensim.models.keyedvectors.Vocab at 0x7fb744d3e1d0>,\n",
       " 'palestinian': <gensim.models.keyedvectors.Vocab at 0x7fb744d3cb90>,\n",
       " 'age': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e250>,\n",
       " 'start': <gensim.models.keyedvectors.Vocab at 0x7fb744d3cc90>,\n",
       " 'administration': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e2d0>,\n",
       " 'russian': <gensim.models.keyedvectors.Vocab at 0x7fb744d3cd90>,\n",
       " 'prices': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e350>,\n",
       " 'round': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ce90>,\n",
       " 'december': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e3d0>,\n",
       " 'nations': <gensim.models.keyedvectors.Vocab at 0x7fb744d3cf90>,\n",
       " \"'m\": <gensim.models.keyedvectors.Vocab at 0x7fb744d3e4d0>,\n",
       " 'human': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e450>,\n",
       " 'india': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e5d0>,\n",
       " 'defense': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e550>,\n",
       " 'asked': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e690>,\n",
       " 'total': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e6d0>,\n",
       " 'october': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e790>,\n",
       " 'players': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e7d0>,\n",
       " 'bill': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e890>,\n",
       " 'important': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e8d0>,\n",
       " 'southern': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e990>,\n",
       " 'move': <gensim.models.keyedvectors.Vocab at 0x7fb744d3e9d0>,\n",
       " 'fire': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ea90>,\n",
       " 'population': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ead0>,\n",
       " 'rose': <gensim.models.keyedvectors.Vocab at 0x7fb744d3eb90>,\n",
       " 'november': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ebd0>,\n",
       " 'include': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ec90>,\n",
       " 'further': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ecd0>,\n",
       " 'nuclear': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ed90>,\n",
       " 'street': <gensim.models.keyedvectors.Vocab at 0x7fb744d3edd0>,\n",
       " 'taken': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ee90>,\n",
       " 'media': <gensim.models.keyedvectors.Vocab at 0x7fb744d3eed0>,\n",
       " 'different': <gensim.models.keyedvectors.Vocab at 0x7fb744d3ef90>,\n",
       " 'issue': <gensim.models.keyedvectors.Vocab at 0x7fb744d3efd0>,\n",
       " 'received': <gensim.models.keyedvectors.Vocab at 0x7fb744d410d0>,\n",
       " 'secretary': <gensim.models.keyedvectors.Vocab at 0x7fb744d41110>,\n",
       " 'return': <gensim.models.keyedvectors.Vocab at 0x7fb744d411d0>,\n",
       " 'college': <gensim.models.keyedvectors.Vocab at 0x7fb744d41210>,\n",
       " 'working': <gensim.models.keyedvectors.Vocab at 0x7fb744d412d0>,\n",
       " 'community': <gensim.models.keyedvectors.Vocab at 0x7fb744d41310>,\n",
       " 'eight': <gensim.models.keyedvectors.Vocab at 0x7fb744d413d0>,\n",
       " 'groups': <gensim.models.keyedvectors.Vocab at 0x7fb744d41410>,\n",
       " 'despite': <gensim.models.keyedvectors.Vocab at 0x7fb744d414d0>,\n",
       " 'level': <gensim.models.keyedvectors.Vocab at 0x7fb744d41510>,\n",
       " 'largest': <gensim.models.keyedvectors.Vocab at 0x7fb744d415d0>,\n",
       " 'whose': <gensim.models.keyedvectors.Vocab at 0x7fb744d41610>,\n",
       " 'attacks': <gensim.models.keyedvectors.Vocab at 0x7fb744d416d0>,\n",
       " 'germany': <gensim.models.keyedvectors.Vocab at 0x7fb744d41710>,\n",
       " 'august': <gensim.models.keyedvectors.Vocab at 0x7fb744d417d0>,\n",
       " 'change': <gensim.models.keyedvectors.Vocab at 0x7fb744d41810>,\n",
       " 'church': <gensim.models.keyedvectors.Vocab at 0x7fb744d418d0>,\n",
       " 'nation': <gensim.models.keyedvectors.Vocab at 0x7fb744d41910>,\n",
       " 'german': <gensim.models.keyedvectors.Vocab at 0x7fb744d419d0>,\n",
       " 'station': <gensim.models.keyedvectors.Vocab at 0x7fb744d41a10>,\n",
       " 'london': <gensim.models.keyedvectors.Vocab at 0x7fb744d41a90>,\n",
       " 'weeks': <gensim.models.keyedvectors.Vocab at 0x7fb744d41b10>,\n",
       " 'having': <gensim.models.keyedvectors.Vocab at 0x7fb744d41b90>,\n",
       " '18': <gensim.models.keyedvectors.Vocab at 0x7fb744d41c10>,\n",
       " 'research': <gensim.models.keyedvectors.Vocab at 0x7fb744d41c90>,\n",
       " 'black': <gensim.models.keyedvectors.Vocab at 0x7fb744d41d10>,\n",
       " 'services': <gensim.models.keyedvectors.Vocab at 0x7fb744d41d90>,\n",
       " 'story': <gensim.models.keyedvectors.Vocab at 0x7fb744d41e10>,\n",
       " '6': <gensim.models.keyedvectors.Vocab at 0x7fb744d41e90>,\n",
       " 'europe': <gensim.models.keyedvectors.Vocab at 0x7fb744d41ed0>,\n",
       " 'sales': <gensim.models.keyedvectors.Vocab at 0x7fb744d41f90>,\n",
       " 'policy': <gensim.models.keyedvectors.Vocab at 0x7fb744d41fd0>,\n",
       " 'visit': <gensim.models.keyedvectors.Vocab at 0x7fb744d430d0>,\n",
       " 'northern': <gensim.models.keyedvectors.Vocab at 0x7fb744d43110>,\n",
       " 'lot': <gensim.models.keyedvectors.Vocab at 0x7fb744d431d0>,\n",
       " 'across': <gensim.models.keyedvectors.Vocab at 0x7fb744d43210>,\n",
       " 'per': <gensim.models.keyedvectors.Vocab at 0x7fb744d432d0>,\n",
       " 'current': <gensim.models.keyedvectors.Vocab at 0x7fb744d43310>,\n",
       " 'board': <gensim.models.keyedvectors.Vocab at 0x7fb744d433d0>,\n",
       " 'football': <gensim.models.keyedvectors.Vocab at 0x7fb744d43410>,\n",
       " 'ministry': <gensim.models.keyedvectors.Vocab at 0x7fb744d434d0>,\n",
       " 'workers': <gensim.models.keyedvectors.Vocab at 0x7fb744d43510>,\n",
       " 'vote': <gensim.models.keyedvectors.Vocab at 0x7fb744d435d0>,\n",
       " 'book': <gensim.models.keyedvectors.Vocab at 0x7fb744d43610>,\n",
       " 'fell': <gensim.models.keyedvectors.Vocab at 0x7fb744d436d0>,\n",
       " 'seen': <gensim.models.keyedvectors.Vocab at 0x7fb744d43710>,\n",
       " 'role': <gensim.models.keyedvectors.Vocab at 0x7fb744d437d0>,\n",
       " 'students': <gensim.models.keyedvectors.Vocab at 0x7fb744d43810>,\n",
       " 'shares': <gensim.models.keyedvectors.Vocab at 0x7fb744d438d0>,\n",
       " 'iran': <gensim.models.keyedvectors.Vocab at 0x7fb744d43910>,\n",
       " 'process': <gensim.models.keyedvectors.Vocab at 0x7fb744d439d0>,\n",
       " 'agreement': <gensim.models.keyedvectors.Vocab at 0x7fb744d43a10>,\n",
       " 'quarter': <gensim.models.keyedvectors.Vocab at 0x7fb744d43ad0>,\n",
       " 'full': <gensim.models.keyedvectors.Vocab at 0x7fb744d43b10>,\n",
       " 'match': <gensim.models.keyedvectors.Vocab at 0x7fb744d43bd0>,\n",
       " 'started': <gensim.models.keyedvectors.Vocab at 0x7fb744d43c10>,\n",
       " 'growth': <gensim.models.keyedvectors.Vocab at 0x7fb744d43cd0>,\n",
       " 'yet': <gensim.models.keyedvectors.Vocab at 0x7fb744d43d10>,\n",
       " 'moved': <gensim.models.keyedvectors.Vocab at 0x7fb744d43dd0>,\n",
       " 'possible': <gensim.models.keyedvectors.Vocab at 0x7fb744d43e10>,\n",
       " 'western': <gensim.models.keyedvectors.Vocab at 0x7fb744d43ed0>,\n",
       " 'special': <gensim.models.keyedvectors.Vocab at 0x7fb744d43f10>,\n",
       " '100': <gensim.models.keyedvectors.Vocab at 0x7fb744d43fd0>,\n",
       " 'plans': <gensim.models.keyedvectors.Vocab at 0x7fb744d46050>,\n",
       " 'interest': <gensim.models.keyedvectors.Vocab at 0x7fb744d46110>,\n",
       " 'behind': <gensim.models.keyedvectors.Vocab at 0x7fb744d46150>,\n",
       " 'strong': <gensim.models.keyedvectors.Vocab at 0x7fb744d46210>,\n",
       " 'england': <gensim.models.keyedvectors.Vocab at 0x7fb744d46250>,\n",
       " 'named': <gensim.models.keyedvectors.Vocab at 0x7fb744d46310>,\n",
       " 'food': <gensim.models.keyedvectors.Vocab at 0x7fb744d46350>,\n",
       " 'period': <gensim.models.keyedvectors.Vocab at 0x7fb744d46410>,\n",
       " 'real': <gensim.models.keyedvectors.Vocab at 0x7fb744d46450>,\n",
       " 'authorities': <gensim.models.keyedvectors.Vocab at 0x7fb744d46510>,\n",
       " 'car': <gensim.models.keyedvectors.Vocab at 0x7fb744d46550>,\n",
       " 'term': <gensim.models.keyedvectors.Vocab at 0x7fb744d46610>,\n",
       " 'rate': <gensim.models.keyedvectors.Vocab at 0x7fb744d46650>,\n",
       " 'race': <gensim.models.keyedvectors.Vocab at 0x7fb744d46710>,\n",
       " 'nearly': <gensim.models.keyedvectors.Vocab at 0x7fb744d46750>,\n",
       " 'korea': <gensim.models.keyedvectors.Vocab at 0x7fb744d46810>,\n",
       " 'enough': <gensim.models.keyedvectors.Vocab at 0x7fb744d46850>,\n",
       " 'site': <gensim.models.keyedvectors.Vocab at 0x7fb744d46910>,\n",
       " 'opposition': <gensim.models.keyedvectors.Vocab at 0x7fb744d46950>,\n",
       " 'keep': <gensim.models.keyedvectors.Vocab at 0x7fb744d46a10>,\n",
       " '25': <gensim.models.keyedvectors.Vocab at 0x7fb744d46a50>,\n",
       " 'call': <gensim.models.keyedvectors.Vocab at 0x7fb744d46b10>,\n",
       " 'future': <gensim.models.keyedvectors.Vocab at 0x7fb744d46b50>,\n",
       " 'taking': <gensim.models.keyedvectors.Vocab at 0x7fb744d46c10>,\n",
       " 'island': <gensim.models.keyedvectors.Vocab at 0x7fb744d46c50>,\n",
       " '2008': <gensim.models.keyedvectors.Vocab at 0x7fb744d46d10>,\n",
       " '2006': <gensim.models.keyedvectors.Vocab at 0x7fb744d46d50>,\n",
       " 'road': <gensim.models.keyedvectors.Vocab at 0x7fb744d46e10>,\n",
       " 'outside': <gensim.models.keyedvectors.Vocab at 0x7fb744d46e50>,\n",
       " 'really': <gensim.models.keyedvectors.Vocab at 0x7fb744d46f10>,\n",
       " 'century': <gensim.models.keyedvectors.Vocab at 0x7fb744d46f50>,\n",
       " 'democratic': <gensim.models.keyedvectors.Vocab at 0x7fb744d48050>,\n",
       " 'almost': <gensim.models.keyedvectors.Vocab at 0x7fb744d48090>,\n",
       " 'single': <gensim.models.keyedvectors.Vocab at 0x7fb744d48150>,\n",
       " 'share': <gensim.models.keyedvectors.Vocab at 0x7fb744d48190>,\n",
       " 'leading': <gensim.models.keyedvectors.Vocab at 0x7fb744d48250>,\n",
       " 'trying': <gensim.models.keyedvectors.Vocab at 0x7fb744d48290>,\n",
       " 'find': <gensim.models.keyedvectors.Vocab at 0x7fb744d48350>,\n",
       " 'album': <gensim.models.keyedvectors.Vocab at 0x7fb744d48390>,\n",
       " 'senior': <gensim.models.keyedvectors.Vocab at 0x7fb744d48450>,\n",
       " 'minutes': <gensim.models.keyedvectors.Vocab at 0x7fb744d48490>,\n",
       " 'together': <gensim.models.keyedvectors.Vocab at 0x7fb744d48550>,\n",
       " 'congress': <gensim.models.keyedvectors.Vocab at 0x7fb744d48590>,\n",
       " 'index': <gensim.models.keyedvectors.Vocab at 0x7fb744d48650>,\n",
       " 'australia': <gensim.models.keyedvectors.Vocab at 0x7fb744d48690>,\n",
       " 'results': <gensim.models.keyedvectors.Vocab at 0x7fb744d48750>,\n",
       " 'hard': <gensim.models.keyedvectors.Vocab at 0x7fb744d48790>,\n",
       " 'hours': <gensim.models.keyedvectors.Vocab at 0x7fb744d48850>,\n",
       " 'land': <gensim.models.keyedvectors.Vocab at 0x7fb744d48890>,\n",
       " 'action': <gensim.models.keyedvectors.Vocab at 0x7fb744d48950>,\n",
       " 'higher': <gensim.models.keyedvectors.Vocab at 0x7fb744d48990>,\n",
       " 'field': <gensim.models.keyedvectors.Vocab at 0x7fb744d48ad0>,\n",
       " 'cut': <gensim.models.keyedvectors.Vocab at 0x7fb744d48a50>,\n",
       " 'coach': <gensim.models.keyedvectors.Vocab at 0x7fb744d48bd0>,\n",
       " 'elections': <gensim.models.keyedvectors.Vocab at 0x7fb744d48b50>,\n",
       " 'san': <gensim.models.keyedvectors.Vocab at 0x7fb744d48cd0>,\n",
       " 'issues': <gensim.models.keyedvectors.Vocab at 0x7fb744d48c50>,\n",
       " 'executive': <gensim.models.keyedvectors.Vocab at 0x7fb744d48dd0>,\n",
       " 'february': <gensim.models.keyedvectors.Vocab at 0x7fb744d48d50>,\n",
       " 'production': <gensim.models.keyedvectors.Vocab at 0x7fb744d48ed0>,\n",
       " 'areas': <gensim.models.keyedvectors.Vocab at 0x7fb744d48e50>,\n",
       " 'river': <gensim.models.keyedvectors.Vocab at 0x7fb744d48fd0>,\n",
       " 'face': <gensim.models.keyedvectors.Vocab at 0x7fb744d48f50>,\n",
       " 'using': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b0d0>,\n",
       " 'japanese': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b090>,\n",
       " 'province': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b1d0>,\n",
       " 'park': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b190>,\n",
       " 'price': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b2d0>,\n",
       " 'commission': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b290>,\n",
       " 'california': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b3d0>,\n",
       " 'father': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b390>,\n",
       " 'son': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b4d0>,\n",
       " 'education': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b490>,\n",
       " '7': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b5d0>,\n",
       " 'village': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b550>,\n",
       " 'energy': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b6d0>,\n",
       " 'shot': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b650>,\n",
       " 'short': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b7d0>,\n",
       " 'africa': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b750>,\n",
       " 'key': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b8d0>,\n",
       " 'red': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b850>,\n",
       " 'association': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b9d0>,\n",
       " 'average': <gensim.models.keyedvectors.Vocab at 0x7fb744d4b950>,\n",
       " 'pay': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bad0>,\n",
       " 'exchange': <gensim.models.keyedvectors.Vocab at 0x7fb744d4ba50>,\n",
       " 'eu': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bbd0>,\n",
       " 'something': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bb50>,\n",
       " 'gave': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bcd0>,\n",
       " 'likely': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bc50>,\n",
       " 'player': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bdd0>,\n",
       " 'george': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bd50>,\n",
       " '2007': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bed0>,\n",
       " 'victory': <gensim.models.keyedvectors.Vocab at 0x7fb744d4be50>,\n",
       " '8': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bfd0>,\n",
       " 'low': <gensim.models.keyedvectors.Vocab at 0x7fb744d4be90>,\n",
       " 'things': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c110>,\n",
       " '2010': <gensim.models.keyedvectors.Vocab at 0x7fb744d4bf90>,\n",
       " 'pakistan': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c210>,\n",
       " '14': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c0d0>,\n",
       " 'post': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c310>,\n",
       " 'social': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c1d0>,\n",
       " 'continue': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c410>,\n",
       " 'ever': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c2d0>,\n",
       " 'look': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c510>,\n",
       " 'chairman': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c3d0>,\n",
       " 'job': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c610>,\n",
       " '2000': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c4d0>,\n",
       " 'soldiers': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c710>,\n",
       " 'able': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c5d0>,\n",
       " 'parliament': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c810>,\n",
       " 'front': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c6d0>,\n",
       " 'himself': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c910>,\n",
       " 'problems': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c7d0>,\n",
       " 'private': <gensim.models.keyedvectors.Vocab at 0x7fb744d4ca10>,\n",
       " 'lower': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c8d0>,\n",
       " 'list': <gensim.models.keyedvectors.Vocab at 0x7fb744d4cb10>,\n",
       " 'built': <gensim.models.keyedvectors.Vocab at 0x7fb744d4c9d0>,\n",
       " '13': <gensim.models.keyedvectors.Vocab at 0x7fb744d4cc10>,\n",
       " 'efforts': <gensim.models.keyedvectors.Vocab at 0x7fb744d4cad0>,\n",
       " 'dollar': <gensim.models.keyedvectors.Vocab at 0x7fb744d4cd10>,\n",
       " 'miles': <gensim.models.keyedvectors.Vocab at 0x7fb744d4cbd0>,\n",
       " 'included': <gensim.models.keyedvectors.Vocab at 0x7fb744d4ce10>,\n",
       " 'radio': <gensim.models.keyedvectors.Vocab at 0x7fb744d4ccd0>,\n",
       " 'live': <gensim.models.keyedvectors.Vocab at 0x7fb744d4cf10>,\n",
       " 'form': <gensim.models.keyedvectors.Vocab at 0x7fb744d4cdd0>,\n",
       " 'david': <gensim.models.keyedvectors.Vocab at 0x7fb744d50050>,\n",
       " 'african': <gensim.models.keyedvectors.Vocab at 0x7fb744d4ced0>,\n",
       " 'increase': <gensim.models.keyedvectors.Vocab at 0x7fb744d50150>,\n",
       " 'reports': <gensim.models.keyedvectors.Vocab at 0x7fb744d4cfd0>,\n",
       " 'sent': <gensim.models.keyedvectors.Vocab at 0x7fb744d50250>,\n",
       " 'fourth': <gensim.models.keyedvectors.Vocab at 0x7fb744d50110>,\n",
       " 'always': <gensim.models.keyedvectors.Vocab at 0x7fb744d50350>,\n",
       " 'king': <gensim.models.keyedvectors.Vocab at 0x7fb744d50210>,\n",
       " '50': <gensim.models.keyedvectors.Vocab at 0x7fb744d50450>,\n",
       " 'tax': <gensim.models.keyedvectors.Vocab at 0x7fb744d50310>,\n",
       " 'taiwan': <gensim.models.keyedvectors.Vocab at 0x7fb744d50550>,\n",
       " 'britain': <gensim.models.keyedvectors.Vocab at 0x7fb744d50410>,\n",
       " '16': <gensim.models.keyedvectors.Vocab at 0x7fb744d50650>,\n",
       " 'playing': <gensim.models.keyedvectors.Vocab at 0x7fb744d50510>,\n",
       " 'title': <gensim.models.keyedvectors.Vocab at 0x7fb744d50750>,\n",
       " 'middle': <gensim.models.keyedvectors.Vocab at 0x7fb744d50610>,\n",
       " 'meet': <gensim.models.keyedvectors.Vocab at 0x7fb744d50850>,\n",
       " 'global': <gensim.models.keyedvectors.Vocab at 0x7fb744d50710>,\n",
       " 'wife': <gensim.models.keyedvectors.Vocab at 0x7fb744d50950>,\n",
       " '2009': <gensim.models.keyedvectors.Vocab at 0x7fb744d50810>,\n",
       " 'position': <gensim.models.keyedvectors.Vocab at 0x7fb744d50a50>,\n",
       " 'located': <gensim.models.keyedvectors.Vocab at 0x7fb744d50910>,\n",
       " 'clear': <gensim.models.keyedvectors.Vocab at 0x7fb744d50b50>,\n",
       " 'ahead': <gensim.models.keyedvectors.Vocab at 0x7fb744d50a10>,\n",
       " '2004': <gensim.models.keyedvectors.Vocab at 0x7fb744d50c50>,\n",
       " '2005': <gensim.models.keyedvectors.Vocab at 0x7fb744d50b10>,\n",
       " 'iraqi': <gensim.models.keyedvectors.Vocab at 0x7fb744d50d50>,\n",
       " 'english': <gensim.models.keyedvectors.Vocab at 0x7fb744d50c10>,\n",
       " 'result': <gensim.models.keyedvectors.Vocab at 0x7fb744d50e50>,\n",
       " 'release': <gensim.models.keyedvectors.Vocab at 0x7fb744d50d10>,\n",
       " 'violence': <gensim.models.keyedvectors.Vocab at 0x7fb744d50f50>,\n",
       " 'goal': <gensim.models.keyedvectors.Vocab at 0x7fb744d50e10>,\n",
       " 'project': <gensim.models.keyedvectors.Vocab at 0x7fb744d52090>,\n",
       " 'closed': <gensim.models.keyedvectors.Vocab at 0x7fb744d50f10>,\n",
       " 'border': <gensim.models.keyedvectors.Vocab at 0x7fb744d52190>,\n",
       " 'body': <gensim.models.keyedvectors.Vocab at 0x7fb744d52050>,\n",
       " 'soon': <gensim.models.keyedvectors.Vocab at 0x7fb744d52290>,\n",
       " 'crisis': <gensim.models.keyedvectors.Vocab at 0x7fb744d52150>,\n",
       " 'division': <gensim.models.keyedvectors.Vocab at 0x7fb744d52390>,\n",
       " '&amp;': <gensim.models.keyedvectors.Vocab at 0x7fb744d52250>,\n",
       " 'served': <gensim.models.keyedvectors.Vocab at 0x7fb744d52450>,\n",
       " 'tour': <gensim.models.keyedvectors.Vocab at 0x7fb744d52350>,\n",
       " 'hospital': <gensim.models.keyedvectors.Vocab at 0x7fb744d52550>,\n",
       " 'kong': <gensim.models.keyedvectors.Vocab at 0x7fb744d524d0>,\n",
       " 'test': <gensim.models.keyedvectors.Vocab at 0x7fb744d52650>,\n",
       " 'hong': <gensim.models.keyedvectors.Vocab at 0x7fb744d525d0>,\n",
       " 'u.n.': <gensim.models.keyedvectors.Vocab at 0x7fb744d52750>,\n",
       " 'inc.': <gensim.models.keyedvectors.Vocab at 0x7fb744d526d0>,\n",
       " 'technology': <gensim.models.keyedvectors.Vocab at 0x7fb744d52850>,\n",
       " 'believe': <gensim.models.keyedvectors.Vocab at 0x7fb744d527d0>,\n",
       " 'organization': <gensim.models.keyedvectors.Vocab at 0x7fb744d52950>,\n",
       " 'published': <gensim.models.keyedvectors.Vocab at 0x7fb744d528d0>,\n",
       " 'weapons': <gensim.models.keyedvectors.Vocab at 0x7fb744d52a50>,\n",
       " 'agreed': <gensim.models.keyedvectors.Vocab at 0x7fb744d529d0>,\n",
       " 'why': <gensim.models.keyedvectors.Vocab at 0x7fb744d52b50>,\n",
       " 'nine': <gensim.models.keyedvectors.Vocab at 0x7fb744d52ad0>,\n",
       " 'summer': <gensim.models.keyedvectors.Vocab at 0x7fb744d52c50>,\n",
       " 'wanted': <gensim.models.keyedvectors.Vocab at 0x7fb744d52bd0>,\n",
       " 'republican': <gensim.models.keyedvectors.Vocab at 0x7fb744d52d50>,\n",
       " 'act': <gensim.models.keyedvectors.Vocab at 0x7fb744d52cd0>,\n",
       " 'recently': <gensim.models.keyedvectors.Vocab at 0x7fb744d52e50>,\n",
       " 'texas': <gensim.models.keyedvectors.Vocab at 0x7fb744d52dd0>,\n",
       " 'course': <gensim.models.keyedvectors.Vocab at 0x7fb744d52f50>,\n",
       " 'problem': <gensim.models.keyedvectors.Vocab at 0x7fb744d52ed0>,\n",
       " 'senate': <gensim.models.keyedvectors.Vocab at 0x7fb744d55090>,\n",
       " 'medical': <gensim.models.keyedvectors.Vocab at 0x7fb744d52fd0>,\n",
       " 'un': <gensim.models.keyedvectors.Vocab at 0x7fb744d55190>,\n",
       " 'done': <gensim.models.keyedvectors.Vocab at 0x7fb744d55110>,\n",
       " 'reached': <gensim.models.keyedvectors.Vocab at 0x7fb744d55290>,\n",
       " 'star': <gensim.models.keyedvectors.Vocab at 0x7fb744d55210>,\n",
       " 'continued': <gensim.models.keyedvectors.Vocab at 0x7fb744d55390>,\n",
       " 'investors': <gensim.models.keyedvectors.Vocab at 0x7fb744d55310>,\n",
       " 'living': <gensim.models.keyedvectors.Vocab at 0x7fb744d55490>,\n",
       " 'care': <gensim.models.keyedvectors.Vocab at 0x7fb744d55410>,\n",
       " 'signed': <gensim.models.keyedvectors.Vocab at 0x7fb744d55590>,\n",
       " '17': <gensim.models.keyedvectors.Vocab at 0x7fb744d55510>,\n",
       " 'art': <gensim.models.keyedvectors.Vocab at 0x7fb744d55690>,\n",
       " 'provide': <gensim.models.keyedvectors.Vocab at 0x7fb744d55610>,\n",
       " 'worked': <gensim.models.keyedvectors.Vocab at 0x7fb744d55790>,\n",
       " 'presidential': <gensim.models.keyedvectors.Vocab at 0x7fb744d55710>,\n",
       " 'gold': <gensim.models.keyedvectors.Vocab at 0x7fb744d55890>,\n",
       " 'obama': <gensim.models.keyedvectors.Vocab at 0x7fb744d55810>,\n",
       " 'morning': <gensim.models.keyedvectors.Vocab at 0x7fb744d55990>,\n",
       " 'dead': <gensim.models.keyedvectors.Vocab at 0x7fb744d55910>,\n",
       " 'opened': <gensim.models.keyedvectors.Vocab at 0x7fb744d55a90>,\n",
       " \"'ll\": <gensim.models.keyedvectors.Vocab at 0x7fb744d55a10>,\n",
       " 'event': <gensim.models.keyedvectors.Vocab at 0x7fb744d55b90>,\n",
       " 'previous': <gensim.models.keyedvectors.Vocab at 0x7fb744d55b10>,\n",
       " 'cost': <gensim.models.keyedvectors.Vocab at 0x7fb744d55c90>,\n",
       " 'instead': <gensim.models.keyedvectors.Vocab at 0x7fb744d55c10>,\n",
       " 'canada': <gensim.models.keyedvectors.Vocab at 0x7fb744d55d90>,\n",
       " 'band': <gensim.models.keyedvectors.Vocab at 0x7fb744d55d10>,\n",
       " 'teams': <gensim.models.keyedvectors.Vocab at 0x7fb744d55e90>,\n",
       " 'daily': <gensim.models.keyedvectors.Vocab at 0x7fb744d55e10>,\n",
       " '2001': <gensim.models.keyedvectors.Vocab at 0x7fb744d55f90>,\n",
       " 'available': <gensim.models.keyedvectors.Vocab at 0x7fb744d55f10>,\n",
       " 'drug': <gensim.models.keyedvectors.Vocab at 0x7fb744d570d0>,\n",
       " 'coming': <gensim.models.keyedvectors.Vocab at 0x7fb744d57050>,\n",
       " '2003': <gensim.models.keyedvectors.Vocab at 0x7fb744d571d0>,\n",
       " 'investment': <gensim.models.keyedvectors.Vocab at 0x7fb744d57150>,\n",
       " '’s': <gensim.models.keyedvectors.Vocab at 0x7fb744d572d0>,\n",
       " 'michael': <gensim.models.keyedvectors.Vocab at 0x7fb744d57190>,\n",
       " 'civil': <gensim.models.keyedvectors.Vocab at 0x7fb744d573d0>,\n",
       " 'woman': <gensim.models.keyedvectors.Vocab at 0x7fb744d57290>,\n",
       " 'training': <gensim.models.keyedvectors.Vocab at 0x7fb744d574d0>,\n",
       " 'appeared': <gensim.models.keyedvectors.Vocab at 0x7fb744d57390>,\n",
       " '9': <gensim.models.keyedvectors.Vocab at 0x7fb744d575d0>,\n",
       " 'involved': <gensim.models.keyedvectors.Vocab at 0x7fb744d57450>,\n",
       " 'indian': <gensim.models.keyedvectors.Vocab at 0x7fb744d576d0>,\n",
       " 'similar': <gensim.models.keyedvectors.Vocab at 0x7fb744d57550>,\n",
       " 'situation': <gensim.models.keyedvectors.Vocab at 0x7fb744d577d0>,\n",
       " '24': <gensim.models.keyedvectors.Vocab at 0x7fb744d57650>,\n",
       " 'los': <gensim.models.keyedvectors.Vocab at 0x7fb744d578d0>,\n",
       " 'running': <gensim.models.keyedvectors.Vocab at 0x7fb744d57750>,\n",
       " 'fighting': <gensim.models.keyedvectors.Vocab at 0x7fb744d579d0>,\n",
       " 'mark': <gensim.models.keyedvectors.Vocab at 0x7fb744d57850>,\n",
       " '40': <gensim.models.keyedvectors.Vocab at 0x7fb744d57ad0>,\n",
       " 'trial': <gensim.models.keyedvectors.Vocab at 0x7fb744d57950>,\n",
       " 'hold': <gensim.models.keyedvectors.Vocab at 0x7fb744d57bd0>,\n",
       " 'australian': <gensim.models.keyedvectors.Vocab at 0x7fb744d57a50>,\n",
       " 'thought': <gensim.models.keyedvectors.Vocab at 0x7fb744d57cd0>,\n",
       " '!': <gensim.models.keyedvectors.Vocab at 0x7fb744d57b50>,\n",
       " 'study': <gensim.models.keyedvectors.Vocab at 0x7fb744d57d10>,\n",
       " 'fall': <gensim.models.keyedvectors.Vocab at 0x7fb744d57c50>,\n",
       " 'mother': <gensim.models.keyedvectors.Vocab at 0x7fb744d57e10>,\n",
       " 'met': <gensim.models.keyedvectors.Vocab at 0x7fb744d57d50>,\n",
       " 'relations': <gensim.models.keyedvectors.Vocab at 0x7fb744d57f10>,\n",
       " 'anti': <gensim.models.keyedvectors.Vocab at 0x7fb744d57e50>,\n",
       " '2002': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a050>,\n",
       " 'song': <gensim.models.keyedvectors.Vocab at 0x7fb744d57f50>,\n",
       " 'popular': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a150>,\n",
       " 'base': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a090>,\n",
       " 'tv': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a250>,\n",
       " 'ground': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a190>,\n",
       " 'markets': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a350>,\n",
       " 'ii': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a290>,\n",
       " 'newspaper': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a450>,\n",
       " 'staff': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a390>,\n",
       " 'saw': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a550>,\n",
       " 'hand': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a490>,\n",
       " 'hope': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a650>,\n",
       " 'operations': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a590>,\n",
       " 'pressure': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a750>,\n",
       " 'americans': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a690>,\n",
       " 'eastern': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a850>,\n",
       " 'st.': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a790>,\n",
       " 'legal': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a950>,\n",
       " 'asia': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a890>,\n",
       " 'budget': <gensim.models.keyedvectors.Vocab at 0x7fb744d5aa50>,\n",
       " 'returned': <gensim.models.keyedvectors.Vocab at 0x7fb744d5a990>,\n",
       " 'considered': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ab10>,\n",
       " 'love': <gensim.models.keyedvectors.Vocab at 0x7fb744d5aa90>,\n",
       " 'wrote': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ac10>,\n",
       " 'stop': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ab90>,\n",
       " 'fight': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ad10>,\n",
       " 'currently': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ac90>,\n",
       " 'charges': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ae10>,\n",
       " 'try': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ad90>,\n",
       " 'aid': <gensim.models.keyedvectors.Vocab at 0x7fb744d5af10>,\n",
       " 'ended': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ae90>,\n",
       " 'management': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c050>,\n",
       " 'brought': <gensim.models.keyedvectors.Vocab at 0x7fb744d5af90>,\n",
       " 'cases': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c150>,\n",
       " 'decided': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c0d0>,\n",
       " 'failed': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c250>,\n",
       " 'network': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c1d0>,\n",
       " 'works': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c350>,\n",
       " 'gas': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c2d0>,\n",
       " 'turned': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c450>,\n",
       " 'fact': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c3d0>,\n",
       " 'vice': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c510>,\n",
       " 'ca': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c550>,\n",
       " 'mexico': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c610>,\n",
       " 'trading': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c650>,\n",
       " 'especially': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c710>,\n",
       " 'reporters': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c750>,\n",
       " 'afghanistan': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c810>,\n",
       " 'common': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c850>,\n",
       " 'looking': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c910>,\n",
       " 'space': <gensim.models.keyedvectors.Vocab at 0x7fb744d5c950>,\n",
       " 'rates': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ca10>,\n",
       " 'manager': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ca50>,\n",
       " 'loss': <gensim.models.keyedvectors.Vocab at 0x7fb744d5cb10>,\n",
       " '2011': <gensim.models.keyedvectors.Vocab at 0x7fb744d5cb50>,\n",
       " 'justice': <gensim.models.keyedvectors.Vocab at 0x7fb744d5cc10>,\n",
       " 'thousands': <gensim.models.keyedvectors.Vocab at 0x7fb744d5cc50>,\n",
       " 'james': <gensim.models.keyedvectors.Vocab at 0x7fb744d5cd10>,\n",
       " 'rather': <gensim.models.keyedvectors.Vocab at 0x7fb744d5cd50>,\n",
       " 'fund': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ce10>,\n",
       " 'thing': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ce50>,\n",
       " 'republic': <gensim.models.keyedvectors.Vocab at 0x7fb744d5cf10>,\n",
       " 'opening': <gensim.models.keyedvectors.Vocab at 0x7fb744d5cf50>,\n",
       " 'accused': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f050>,\n",
       " 'winning': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f090>,\n",
       " 'scored': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f150>,\n",
       " 'championship': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f190>,\n",
       " 'example': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f250>,\n",
       " 'getting': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f290>,\n",
       " 'biggest': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f350>,\n",
       " 'performance': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f390>,\n",
       " 'sports': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f450>,\n",
       " '1998': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f490>,\n",
       " 'let': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f550>,\n",
       " 'allowed': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f590>,\n",
       " 'schools': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f650>,\n",
       " 'means': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f690>,\n",
       " 'turn': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f750>,\n",
       " 'leave': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f790>,\n",
       " 'no.': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f850>,\n",
       " 'robert': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f890>,\n",
       " 'personal': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f950>,\n",
       " 'stocks': <gensim.models.keyedvectors.Vocab at 0x7fb744d5f990>,\n",
       " 'showed': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fa50>,\n",
       " 'light': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fa90>,\n",
       " 'arrested': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fb50>,\n",
       " 'person': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fb90>,\n",
       " 'either': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fc50>,\n",
       " 'offer': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fc90>,\n",
       " 'majority': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fd50>,\n",
       " 'battle': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fd90>,\n",
       " '19': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fe50>,\n",
       " 'class': <gensim.models.keyedvectors.Vocab at 0x7fb744d5fe90>,\n",
       " 'evidence': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ff50>,\n",
       " 'makes': <gensim.models.keyedvectors.Vocab at 0x7fb744d5ff90>,\n",
       " 'society': <gensim.models.keyedvectors.Vocab at 0x7fb744d62090>,\n",
       " 'products': <gensim.models.keyedvectors.Vocab at 0x7fb744d620d0>,\n",
       " 'regional': <gensim.models.keyedvectors.Vocab at 0x7fb744d62190>,\n",
       " 'needed': <gensim.models.keyedvectors.Vocab at 0x7fb744d621d0>,\n",
       " 'stage': <gensim.models.keyedvectors.Vocab at 0x7fb744d62290>,\n",
       " 'am': <gensim.models.keyedvectors.Vocab at 0x7fb744d622d0>,\n",
       " 'doing': <gensim.models.keyedvectors.Vocab at 0x7fb744d62390>,\n",
       " 'families': <gensim.models.keyedvectors.Vocab at 0x7fb744d623d0>,\n",
       " 'construction': <gensim.models.keyedvectors.Vocab at 0x7fb744d62490>,\n",
       " 'various': <gensim.models.keyedvectors.Vocab at 0x7fb744d624d0>,\n",
       " '1996': <gensim.models.keyedvectors.Vocab at 0x7fb744d62590>,\n",
       " 'sold': <gensim.models.keyedvectors.Vocab at 0x7fb744d625d0>,\n",
       " 'independent': <gensim.models.keyedvectors.Vocab at 0x7fb744d62690>,\n",
       " 'kind': <gensim.models.keyedvectors.Vocab at 0x7fb744d626d0>,\n",
       " 'airport': <gensim.models.keyedvectors.Vocab at 0x7fb744d62790>,\n",
       " 'paul': <gensim.models.keyedvectors.Vocab at 0x7fb744d627d0>,\n",
       " 'judge': <gensim.models.keyedvectors.Vocab at 0x7fb744d62890>,\n",
       " 'internet': <gensim.models.keyedvectors.Vocab at 0x7fb744d628d0>,\n",
       " 'movement': <gensim.models.keyedvectors.Vocab at 0x7fb744d62990>,\n",
       " 'room': <gensim.models.keyedvectors.Vocab at 0x7fb744d629d0>,\n",
       " 'followed': <gensim.models.keyedvectors.Vocab at 0x7fb744d62a90>,\n",
       " 'original': <gensim.models.keyedvectors.Vocab at 0x7fb744d62ad0>,\n",
       " 'angeles': <gensim.models.keyedvectors.Vocab at 0x7fb744d62b90>,\n",
       " 'italy': <gensim.models.keyedvectors.Vocab at 0x7fb744d62bd0>,\n",
       " '`': <gensim.models.keyedvectors.Vocab at 0x7fb744d62c90>,\n",
       " 'data': <gensim.models.keyedvectors.Vocab at 0x7fb744d62c10>,\n",
       " 'comes': <gensim.models.keyedvectors.Vocab at 0x7fb744d62d90>,\n",
       " 'parties': <gensim.models.keyedvectors.Vocab at 0x7fb744d62d10>,\n",
       " 'nothing': <gensim.models.keyedvectors.Vocab at 0x7fb744d62e90>,\n",
       " 'sea': <gensim.models.keyedvectors.Vocab at 0x7fb744d62e10>,\n",
       " 'bring': <gensim.models.keyedvectors.Vocab at 0x7fb744d62f90>,\n",
       " '2012': <gensim.models.keyedvectors.Vocab at 0x7fb744d62f10>,\n",
       " 'annual': <gensim.models.keyedvectors.Vocab at 0x7fb744d640d0>,\n",
       " 'officer': <gensim.models.keyedvectors.Vocab at 0x7fb744d64050>,\n",
       " 'beijing': <gensim.models.keyedvectors.Vocab at 0x7fb744d641d0>,\n",
       " 'present': <gensim.models.keyedvectors.Vocab at 0x7fb744d64150>,\n",
       " 'remain': <gensim.models.keyedvectors.Vocab at 0x7fb744d642d0>,\n",
       " 'nato': <gensim.models.keyedvectors.Vocab at 0x7fb744d64250>,\n",
       " '1999': <gensim.models.keyedvectors.Vocab at 0x7fb744d643d0>,\n",
       " '22': <gensim.models.keyedvectors.Vocab at 0x7fb744d64350>,\n",
       " 'remains': <gensim.models.keyedvectors.Vocab at 0x7fb744d644d0>,\n",
       " 'allow': <gensim.models.keyedvectors.Vocab at 0x7fb744d64450>,\n",
       " 'florida': <gensim.models.keyedvectors.Vocab at 0x7fb744d645d0>,\n",
       " 'computer': <gensim.models.keyedvectors.Vocab at 0x7fb744d64550>,\n",
       " '21': <gensim.models.keyedvectors.Vocab at 0x7fb744d646d0>,\n",
       " 'contract': <gensim.models.keyedvectors.Vocab at 0x7fb744d64650>,\n",
       " 'coast': <gensim.models.keyedvectors.Vocab at 0x7fb744d647d0>,\n",
       " 'created': <gensim.models.keyedvectors.Vocab at 0x7fb744d64750>,\n",
       " 'demand': <gensim.models.keyedvectors.Vocab at 0x7fb744d648d0>,\n",
       " 'operation': <gensim.models.keyedvectors.Vocab at 0x7fb744d64850>,\n",
       " 'events': <gensim.models.keyedvectors.Vocab at 0x7fb744d64a50>,\n",
       " 'islamic': <gensim.models.keyedvectors.Vocab at 0x7fb744d64950>,\n",
       " 'beat': <gensim.models.keyedvectors.Vocab at 0x7fb744d64ad0>,\n",
       " 'analysts': <gensim.models.keyedvectors.Vocab at 0x7fb744d64a10>,\n",
       " 'interview': <gensim.models.keyedvectors.Vocab at 0x7fb744d64bd0>,\n",
       " 'helped': <gensim.models.keyedvectors.Vocab at 0x7fb744d64b50>,\n",
       " 'child': <gensim.models.keyedvectors.Vocab at 0x7fb744d64cd0>,\n",
       " 'probably': <gensim.models.keyedvectors.Vocab at 0x7fb744d64c50>,\n",
       " 'spent': <gensim.models.keyedvectors.Vocab at 0x7fb744d64dd0>,\n",
       " 'asian': <gensim.models.keyedvectors.Vocab at 0x7fb744d64d50>,\n",
       " 'effort': <gensim.models.keyedvectors.Vocab at 0x7fb744d64ed0>,\n",
       " 'cooperation': <gensim.models.keyedvectors.Vocab at 0x7fb744d64e50>,\n",
       " 'shows': <gensim.models.keyedvectors.Vocab at 0x7fb744d64fd0>,\n",
       " 'calls': <gensim.models.keyedvectors.Vocab at 0x7fb744d64f50>,\n",
       " 'investigation': <gensim.models.keyedvectors.Vocab at 0x7fb744d66110>,\n",
       " 'lives': <gensim.models.keyedvectors.Vocab at 0x7fb744d66090>,\n",
       " 'video': <gensim.models.keyedvectors.Vocab at 0x7fb744d66210>,\n",
       " 'yen': <gensim.models.keyedvectors.Vocab at 0x7fb744d66190>,\n",
       " 'runs': <gensim.models.keyedvectors.Vocab at 0x7fb744d66310>,\n",
       " 'tried': <gensim.models.keyedvectors.Vocab at 0x7fb744d66290>,\n",
       " 'bad': <gensim.models.keyedvectors.Vocab at 0x7fb744d66410>,\n",
       " 'described': <gensim.models.keyedvectors.Vocab at 0x7fb744d66390>,\n",
       " '1994': <gensim.models.keyedvectors.Vocab at 0x7fb744d66510>,\n",
       " 'toward': <gensim.models.keyedvectors.Vocab at 0x7fb744d66490>,\n",
       " 'written': <gensim.models.keyedvectors.Vocab at 0x7fb744d66610>,\n",
       " 'throughout': <gensim.models.keyedvectors.Vocab at 0x7fb744d66590>,\n",
       " 'established': <gensim.models.keyedvectors.Vocab at 0x7fb744d66710>,\n",
       " 'mission': <gensim.models.keyedvectors.Vocab at 0x7fb744d66690>,\n",
       " 'associated': <gensim.models.keyedvectors.Vocab at 0x7fb744d667d0>,\n",
       " 'buy': <gensim.models.keyedvectors.Vocab at 0x7fb744d66810>,\n",
       " 'growing': <gensim.models.keyedvectors.Vocab at 0x7fb744d668d0>,\n",
       " 'green': <gensim.models.keyedvectors.Vocab at 0x7fb744d66910>,\n",
       " 'forward': <gensim.models.keyedvectors.Vocab at 0x7fb744d669d0>,\n",
       " 'competition': <gensim.models.keyedvectors.Vocab at 0x7fb744d66a10>,\n",
       " 'poor': <gensim.models.keyedvectors.Vocab at 0x7fb744d66a90>,\n",
       " 'latest': <gensim.models.keyedvectors.Vocab at 0x7fb744d66b10>,\n",
       " 'banks': <gensim.models.keyedvectors.Vocab at 0x7fb744d66b90>,\n",
       " 'question': <gensim.models.keyedvectors.Vocab at 0x7fb744d66c10>,\n",
       " '1997': <gensim.models.keyedvectors.Vocab at 0x7fb744d66c90>,\n",
       " 'prison': <gensim.models.keyedvectors.Vocab at 0x7fb744d66d10>,\n",
       " 'feel': <gensim.models.keyedvectors.Vocab at 0x7fb744d66d90>,\n",
       " 'attention': <gensim.models.keyedvectors.Vocab at 0x7fb744d66e10>,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173d52efaa6743719962aeac8fcaaf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=624181.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximebonnin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/Users/maximebonnin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:74: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(624181, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-078a3819f7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mmb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mprint_silhouette_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m df_clusters = pd.DataFrame({\n",
      "\u001b[0;32m<ipython-input-17-078a3819f7af>\u001b[0m in \u001b[0;36mmbkmeans_clusters\u001b[0;34m(X, k, mb, print_silhouette_values)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniBatchKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"For n_clusters = {k}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Inertia:{km.inertia_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    232\u001b[0m                                     labels=labels, label_freqs=label_freqs)\n\u001b[1;32m    233\u001b[0m     results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func,\n\u001b[0;32m--> 234\u001b[0;31m                                               **kwds))\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0;32m-> 1592\u001b[0;31m                                      n_jobs=n_jobs, **kwds)\n\u001b[0m\u001b[1;32m   1593\u001b[0m         if ((X is Y or Y is None)\n\u001b[1;32m   1594\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    #print(f\"For n_clusters = {k}\")\n",
    "    #print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    #print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=vectorized_docs,\n",
    "    k=50,\n",
    "    mb=500,\n",
    "    print_silhouette_values=False,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": df[\"text\"],\n",
    "    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n",
    "    \"cluster\": cluster_labels\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = MiniBatchKMeans(n_clusters=50, batch_size=500).fit(vectorized_docs)\n",
    "predictions = clustering.predict(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20588301  0.24067159  0.19165999 ... -0.01171241 -0.03965203\n",
      "  -0.08586896]\n",
      " [ 0.16592961  0.19839012  0.03009064 ...  0.05206217 -0.00211239\n",
      "   0.08112168]\n",
      " [ 0.12366415  0.22503281  0.09871089 ... -0.01294877 -0.00832571\n",
      "  -0.02904039]\n",
      " ...\n",
      " [ 0.14220501 -0.00394353 -0.00162601 ...  0.14857209 -0.06222622\n",
      "   0.00346095]\n",
      " [ 0.08820026  0.14914491  0.06434108 ... -0.03626029  0.01993634\n",
      "  -0.00894024]\n",
      " [-0.02962896  0.09460649  0.02092578 ... -0.04177928 -0.07266515\n",
      "  -0.01178901]]\n"
     ]
    }
   ],
   "source": [
    "print(clustering.cluster_centers_)\n",
    "np.save(\"cluster_centers.npy\", clustering.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(data)[:,0]\n",
    "del data, tokenized_docs, model, texts\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15bf245cb5a4020b5bf5752896c9d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#results[\"cluster_centers\"] = clustering.cluster_centers_\n",
    "results = {}\n",
    "\n",
    "for i,id_ in tqdm(enumerate(ids)):\n",
    "    results[id_] = {}\n",
    "    results[id_][\"vector\"] = str(list(vectorized_docs[i]))\n",
    "    results[id_][\"cluster\"] = int(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Kernel is dead",
     "output_type": "error",
     "traceback": [
      "Error: Kernel is dead",
      "at g._sendKernelShellControl (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1006305)",
      "at g.sendShellMessage (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1006074)",
      "at g.requestExecute (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1008616)",
      "at d.requestExecute (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:37:328037)",
      "at S.requestExecute (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:32:19306)",
      "at w.executeCodeCell (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300924)",
      "at w.execute (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300551)",
      "at w.start (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:296215)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at t.CellExecutionQueue.executeQueuedCells (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310950)",
      "at t.CellExecutionQueue.start (/Users/maximebonnin/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "with open('Vectors_and_clusters.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'vector': '[2.00304e-06, 1.0]', 'pred': 1}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "s = {\"1\":{\"vector\":str(list(np.array([0.00000200304002001000001000000000000001,1],dtype=np.float32))),\"pred\":1}}\n",
    "print(s)\n",
    "with open('Vectors_and_clusters.json', 'w') as f:\n",
    "    json.dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: example specific particular instance similar \n",
      "Cluster 1: example instance same this well \n",
      "Cluster 2: en que la con de \n",
      "Cluster 3: system using data use example \n",
      "Cluster 4: example instance same this use \n",
      "Cluster 5: particular specific example certain instance \n",
      "Cluster 6: any specific instance example means \n",
      "Cluster 7: und eine der über „ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximebonnin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 8: example instance particular similar specific \n",
      "Cluster 9: provide example instance provided use \n",
      "Cluster 10: using uses system allows use \n",
      "Cluster 11: system example provide use creating \n",
      "Cluster 12: la de et du en \n",
      "Cluster 13: using instance specific data allows \n",
      "Cluster 14: using allows available use system \n",
      "Cluster 15: specific example particular instance use \n",
      "Cluster 16: using system allows directly use \n",
      "Cluster 17: example specific i.e. particular function \n",
      "Cluster 18: example specific using uses particular \n",
      "Cluster 19: example similar usually using typically \n",
      "Cluster 20: example specific particular instance certain \n",
      "Cluster 21: example specific use using system \n",
      "Cluster 22: provide specific example system particular \n",
      "Cluster 23: focus particular well example important \n",
      "Cluster 24: specific furthermore particular certain similar \n",
      "Cluster 25: provided provide instance system use \n",
      "Cluster 26: system allows direct using directly \n",
      "Cluster 27: same example this instance similar \n",
      "Cluster 28: system using use allows uses \n",
      "Cluster 29: example well work particular focus \n",
      "Cluster 30: example particular rather this instance \n",
      "Cluster 31: cells proteins function e.g. protein \n",
      "Cluster 32: example instance using particular similar \n",
      "Cluster 33: example focus particular provide creating \n",
      "Cluster 34: system use using example same \n",
      "Cluster 35: con que y mas por \n",
      "Cluster 36: example instance specific particular system \n",
      "Cluster 37: example particular rather instance any \n",
      "Cluster 38: specific example particular basic context \n",
      "Cluster 39: instance example using same use \n",
      "Cluster 40: for also work based new \n",
      "Cluster 41: this well which example result \n",
      "Cluster 42: use using instance example specific \n",
      "Cluster 43: example particular specific rather instance \n",
      "Cluster 44: uses system using use applications \n",
      "Cluster 45: example system use provide specific \n",
      "Cluster 46: using system signal uses input \n",
      "Cluster 47: well focus its provide example \n",
      "Cluster 48: system provide use using example \n",
      "Cluster 49: method specific parameters methods processes \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(50):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------10-------------------\n",
      "Multimedia applications targeting batteryoperated wearable devices must be poweraware to exploit the capabilities of variable voltage processors. This paper presents a feedback (FB) controller for video decoding that regulates the voltage for individual frames. The decoding complexity of (parts of) individual frames is estimated using a simple frame length decoding time correlation obtained from statistics gathered on the target hardware (StrongARM processor). Experiments with a modified H.263 decoder show that the FB controller closely approaches ( 10%) the optimal case in which each frame is decoded at the minimal frequency/voltage. Furthermore, we observe that incorporating additional complexity information in the video stream will only be useful when the energy consumption of the (fixed) memory subsystem is significantly reduced.\n",
      "-------------\n",
      "A system for videoconferencing that offers, among other features, extremely low endtoend delay as well as very high scalability. The system accommodates heterogeneous receivers and networks, as well as the besteffort nature of networks such as those based on the Internet Protocol. The system relies on scalable video coding to provide a coded representation of a source video signal at multiple temporal, quality, and spatial resolutions. These resolutions are represented by distinct bitstream components that are created at each enduser encoder. System architecture and processes called SVC Thinning allow the separation of data into data used for prediction in other pictures and data not used for prediction in other pictures. SVC Thinning processes, which can be performed at video conferencing endpoints or at MCUs, can selectively remove or replace with fewer bits the data not used for prediction in other pictures from transmitted bit streams. This separation and selective removal or replacement of data for transmission allows a tradeoff between scalability support (i.e. number of decodable video resolutions), error resiliency and coding efficiency.\n",
      "-------------\n",
      "A user device having a plurality of microphones and an operating method thereof are provided. The user device includes a plurality of microphones for converting sounds into electrical signals, an analysis unit for analyzing acoustic characteristics of the electrical signals outputted from the plurality of microphones, a switch unit for electrically connecting a specific microphone and a host device according to the analysis result of the analysis unit, and the host device for executing a function using an electrical signal outputted from the specific microphone. Various other exemplary embodiments are possible.\n",
      "-------------\n",
      "A system and method for extracting acoustic features and speech activity on a device and transmitting them in a distributed voice recognition system. The distributed voice recognition system includes a local VR engine in a subscriber unit and a server VR engine on a server . The local VR engine comprises a feature extraction (FE) module that extracts features from a speech signal, and a voice activity detection module (VAD) that detects voice activity within a speech signal. The system includes filters, framing and windowing modules, power spectrum analyzers, a neural network, a nonlinear element, and other components to selectively provide an advanced front end vector including predetermined portions of the voice activity detection indication and extracted features from the subscriber unit to the server . The system also includes a module to generate additional feature vectors on the server from the received features using a feedforward multilayer perceptron (MLP) and providing the same to the speech server.\n",
      "-------------\n",
      "Systems and methods of encoding a video signal that includes a succession of images are disclosed. A system may include a plurality of independently programmable processing elements (PEs), an input interface device adapted to receive, buffer, and divide the input video signal in a manner appropriate to the plurality of PEs, and an output interface device adapted to receive encoded bitstreams generated by the plurality of PEs and provide an encoded video signal. Each PE is configurable to carry out the steps of a selected encoding algorithm and includes a digital processor and a memory in communication with the digital processor. The memories are independently accessible, and PEs communicate with each another during the encoding.\n",
      "-------------\n",
      "Standard deep neural networkbased acoustic models for automatic speech recognition (ASR) rely on handengineered input features, typically logmel filterbank magnitudes. In this paper, we describe a convolutional neural network  deep neural network (CNNDNN) acoustic model which takes raw multichannel waveforms as input, i.e. without any preceding feature extraction, and learns a similar feature representation through supervised training. By operating directly in the time domain, the network is able to take advantage of the signal's fine time structure that is discarded when computing filterbank magnitude features. This structure is especially useful when analyzing multichannel inputs, where timing differences between input channels can be used to localize a signal in space. The first convolutional layer of the proposed model naturally learns a filterbank that is selective in both frequency and direction of arrival, i.e. a bank of bandpass beamformers with an auditorylike frequency scale. When trained on data corrupted with noise coming from different spatial locations, the network learns to filter them out by steering nulls in the directions corresponding to the noise sources. Experiments on a simulated multichannel dataset show that the proposed acoustic model outperforms a DNN that uses logmel filterbank magnitude features under noisy and reverberant conditions.\n",
      "-------------\n",
      "A method for synthesizing a binaural audio signal, the method comprising: inputting a parametrically encoded audio signal comprising at least one combined signal of a plurality of audio channels and one or more corresponding sets of side information describing a multichannel sound image; and applying a predetermined set of headrelated transfer function filters to the at least one combined signal in proportion determined by the corresponding set of side information to synthesize a binaural audio signal. A corresponding parametric audio decoder, parametric audio encoder, computer program product, and apparatus for synthesizing a binaural audio signal are also described.\n",
      "-------------\n",
      "An audio signal encoding method and apparatus and an audio signal decoding method and apparatus are provided. The audio signal encoding method includes: transforming an audio signal into a signal of a frequency domain; extracting semantic information from the audio signal; variably reconfiguring one or more subbands included in the audio signal by segmenting or grouping the one or more subbands using the extracted semantic information; and generating a quantized bitstream by calculating a quantization step size and a scale factor with respect to a reconfigured subband of the one or more subbands.\n",
      "-------------\n",
      "Modern passive mobile radar systems equipped with circular arrays, require to elaborate simultaneously and in real time a certain number of passive signals. The signals are corresponding to, as an example, eight element of the circular array and ten carrier frequencies for each element in FM band. Further data are acquired if the system is intended for deploying at the same time DAB and/or DVBT signals. This sizing implies that the processing architecture has to sustain the load corresponding to, at least, more than ten adaptive cancellers (in time or space or both), pulse compressors, detection logics and bistatic and Cartesian trackers. Some of those algorithms map pretty well on GPU, thus giving a sizeable advantage in computational power with respect to standard Central Processing Unit (CPU)based devices. This paper contains a rationale for the selection of algorithms to be mapped on GPU, some rules followed for the mapping strategy and comparisons of computational time obtained with GPU and CPUbased devices operating on the same input data. A computational architecture is proposed that consists in a wise mixture of GPU and CPU devices that exploits, at their best, the available equipment.\n",
      "-------------\n",
      "An apparatus for synthesizing a rendered output signal having a first audio channel and a second audio channel includes a decorrelator stage for generating a decorrelator signal based on a downmix signal, and a combiner for performing a weighted combination of the downmix signal and a decorrelated signal based on parametric audio object information, downmix information and target rendering information. The combiner solves the problem of optimally combining matrixing with decorrelation for a high quality stereo scene reproduction of a number of individual audio objects using a multichannel downmix.\n",
      "-------------\n",
      "-------------------1-------------------\n",
      " The thousands of specialized structured file formats in use today present a substantial barrier to freely exchanging information between applications programs. In Chapter 8, we consider the problem of deducing such basic features as the whitespace characters, bracketing delimiter symbols, and selfdelimiter characters of a given file format from one or more example files. We demonstrate that for sufficiently large example files, we can typically identify the basic features of interest.\n",
      "-------------\n",
      " system should perform the translation and search in a single step without any user’s supervision.\n",
      "-------------\n",
      "The availability of large amounts of data is a fundamental prerequisite for building handwriting recognition systems. Any system needs a test set of labelled samples for measuring its performance along its development and guiding it. Moreover, there are systems that need additional samples for learning the recognition task they have to cope with later, i.e. a training set. Thus, the acquisition and distribution of standard databases has become an important issue in the handwriting recognition research community. Examples of widely used databases in the online domain are UNIPEN, IRONOFF, and Pendigits. This paper describes the current state of our own database, UJIpenchars, whose first version contains online representations of 1 364 isolated handwritten characters produced by 11 writers and is freely available at the UCI Machine Learning Repository. Moreover, we have recently concluded a second acquisition phase, totalling more than 11 000 samples from 60 writers to be made available in short as UJIpenchars2.\n",
      "-------------\n",
      "In this paper we study the problem of identifying systems that automatically inject nonpersonal messages in microblogging message streams, thus potentially biasing results of certain information extraction procedures, such as opinionmining and trend analysis. We also study several classes of features, namely features based on the time of posting, the client used to post, the presence of links, the user interaction and the writing style. This last class of features, that we introduce here for the first time, is proved to be a top performer, achieving accuracy near the 90%, on par with the best features previously used for this task.\n",
      "-------------\n",
      "Instance unification determines whether two instances in an ontology refer to the same object in the real world. More specifically, this paper addresses the instance unification problem for person names. The approach combines the use of citation information (i.e., abstract, initials, titles and coauthorship information) with web mining, in order to gather additional evidence for the instance unification algorithm. The method is evaluated on two datasets – one from the BT digital library and one used in previous work on name disambiguation. The results show that the information mined from the web contributes substantially towards the successful handling of highly ambiguous cases which lowered the performance of previous methods.\n",
      "-------------\n",
      "This paper describes a novel statistical namedentity (i.e. \"proper name\") recognition system built around a maximum entity framework. By working v,ithin the framework of maximum entropy theory and utilizing a flexible objectbased architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multiword terms. The purely statistical system contains no handgenerated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thusfar published. 1 I N T R O D U C T I O N Named entity recognition is one of the simplest of the common message understanding tasks. The objective is to identify and categorize all members of certain categories of \"proper names\" from a given corpus. The specific test bed which will be the subject of this paper is that of the Seventh Message Understanding Conference (MUC7), in which the task was to identify \"names\" falling into one of seven categories: person, organization, location, date, time, percentage, and monetary amount. This paper describes a new system called \"Maximum Entropy Named Entity\" or \"MENE\" (pronounced \"meanie\"). By working within the framework of maximum entropy theory and utilizing a flexible objectbased architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decision. These knowledge sources include capitalization features, lexical features, and features indicating the current section of text. It makes use of a broad array of dictionaries of useful single or multiword terms such as first names, company names, and corporate suffixes, and automatically handles cases where words are in more than one dictionary. Our dictio152 naries required no manual editing and were either downloaded from the web or were simply \"obvious\" lists entered by hand. This system, built from offtheshelf knowledge sources, contained no handgenerated pat terns and achieved a result which is comparable with that of the best statistical systems. Further experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thusfar reported by any system on a MUC evaluation. Given appropriate training data, we believe that this system is highly portable to other domains and languages and have already achieved good results on uppercase English. We also feel that there are plenty of avenues to explore in enhancing the system's performance on Englishlanguage newspaper\n",
      "-------------\n",
      "We describe the use of a recommender system to enable continuous knowledge acquisition and individualized tutoring of application software across an organization. Installing such systems will result in the capture of evolving expertise and in organizationwide learning (OWL). We present the results of a yearlong naturalistic inquiry into application’s usage patterns, based on logging users’ actions. We analyze the data to develop user models, individualized expert models, confidence intervals, and instructional indicators. We show how this information could be used to tutor users. Introduction Recommender Systems typically help people select products, services, and information. A novel application of recommender systems is to help individuals select ’what to learn next’ by recommending knowledge that their peers have found useful. For example, people typically utilize only a small portion of a software application’s functionality (one study shows users applying less than 10% of Microsoft Word’s commands). A recommender system can unobtrusively note which portions of an application’s functionality that the members of an organization find useful, group the organization’s members into sets of similar users, or peers (based on similar demographic factors such as job title, or similarities in command usage patterns), and produce recommendations for learning that are specific to the individual in the context of his/her organization, peers, and current activities. This paper reports research on a recommender system (Resnick & Varian, 1997) intended to promote gradual but perpetual performance improvement in the use of application software. We present our rationale, an analysis of a year’s collected data, and a vision of how users might learn from the system. We have worked with one commercial application, and believe our approach is generally applicable. The research explores the potential of a new sort of user modeling based on summaries of logged user data. This method of user modeling enables the observation of a large number of users over a long period of time, enables concurrent development of student models and individualized expert models, and applies recommender system techniques to onthejob instruction. Earlier work is reported in Linton (1990), and Linton (1996). Kay and Thomas (1995), Thomas (1996) report on related work with a text editor in an academic environment. A recommender system to enhance the organizationwide learning of application software is a means of promoting organizational learning (Senge, 1990). By pooling and sharing expertise, recommender systems augment and assist the natural social process of people learning from each other. This approach is quite distinct from systems, such as Microsoft’s Office Assistant, which recommend new commands based on their logical equivalence to the lessefficient way a user may be performing a task. The system presented here will (1) capture evolving expertise from community of practice (Lave & Wenger 1991), (2) support lessskilled members of the community in acquiring expertise, and (3) serve as an organizational memory for the expertise it captures. In many workplaces ... mastery is in short supply and what is required is a kind of collaborative bootstrapping of expertise. (Eales & Welch, 1995, p. 100) The main goal of the approach taken in this work is to continuously improve the performance of application users by providing individualized modeling and coaching based on the automated comparison of user models to expert models. The system described here would be applicable in any situation where a number of application users perform similar tasks on networked computers 65 From: AAAI Technical Report WS9808. Compilation copyright © 1998, AAAI (www.aaai.org). All rights reserved. In the remainder of this section we describe the logging process and make some initial remarks about modeling and coaching software users. We then present an analysis of the data we have logged and our process of creating individual models of expertise. In the final section we describe further work and close with a summary. Each time a user issues a Word command such as Cut or Paste, the command is written to the log, together with a time stamp, and then executed. The logger, called OWL for OrganizationWide Learning, comes up when the user opens Word; it creates a separate log for each file the user edits, and when the user quits Word, it sends the logs to a server where they are periodically loaded into a database for analysis. A toolbar button labeled ’OWL is ON’ (or OFF) informs users of OWL’s tate and gives them control. Individual models of expertise We have selected the Edit commands for further analysis. A similar analysis could be performed for each type of command. The first of the three tables in Figure 1 presents data on the Edit commands for each of our 16 users. In the table, each column contains data for one user and each row contains data for one command (Edit commands that were not used have been omitted). A cell then, contains the count of the number of times the individual has used the command. The columns have been sorted so that the person using the most commands is on the left and the person using the fewest is on the right. Similarly, the rows have been sorted so that the most frequently used command is in the top row and the least frequently used command is in the bottom row. Consequently the cells with the largest values are in the upper left corner and those with the smallest values are in the lower right comer. The table has been shaded to make the contours of the numbers visible: the largest numbers have the darkest shading and the smallest numbers have no shading, each shade indicates an order of magnitude. Inspection of the first table reveals that users tend to acquire the Edit commands in a specific sequence, i.e., those that know fewer commands know a subset of the commands used by their moreknowledgeable peers. If instead, users acquired commands in an idiosyncratic order, the data would not sort as it does. And if they acquired commands in a manner that strongly reflected their job tasks or their writing tasks, there would be subgroups of users who shared common commands. Also, the moreknowledgeable users do not replace commands learned early on with more powerful commands, but instead keep adding new commands to their repertoire. Finally, the sequence of command acquisition corresponds to the commands’ frequency of use. While this last point is not necessarily a surprise, neither is it a given. There are some peaks and valleys in the data as sorted, and a fairly rough edge where commands transition from being used rarely to being used not at all. These peaks, valleys, and rough edges may represent periods of repetitive tasks or lack of data, respectively, or they may represent overdependence on some command that has a more powerful substitute or ignorance of a command or of a task (a sequence of commands) that uses the command. In other words, some of the peaks, valleys, and rough edges may represent opportunities to learn more effective use of the software. In the second table in Figure 1 the data have been smoothed. The observed value in each cell has been replaced by an expected value, the most likely value for the cell, using a method taken from statistics, based on the row, column and grand totals for the table (Howell, 1982). In the case of software use, the row effect is the overall relative utility of the command (for all users) and the column effect is the usage of related commands by the individual user. The expected value is the usage the command would have if the individual used it in a manner consistent with his/her usage of related commands and consistent with his/her peers’ usage of the command. These expected values are a new kind of expert model, one that is unique to each individual and each moment in time; the expected value in each cell reflects the individual’s use of related commands, and one’s peers’ use of the same command. The reason for differences between observed and expected values, between one’s actual and expert model, might have several explanations such as the individual’s tasks, preferences, experiences, or hardware, but we are most interested when the difference indicates the lack of knowledge or skill.\n",
      "-------------\n",
      "Nowadays with the proliferation of smartphones and tablets on the market, almost everyone has access to mobile devices that offer better processing capabilities and access to new information and services. The Web is undoubtedly the best tool for sharing content, especially through social networks. One of the most useful information that can be extracted is the geographical one. Current navigation systems lack in several ways to satisfy the need to process and reason upon such volumes of data, namely, to accurately provide information about urban traffic in realtime and the possibility to personalize the information used by such systems. This paper describes an approach to integrate and fuse tweet messages from traffic agencies in UK, with the objective of detecting the geographical focus of traffic events. Tweet messages are considered in this work given its uniqueness, the real time nature of tweets which may be used to quickly detect a traffic event and its simplicity; it only cost 140 characters to generate a message (called “tweet”) for any user. The approach presented here is composed by several steps: tweet classification, event type classification, name entity recognition, geolocation and event tracking. Finally, we do an experimental study on a real dataset composed by traffic related tweet messages to access the accuracy of proposed approach. We present some inaccuracies ranging from lack of geographical information, imprecise and ambiguous toponyms, overlaps and repetitions as well as visualization to our data set in UK. We finally give an outlook into potential corrections. The work presented here is still part of ongoing work. Results achieved so far do not address the final conclusions but form the basis for the formalization of a domain knowledge along with the services.\n",
      "-------------\n",
      "The world wide web is growing continuously and rapidly; it is quickly facilitating the migration of tasks of the daily life into webbased. This trend shows time will come when everyone is forced to use the web for daily activities. Naive users arc the major concern of such a shift; so, it is necessary to have the web ready to serve them. We argue that this requires well optimized websites for users to quickly locate the information they arc looking for. This, on the other hand, becomes more and more important due to the widespread reliance on the many services available on the Internet nowadays. It is true that search engines can facilitate the task of finding the information one is looking for. However, search engines will never replace but do complement the optimization of a website’s internal structure based on previously recorded user behavior. In this chapter, wc will present a novel approach for identifying problematic structures in websites. This method consists of two phases. The first phase compares user behavior, derived via web log mining techniques, to a combined analysis of the website’s link structure obtained by applying three methods leading to more robust framework and hence strong and consistent outcome: (1) constructing and analyzing a social network of the pages constituting the website by considering both the structure and the usage information; (2) applying the Weighted PageRank algorithm; and (3) applying the Hypertext Induced Topic Selection (HITS) method. In the second phase, we use the term frequencyinverse document frequency (TFIDF) measure to investigate further the correlation between the page that contains the link and the linked to pages in order to further support the findings of the first phase of our approach. We will then show how to use these intermediate results in order to point out problematic website structures to the website owner.\n",
      "-------------\n",
      "This study aims at developing a news surveillance system able to address multilingual web corpora. As an example of a domain where multilingual capacity is crucial, we focus on Epidemic Surveillance. This task necessitates worldwide coverage of news in order to detect new events as quickly as possible, anywhere, whatever the language it is first reported in. In this study, textgenre is used rather than sentence analysis. The newsgenre properties allow us to assess the thematic relevance of news, filtered with the help of a specialised lexicon that is automatically collected on Wikipedia. Afterwards, a more detailed analysis of text specific properties is applied to relevant documents to better characterize the epidemic event (i.e., which disease spreads where?). Results from 400 documents in each language demonstrate the interest of this multilingual approach with light resources. DAnIEL achieves an F 1measure score around 85%. Two issues are addressed: the first is morphology rich languages, e.g. Greek, Polish and Russian as compared to English. The second is event location detection as related to disease detection. This system provides a reliable alternative to the generic IE architecture that is constrained by the lack of numerous components in many languages.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for test_cluster in [10,1]:\n",
    "    print(f\"-------------------{test_cluster}-------------------\")\n",
    "    most_representative_docs = np.argsort(\n",
    "        np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    "    )\n",
    "    for d in most_representative_docs[:10]:\n",
    "        print(df[\"text\"][d])\n",
    "        print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41603511,  0.02993672,  0.08477567, ...,  0.42157972,\n",
       "         0.05389575,  0.08879889],\n",
       "       [ 0.2547176 ,  0.12962069,  0.05133368, ...,  0.41265002,\n",
       "         0.03695938,  0.02680716],\n",
       "       [ 0.21775689,  0.58536683, -0.57444827, ..., -0.38275401,\n",
       "         0.88557918,  1.01202863],\n",
       "       ...,\n",
       "       [ 0.32258523,  0.02653912,  0.13975534, ...,  0.44815915,\n",
       "         0.07009276,  0.15324835],\n",
       "       [ 0.39768368, -0.00493577,  0.35454605, ...,  0.29803471,\n",
       "         0.02942713,  0.12025491],\n",
       "       [ 0.38876871, -0.02372196,  0.03457274, ...,  0.50404936,\n",
       "         0.14686806,  0.10216383]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Clusters.npy\",clustering.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clustering.predict(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame()\n",
    "df_clusters[\"id\"] = data[:,0]\n",
    "df_clusters[\"prediction\"] = predictions\n",
    "pd.DataFrame(df_clusters).to_csv(\"Clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c77158d38858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a27dfe8365d0>\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(file_str)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnew_file_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_file_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a27dfe8365d0>\u001b[0m in \u001b[0;36mget_id\u001b[0;34m(file_str)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnew_file_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from json import JSONDecodeError\n",
    "for text in texts:\n",
    "    try:\n",
    "        j = get_json(text)\n",
    "    except JSONDecodeError:\n",
    "        print(text)\n",
    "        break\n",
    "    if j['IndexLength']==0:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeb980a7680441eac42ec2d85ab9999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=624181.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files_str = [get_descritpion(file_str) for file_str in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=files_str,size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'min_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0fc73ed17709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'min_count'"
     ]
    }
   ],
   "source": [
    "model.train(sentences=files_str, total_examples=len(files_str), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |  \n",
      " |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |  \n",
      " |  Once you're finished training a model (=no more updates, only querying)\n",
      " |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      " |  \n",
      " |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |  \n",
      " |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |  \n",
      " |  Some important attributes are the following:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      " |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      " |  \n",
      " |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      " |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      " |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      " |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      " |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use `self.wv.__contains__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |      Deprecated. Use `self.wv.accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      " |      \n",
      " |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      " |      Discard parameters that are used in training and scoring, to save memory.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Use only if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace_word_vectors_with_normalized : bool, optional\n",
      " |          If True, forget the original (not normalized) word vectors and only keep\n",
      " |          the L2-normalized word vectors, to save even more memory.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Deprecated. Use `self.wv.init_sims` instead.\n",
      " |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of str\n",
      " |          List of context words.\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      " |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      " |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated, use self.wv.doesnt_match() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated, use self.wv.most_similar() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated, use self.wv.n_similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_word() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated, use self.wv.similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated, use self.wv.wmdistance() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vecPaperDescription.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maximebonnin/Notebooks/3A Notebook/INF554/INF554_Kaggle_Project/Exploration'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/maximebonnin/Notebooks/3A Notebook/INF554/INF554_Kaggle_Project/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 217801\n",
      "Number of edges: 1718164\n"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]\n",
    "\n",
    "# load the graph    \n",
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges() \n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)\n",
    "\n",
    "\n",
    "# computes structural features for each node\n",
    "core_number = nx.core_number(G)\n",
    "\n",
    "# create the training matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number \n",
    "X_train = np.zeros((n_train, 2))\n",
    "y_train = np.zeros(n_train)\n",
    "for i,row in df_train.iterrows():\n",
    "    node = row['author']\n",
    "    X_train[i,0] = G.degree(node)\n",
    "    X_train[i,1] = core_number[node]\n",
    "    y_train[i] = row['hindex']\n",
    "\n",
    "# create the test matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number\n",
    "X_test = np.zeros((n_test, 2))\n",
    "for i,row in df_test.iterrows():\n",
    "    node = row['author']\n",
    "    X_test[i,0] = G.degree(node)\n",
    "    X_test[i,1] = core_number[node]\n",
    "    \n",
    "# train a regression model and make predictions\n",
    "reg = Lasso(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# write the predictions to file\n",
    "df_test['hindex'] = pd.Series(np.round_(y_pred, decimals=3))\n",
    "\n",
    "\n",
    "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(G, node_size=10)\n",
    "plt.title(\"Raw graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"data/abstracts.txt\") as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8289804\n",
      "{'IndexLength': 82, 'InvertedIndex': {'For': [0], 'the': [1, 48, 51, 73], 'feature': [2], 'analysis': [3], 'of': [4, 47, 50, 65, 71], 'vector': [5, 11, 23, 30, 59], 'fields': [6, 60], 'we': [7], 'decompose': [8], 'a': [9, 16, 18, 21, 29, 33, 79], 'given': [10], 'field': [12, 31], 'into': [13], 'three': [14], 'components:': [15], 'divergence-free,': [17], 'rotation-free,': [19], 'and': [20, 36, 42, 63, 78], 'harmonic': [22], 'field.': [24], 'This': [25], 'Hodge-type': [26], 'decomposition': [27], 'splits': [28], 'using': [32], 'variational': [34], 'approach,': [35], 'allows': [37], 'to': [38, 56, 75], 'locate': [39], 'sources,': [40], 'sinks,': [41], 'vortices': [43], 'as': [44], 'extremal': [45], 'points': [46], 'potentials': [49], 'components.': [52], 'Our': [53], 'method': [54, 74], 'applies': [55], 'discrete': [57], 'tangential': [58], 'on': [61], 'surfaces,': [62], 'is': [64], 'global': [66], 'nature.': [67], 'Results': [68], 'are': [69], 'presented': [70], 'applying': [72], 'test': [76], 'cases': [77], 'CFD': [80], 'flow.': [81]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'paper,',\n",
       " 'we',\n",
       " 'describe',\n",
       " 'a',\n",
       " 'new',\n",
       " 'bitmap',\n",
       " 'indexing',\n",
       " 'technique',\n",
       " 'to',\n",
       " 'cluster',\n",
       " 'XML',\n",
       " 'documents.',\n",
       " 'XML',\n",
       " 'is',\n",
       " 'a',\n",
       " 'new',\n",
       " 'standard',\n",
       " 'for',\n",
       " 'exchanging',\n",
       " 'and',\n",
       " 'representing',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Internet.',\n",
       " 'Documents',\n",
       " 'can',\n",
       " 'be',\n",
       " 'hierarchically',\n",
       " 'represented',\n",
       " 'by',\n",
       " 'XML-elements.',\n",
       " 'XML',\n",
       " 'documents',\n",
       " 'are',\n",
       " 'represented',\n",
       " 'and',\n",
       " 'indexed',\n",
       " 'using',\n",
       " 'a',\n",
       " 'bitmap',\n",
       " 'indexing',\n",
       " 'technique.',\n",
       " 'We',\n",
       " 'define',\n",
       " 'the',\n",
       " 'similarity',\n",
       " 'and',\n",
       " 'popularity',\n",
       " 'operations',\n",
       " 'available',\n",
       " 'in',\n",
       " 'bitmap',\n",
       " 'indexes',\n",
       " 'and',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'method',\n",
       " 'for',\n",
       " 'partitioning',\n",
       " 'a',\n",
       " 'XML',\n",
       " 'document',\n",
       " 'set.',\n",
       " 'Furthermore,',\n",
       " 'a',\n",
       " '2-dimensional',\n",
       " 'bitmap',\n",
       " 'index',\n",
       " 'is',\n",
       " 'extended',\n",
       " 'to',\n",
       " 'a',\n",
       " '3dimensional',\n",
       " 'bitmap',\n",
       " 'index,',\n",
       " 'called',\n",
       " 'BitCube.',\n",
       " 'We',\n",
       " 'define',\n",
       " 'statistical',\n",
       " 'measurements',\n",
       " 'in',\n",
       " 'the',\n",
       " 'BitCube:',\n",
       " 'mean,',\n",
       " 'mode,',\n",
       " 'standard',\n",
       " 'derivation,',\n",
       " 'and',\n",
       " 'correlation',\n",
       " 'coefficient.',\n",
       " 'Based',\n",
       " 'on',\n",
       " 'these',\n",
       " 'measurements,',\n",
       " 'we',\n",
       " 'also',\n",
       " 'define',\n",
       " 'the',\n",
       " 'slice,',\n",
       " 'project,',\n",
       " 'and',\n",
       " 'dice',\n",
       " 'operations',\n",
       " 'on',\n",
       " 'a',\n",
       " 'BitCube.',\n",
       " 'BitCube',\n",
       " 'can',\n",
       " 'be',\n",
       " 'manipulated',\n",
       " 'efficiently',\n",
       " 'and',\n",
       " 'improves',\n",
       " 'the',\n",
       " 'performance',\n",
       " 'of',\n",
       " 'document',\n",
       " 'retrieval.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(get_id(texts[1000]))\n",
    "print(get_json(texts[1000]))\n",
    "get_descritpion(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-15d36fd2391f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_descritpion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-15d36fd2391f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_descritpion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-e00f98af3848>\u001b[0m in \u001b[0;36mget_descritpion\u001b[0;34m(file_str)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_descritpion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IndexLength\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-e00f98af3848>\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(file_str)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_descritpion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "files_str = [get_descritpion(file_str) for file_str in texts]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "287892eb55904b50337f16055d7cf7eaa1f5a3cda4903bb2a548d1e286ec920a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
