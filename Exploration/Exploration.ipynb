{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maximebonnin/Notebooks/3A Notebook/INF554/INF554_Kaggle_Project/Exploration'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/maximebonnin/Notebooks/3A Notebook/INF554/INF554_Kaggle_Project/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 217801\n",
      "Number of edges: 1718164\n"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]\n",
    "\n",
    "# load the graph    \n",
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges() \n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)\n",
    "\n",
    "\n",
    "# computes structural features for each node\n",
    "core_number = nx.core_number(G)\n",
    "\n",
    "# create the training matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number \n",
    "X_train = np.zeros((n_train, 2))\n",
    "y_train = np.zeros(n_train)\n",
    "for i,row in df_train.iterrows():\n",
    "    node = row['author']\n",
    "    X_train[i,0] = G.degree(node)\n",
    "    X_train[i,1] = core_number[node]\n",
    "    y_train[i] = row['hindex']\n",
    "\n",
    "# create the test matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number\n",
    "X_test = np.zeros((n_test, 2))\n",
    "for i,row in df_test.iterrows():\n",
    "    node = row['author']\n",
    "    X_test[i,0] = G.degree(node)\n",
    "    X_test[i,1] = core_number[node]\n",
    "    \n",
    "# train a regression model and make predictions\n",
    "reg = Lasso(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# write the predictions to file\n",
    "df_test['hindex'] = pd.Series(np.round_(y_pred, decimals=3))\n",
    "\n",
    "\n",
    "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(G, node_size=10)\n",
    "plt.title(\"Raw graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "os.chdir(\"/Users/maximebonnin/Notebooks/3A Notebook/INF554/INF554_Kaggle_Project/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maximebonnin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/abstracts.txt\") as file:\n",
    "    texts = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624181"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3603----{\"IndexLength\":122,\"InvertedIndex\":{\"In\":[0],\"this\":[1],\"paper,\":[2],\"we\":[3,98],\"describe\":[4],\"a\":[5,16,41,58,62,67,74,108],\"new\":[6,17],\"bitmap\":[7,42,54,69,76],\"indexing\":[8,43],\"technique\":[9],\"to\":[10,73],\"cluster\":[11],\"XML\":[12,14,34,63],\"documents.\":[13],\"is\":[15,71],\"standard\":[18,89],\"for\":[19,60],\"exchanging\":[20],\"and\":[21,38,49,56,91,104,115],\"representing\":[22],\"information\":[23],\"on\":[24,95,107],\"the\":[25,47,85,101,117],\"Internet.\":[26],\"Documents\":[27],\"can\":[28,111],\"be\":[29,112],\"hierarchically\":[30],\"represented\":[31,37],\"by\":[32],\"XML-elements.\":[33],\"documents\":[35],\"are\":[36],\"indexed\":[39],\"using\":[40],\"technique.\":[44],\"We\":[45,80],\"define\":[46,81,100],\"similarity\":[48],\"popularity\":[50],\"operations\":[51,106],\"available\":[52],\"in\":[53,84],\"indexes\":[55],\"propose\":[57],\"method\":[59],\"partitioning\":[61],\"document\":[64,120],\"set.\":[65],\"Furthermore,\":[66],\"2-dimensional\":[68],\"index\":[70],\"extended\":[72],\"3dimensional\":[75],\"index,\":[77],\"called\":[78],\"BitCube.\":[79,109],\"statistical\":[82],\"measurements\":[83],\"BitCube:\":[86],\"mean,\":[87],\"mode,\":[88],\"derivation,\":[90],\"correlation\":[92],\"coefficient.\":[93],\"Based\":[94],\"these\":[96],\"measurements,\":[97],\"also\":[99],\"slice,\":[102],\"project,\":[103],\"dice\":[105],\"BitCube\":[110],\"manipulated\":[113],\"efficiently\":[114],\"improves\":[116],\"performance\":[118],\"of\":[119],\"retrieval.\":[121]}}\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_str = texts[0]\n",
    "print(type(file_str))\n",
    "file_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(file_str):\n",
    "    return int(file_str.split(\"----\")[0])\n",
    "\n",
    "def get_json(file_str):\n",
    "    new_file_str = file_str.replace(\"-\",\"\")\n",
    "    json_str = new_file_str.split(str(get_id(file_str)))[-1]\n",
    "\n",
    "    return json.loads(json_str)\n",
    "\n",
    "def get_descritpion(file_str):\n",
    "\n",
    "    json_file = get_json(file_str)\n",
    "    words = [\"\"]*int(json_file[\"IndexLength\"])\n",
    "\n",
    "    for word in json_file[\"InvertedIndex\"].keys():\n",
    "        indexes = json_file[\"InvertedIndex\"][word]\n",
    "\n",
    "        for index in indexes:\n",
    "            words[int(index)] = word\n",
    "        \n",
    "        \n",
    "    return words #' '.join(words).replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def get_line(file_str):\n",
    "    \n",
    "    words = get_descritpion(file_str)\n",
    "\n",
    "    return ' '.join(words).replace(\"\\n\", \" \")\n",
    "\n",
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in tqdm(list_of_docs):\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = word_tokenize\n",
    "stpwds = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fucking', 0.5291999578475952),\n",
       " ('arsehole', 0.43101203441619873),\n",
       " ('fuck', 0.43084827065467834),\n",
       " ('nigger', 0.4102444350719452),\n",
       " ('no-no', 0.4088124930858612),\n",
       " ('whitesnake', 0.40676942467689514),\n",
       " ('cheater', 0.40527766942977905),\n",
       " ('extractions', 0.404652863740921),\n",
       " ('earlobes', 0.4043155610561371),\n",
       " ('hymen', 0.40006449818611145)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796476bdb73746c68e609493fd79493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[ get_id(file_str), get_line(file_str)] for i,file_str in tqdm(enumerate(texts))]#[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, columns=[\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] =  df[\"text\"].map(lambda x: clean_text(x, tokenizer, stpwds))\n",
    "tokenized_docs = df[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [paper, describe, new, bitmap, indexing, techn...\n",
       "1         [paper, starts, observation, inclusionbased, a...\n",
       "2         [contribution, describes, approach, integrate,...\n",
       "3         [cleaneval, shared, task, competitive, evaluat...\n",
       "4         [xax, browser, plugin, model, enables, develop...\n",
       "                                ...                        \n",
       "624176    [xray, polarimetry, sometimes, alone, sometime...\n",
       "624177    [recent, years, underwater, wireless, sensor, ...\n",
       "624178    [todays, cyber, physical, systems, cps, well, ...\n",
       "624179    [software, service, cloud, computing, model, f...\n",
       "624180    [penetration, testing, wellestablished, practi...\n",
       "Name: tokens, Length: 624181, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.6594e-01,  6.6349e-01,  2.6309e-01, -5.4793e-01,  1.0673e+00,\n",
       "       -9.0648e-02, -1.8309e-01,  4.4575e-01, -4.4378e-01,  2.6572e-01,\n",
       "       -6.3668e-02,  3.0498e-01, -4.1506e-01, -2.9739e-01, -1.8309e-01,\n",
       "        8.4494e-02,  3.8472e-01, -9.2856e-01,  4.4563e-01,  3.2183e-01,\n",
       "        3.8717e-01,  1.6766e+00,  3.8729e-01,  3.9076e-01,  6.2466e-01,\n",
       "        1.4462e-01, -6.5783e-02, -4.3507e-01,  8.1413e-01,  5.2090e-02,\n",
       "        2.5516e-01, -3.9658e-02, -5.0672e-01, -2.9341e-01,  2.3707e-01,\n",
       "        1.2230e-01, -1.1576e-01, -2.7695e-01,  4.1394e-01,  4.3311e-02,\n",
       "        4.8973e-01, -4.4549e-01, -6.1259e-02,  1.9547e-02,  5.2209e-01,\n",
       "        9.1658e-01,  3.5733e-01, -4.9494e-01, -9.8224e-01,  2.0034e-01,\n",
       "       -3.0868e-02, -1.7801e-02, -2.3831e-01,  2.8774e-01,  7.4100e-02,\n",
       "        1.5456e-01,  3.3661e-01,  3.4376e-01, -5.9647e-01, -9.4255e-02,\n",
       "       -3.4176e-01,  2.5309e-02, -2.3511e-02,  1.2011e+00, -2.1605e-01,\n",
       "       -4.8390e-01,  4.3511e-01,  5.4461e-01,  1.2633e+00,  2.5917e-01,\n",
       "       -1.1421e-01,  5.3720e-01, -5.4331e-01, -3.6742e-01, -5.7027e-01,\n",
       "       -1.7175e-01, -6.8733e-01,  9.8460e-01,  2.4515e-01, -2.2371e-01,\n",
       "        9.7174e-01, -4.1807e-02, -1.1784e+00,  1.3977e-01,  4.6078e-01,\n",
       "       -1.9632e-01, -7.4474e-01,  2.5423e-02,  8.0462e-01, -3.3450e-01,\n",
       "       -2.1865e-01, -6.5424e-01,  2.1113e-01,  1.8232e-01,  4.2931e-01,\n",
       "       -8.7038e-01,  4.1972e-01, -4.1389e-01,  7.1248e-01,  2.2949e-01,\n",
       "       -1.2032e-01, -8.5783e-01, -8.0654e-01,  8.9839e-01,  4.8607e-01,\n",
       "        1.3298e-01, -1.2310e+00,  3.1057e-01, -5.6629e-02,  5.4886e-01,\n",
       "        6.1032e-01,  7.4684e-01,  3.4069e-01, -2.3033e-02, -1.2998e+00,\n",
       "        1.6497e-01, -1.8836e-01, -1.3293e-01, -2.5130e-01, -1.8927e-01,\n",
       "       -1.5952e-01,  3.7192e-04, -9.2012e-01,  3.6893e-02, -5.1207e-01,\n",
       "       -3.4395e-01, -2.7056e-01, -8.3500e-02,  3.0654e-01, -1.5095e-01,\n",
       "       -4.2247e-02, -8.6109e-01, -7.7691e-01,  1.9575e-01, -2.8952e-01,\n",
       "        3.4186e-01, -1.3374e-01,  4.6298e-01,  1.9810e-01,  6.7440e-02,\n",
       "       -2.0761e-01, -3.5577e-02, -2.9432e-01, -2.1170e-01,  3.6431e-01,\n",
       "        8.5816e-03, -1.0175e-02, -1.6175e-01, -2.2964e-01, -6.4495e-01,\n",
       "        9.1279e-01, -9.2916e-02, -1.1923e-01, -2.3787e-01,  1.5188e-01,\n",
       "       -2.8293e-02, -1.1462e-01, -8.6337e-02,  4.8884e-01, -2.5865e-01,\n",
       "        4.4624e-01, -4.1583e-01,  1.3673e-01,  2.5084e-02, -1.7666e-01,\n",
       "       -7.1640e-02, -3.2530e-01, -4.0709e-02, -8.5925e-01, -6.6533e-02,\n",
       "        3.9017e-01,  9.4013e-02,  2.3947e-01,  3.3033e-01,  1.0883e+00,\n",
       "       -1.3063e-01, -7.3562e-01,  4.3786e-01,  2.6336e-01,  1.0088e+00,\n",
       "        1.9219e-01,  5.0724e-01, -1.6973e-01,  9.2276e-01, -1.7674e-02,\n",
       "       -3.0621e-01, -1.5001e-01, -6.2389e-01, -1.6403e-01, -5.3258e-01,\n",
       "        2.1697e-01,  1.2172e-01,  3.5392e-01,  5.4564e-01,  1.8090e-01,\n",
       "       -6.6867e-01,  4.1631e-01,  1.8434e-01,  2.0702e-01, -4.7242e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector(\"math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': <gensim.models.keyedvectors.Vocab at 0x7fe710455a10>,\n",
       " ',': <gensim.models.keyedvectors.Vocab at 0x7fe71045b8d0>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x7fe710455a50>,\n",
       " 'of': <gensim.models.keyedvectors.Vocab at 0x7fe71045b7d0>,\n",
       " 'to': <gensim.models.keyedvectors.Vocab at 0x7fe710455ad0>,\n",
       " 'and': <gensim.models.keyedvectors.Vocab at 0x7fe71045b790>,\n",
       " 'in': <gensim.models.keyedvectors.Vocab at 0x7fe710455b90>,\n",
       " 'a': <gensim.models.keyedvectors.Vocab at 0x7fe71045b690>,\n",
       " '\"': <gensim.models.keyedvectors.Vocab at 0x7fe710455bd0>,\n",
       " \"'s\": <gensim.models.keyedvectors.Vocab at 0x7fe71045b650>,\n",
       " 'for': <gensim.models.keyedvectors.Vocab at 0x7fe710455c10>,\n",
       " '-': <gensim.models.keyedvectors.Vocab at 0x7fe71045b5d0>,\n",
       " 'that': <gensim.models.keyedvectors.Vocab at 0x7fe710455c50>,\n",
       " 'on': <gensim.models.keyedvectors.Vocab at 0x7fe71045b2d0>,\n",
       " 'is': <gensim.models.keyedvectors.Vocab at 0x7fe710455cd0>,\n",
       " 'was': <gensim.models.keyedvectors.Vocab at 0x7fe71045b350>,\n",
       " 'said': <gensim.models.keyedvectors.Vocab at 0x7fe710455d50>,\n",
       " 'with': <gensim.models.keyedvectors.Vocab at 0x7fe71045b550>,\n",
       " 'he': <gensim.models.keyedvectors.Vocab at 0x7fe710455dd0>,\n",
       " 'as': <gensim.models.keyedvectors.Vocab at 0x7fe71045ba50>,\n",
       " 'it': <gensim.models.keyedvectors.Vocab at 0x7fe710455e50>,\n",
       " 'by': <gensim.models.keyedvectors.Vocab at 0x7fe71045bf90>,\n",
       " 'at': <gensim.models.keyedvectors.Vocab at 0x7fe710455ed0>,\n",
       " '(': <gensim.models.keyedvectors.Vocab at 0x7fe71045bf10>,\n",
       " ')': <gensim.models.keyedvectors.Vocab at 0x7fe710455f10>,\n",
       " 'from': <gensim.models.keyedvectors.Vocab at 0x7fe71045bed0>,\n",
       " 'his': <gensim.models.keyedvectors.Vocab at 0x7fe710455f90>,\n",
       " \"''\": <gensim.models.keyedvectors.Vocab at 0x7fe71045bd50>,\n",
       " '``': <gensim.models.keyedvectors.Vocab at 0x7fe71045a050>,\n",
       " 'an': <gensim.models.keyedvectors.Vocab at 0x7fe71045bcd0>,\n",
       " 'be': <gensim.models.keyedvectors.Vocab at 0x7fe71045a0d0>,\n",
       " 'has': <gensim.models.keyedvectors.Vocab at 0x7fe71045bc10>,\n",
       " 'are': <gensim.models.keyedvectors.Vocab at 0x7fe71045a150>,\n",
       " 'have': <gensim.models.keyedvectors.Vocab at 0x7fe71045bb90>,\n",
       " 'but': <gensim.models.keyedvectors.Vocab at 0x7fe71045a1d0>,\n",
       " 'were': <gensim.models.keyedvectors.Vocab at 0x7fe71041e9d0>,\n",
       " 'not': <gensim.models.keyedvectors.Vocab at 0x7fe71045a250>,\n",
       " 'this': <gensim.models.keyedvectors.Vocab at 0x7fe7788db590>,\n",
       " 'who': <gensim.models.keyedvectors.Vocab at 0x7fe71045a2d0>,\n",
       " 'they': <gensim.models.keyedvectors.Vocab at 0x7fe710455050>,\n",
       " 'had': <gensim.models.keyedvectors.Vocab at 0x7fe71045a350>,\n",
       " 'i': <gensim.models.keyedvectors.Vocab at 0x7fe7104550d0>,\n",
       " 'which': <gensim.models.keyedvectors.Vocab at 0x7fe71045a390>,\n",
       " 'will': <gensim.models.keyedvectors.Vocab at 0x7fe710455150>,\n",
       " 'their': <gensim.models.keyedvectors.Vocab at 0x7fe71045a410>,\n",
       " ':': <gensim.models.keyedvectors.Vocab at 0x7fe7104551d0>,\n",
       " 'or': <gensim.models.keyedvectors.Vocab at 0x7fe71045a450>,\n",
       " 'its': <gensim.models.keyedvectors.Vocab at 0x7fe710455250>,\n",
       " 'one': <gensim.models.keyedvectors.Vocab at 0x7fe71045a4d0>,\n",
       " 'after': <gensim.models.keyedvectors.Vocab at 0x7fe7104552d0>,\n",
       " 'new': <gensim.models.keyedvectors.Vocab at 0x7fe71045a550>,\n",
       " 'been': <gensim.models.keyedvectors.Vocab at 0x7fe710455350>,\n",
       " 'also': <gensim.models.keyedvectors.Vocab at 0x7fe71045a5d0>,\n",
       " 'we': <gensim.models.keyedvectors.Vocab at 0x7fe7104553d0>,\n",
       " 'would': <gensim.models.keyedvectors.Vocab at 0x7fe71045a650>,\n",
       " 'two': <gensim.models.keyedvectors.Vocab at 0x7fe710455450>,\n",
       " 'more': <gensim.models.keyedvectors.Vocab at 0x7fe71045a6d0>,\n",
       " \"'\": <gensim.models.keyedvectors.Vocab at 0x7fe7104554d0>,\n",
       " 'first': <gensim.models.keyedvectors.Vocab at 0x7fe71045a710>,\n",
       " 'about': <gensim.models.keyedvectors.Vocab at 0x7fe710455550>,\n",
       " 'up': <gensim.models.keyedvectors.Vocab at 0x7fe71045a790>,\n",
       " 'when': <gensim.models.keyedvectors.Vocab at 0x7fe7104555d0>,\n",
       " 'year': <gensim.models.keyedvectors.Vocab at 0x7fe71045a810>,\n",
       " 'there': <gensim.models.keyedvectors.Vocab at 0x7fe710455650>,\n",
       " 'all': <gensim.models.keyedvectors.Vocab at 0x7fe71045a890>,\n",
       " '--': <gensim.models.keyedvectors.Vocab at 0x7fe7104556d0>,\n",
       " 'out': <gensim.models.keyedvectors.Vocab at 0x7fe71045a910>,\n",
       " 'she': <gensim.models.keyedvectors.Vocab at 0x7fe710455750>,\n",
       " 'other': <gensim.models.keyedvectors.Vocab at 0x7fe71045a990>,\n",
       " 'people': <gensim.models.keyedvectors.Vocab at 0x7fe7104557d0>,\n",
       " \"n't\": <gensim.models.keyedvectors.Vocab at 0x7fe71045aa10>,\n",
       " 'her': <gensim.models.keyedvectors.Vocab at 0x7fe710455850>,\n",
       " 'percent': <gensim.models.keyedvectors.Vocab at 0x7fe71045aa90>,\n",
       " 'than': <gensim.models.keyedvectors.Vocab at 0x7fe7104558d0>,\n",
       " 'over': <gensim.models.keyedvectors.Vocab at 0x7fe71045ab10>,\n",
       " 'into': <gensim.models.keyedvectors.Vocab at 0x7fe710455950>,\n",
       " 'last': <gensim.models.keyedvectors.Vocab at 0x7fe71045ab90>,\n",
       " 'some': <gensim.models.keyedvectors.Vocab at 0x7fe7104559d0>,\n",
       " 'government': <gensim.models.keyedvectors.Vocab at 0x7fe71045ac10>,\n",
       " 'time': <gensim.models.keyedvectors.Vocab at 0x7fe71045ad50>,\n",
       " '$': <gensim.models.keyedvectors.Vocab at 0x7fe71045ac90>,\n",
       " 'you': <gensim.models.keyedvectors.Vocab at 0x7fe71045ad90>,\n",
       " 'years': <gensim.models.keyedvectors.Vocab at 0x7fe71045fd50>,\n",
       " 'if': <gensim.models.keyedvectors.Vocab at 0x7fe71045ae10>,\n",
       " 'no': <gensim.models.keyedvectors.Vocab at 0x7fe71045fdd0>,\n",
       " 'world': <gensim.models.keyedvectors.Vocab at 0x7fe71045ae90>,\n",
       " 'can': <gensim.models.keyedvectors.Vocab at 0x7fe71045fe50>,\n",
       " 'three': <gensim.models.keyedvectors.Vocab at 0x7fe71045af10>,\n",
       " 'do': <gensim.models.keyedvectors.Vocab at 0x7fe71045fed0>,\n",
       " ';': <gensim.models.keyedvectors.Vocab at 0x7fe71045af90>,\n",
       " 'president': <gensim.models.keyedvectors.Vocab at 0x7fe71045ff10>,\n",
       " 'only': <gensim.models.keyedvectors.Vocab at 0x7fe71045f050>,\n",
       " 'state': <gensim.models.keyedvectors.Vocab at 0x7fe71045ff90>,\n",
       " 'million': <gensim.models.keyedvectors.Vocab at 0x7fe71045f0d0>,\n",
       " 'could': <gensim.models.keyedvectors.Vocab at 0x7fe710457050>,\n",
       " 'us': <gensim.models.keyedvectors.Vocab at 0x7fe71045f150>,\n",
       " 'most': <gensim.models.keyedvectors.Vocab at 0x7fe7104570d0>,\n",
       " '_': <gensim.models.keyedvectors.Vocab at 0x7fe71045f1d0>,\n",
       " 'against': <gensim.models.keyedvectors.Vocab at 0x7fe710457110>,\n",
       " 'u.s.': <gensim.models.keyedvectors.Vocab at 0x7fe71045f250>,\n",
       " 'so': <gensim.models.keyedvectors.Vocab at 0x7fe710457190>,\n",
       " 'them': <gensim.models.keyedvectors.Vocab at 0x7fe71045f2d0>,\n",
       " 'what': <gensim.models.keyedvectors.Vocab at 0x7fe710457210>,\n",
       " 'him': <gensim.models.keyedvectors.Vocab at 0x7fe71045f350>,\n",
       " 'united': <gensim.models.keyedvectors.Vocab at 0x7fe710457290>,\n",
       " 'during': <gensim.models.keyedvectors.Vocab at 0x7fe71045f3d0>,\n",
       " 'before': <gensim.models.keyedvectors.Vocab at 0x7fe710457310>,\n",
       " 'may': <gensim.models.keyedvectors.Vocab at 0x7fe71045f450>,\n",
       " 'since': <gensim.models.keyedvectors.Vocab at 0x7fe710457390>,\n",
       " 'many': <gensim.models.keyedvectors.Vocab at 0x7fe71045f4d0>,\n",
       " 'while': <gensim.models.keyedvectors.Vocab at 0x7fe710457410>,\n",
       " 'where': <gensim.models.keyedvectors.Vocab at 0x7fe71045f550>,\n",
       " 'states': <gensim.models.keyedvectors.Vocab at 0x7fe710457490>,\n",
       " 'because': <gensim.models.keyedvectors.Vocab at 0x7fe71045f5d0>,\n",
       " 'now': <gensim.models.keyedvectors.Vocab at 0x7fe710457510>,\n",
       " 'city': <gensim.models.keyedvectors.Vocab at 0x7fe71045f650>,\n",
       " 'made': <gensim.models.keyedvectors.Vocab at 0x7fe710457590>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x7fe71045f6d0>,\n",
       " 'between': <gensim.models.keyedvectors.Vocab at 0x7fe710457610>,\n",
       " 'did': <gensim.models.keyedvectors.Vocab at 0x7fe71045f750>,\n",
       " 'just': <gensim.models.keyedvectors.Vocab at 0x7fe710457690>,\n",
       " 'national': <gensim.models.keyedvectors.Vocab at 0x7fe71045f7d0>,\n",
       " 'day': <gensim.models.keyedvectors.Vocab at 0x7fe710457710>,\n",
       " 'country': <gensim.models.keyedvectors.Vocab at 0x7fe71045f850>,\n",
       " 'under': <gensim.models.keyedvectors.Vocab at 0x7fe710457790>,\n",
       " 'such': <gensim.models.keyedvectors.Vocab at 0x7fe71045f8d0>,\n",
       " 'second': <gensim.models.keyedvectors.Vocab at 0x7fe710457810>,\n",
       " 'then': <gensim.models.keyedvectors.Vocab at 0x7fe71045f950>,\n",
       " 'company': <gensim.models.keyedvectors.Vocab at 0x7fe710457890>,\n",
       " 'group': <gensim.models.keyedvectors.Vocab at 0x7fe71045f9d0>,\n",
       " 'any': <gensim.models.keyedvectors.Vocab at 0x7fe710457910>,\n",
       " 'through': <gensim.models.keyedvectors.Vocab at 0x7fe71045fa50>,\n",
       " 'china': <gensim.models.keyedvectors.Vocab at 0x7fe710457990>,\n",
       " 'four': <gensim.models.keyedvectors.Vocab at 0x7fe71045fad0>,\n",
       " 'being': <gensim.models.keyedvectors.Vocab at 0x7fe710457a10>,\n",
       " 'down': <gensim.models.keyedvectors.Vocab at 0x7fe71045fb50>,\n",
       " 'war': <gensim.models.keyedvectors.Vocab at 0x7fe710457a90>,\n",
       " 'back': <gensim.models.keyedvectors.Vocab at 0x7fe71045fbd0>,\n",
       " 'off': <gensim.models.keyedvectors.Vocab at 0x7fe710457b10>,\n",
       " 'south': <gensim.models.keyedvectors.Vocab at 0x7fe71045fc50>,\n",
       " 'american': <gensim.models.keyedvectors.Vocab at 0x7fe710457b90>,\n",
       " 'minister': <gensim.models.keyedvectors.Vocab at 0x7fe71045fcd0>,\n",
       " 'police': <gensim.models.keyedvectors.Vocab at 0x7fe710457c10>,\n",
       " 'well': <gensim.models.keyedvectors.Vocab at 0x7fe710457dd0>,\n",
       " 'including': <gensim.models.keyedvectors.Vocab at 0x7fe710457c90>,\n",
       " 'team': <gensim.models.keyedvectors.Vocab at 0x7fe710457e50>,\n",
       " 'international': <gensim.models.keyedvectors.Vocab at 0x7fe710457d10>,\n",
       " 'week': <gensim.models.keyedvectors.Vocab at 0x7fe710457ed0>,\n",
       " 'officials': <gensim.models.keyedvectors.Vocab at 0x7fe710457d90>,\n",
       " 'still': <gensim.models.keyedvectors.Vocab at 0x7fe710457f50>,\n",
       " 'both': <gensim.models.keyedvectors.Vocab at 0x7fe710458e90>,\n",
       " 'even': <gensim.models.keyedvectors.Vocab at 0x7fe710457fd0>,\n",
       " 'high': <gensim.models.keyedvectors.Vocab at 0x7fe710458f10>,\n",
       " 'part': <gensim.models.keyedvectors.Vocab at 0x7fe710458090>,\n",
       " 'told': <gensim.models.keyedvectors.Vocab at 0x7fe710458f90>,\n",
       " 'those': <gensim.models.keyedvectors.Vocab at 0x7fe710458110>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x7fe71044c050>,\n",
       " 'former': <gensim.models.keyedvectors.Vocab at 0x7fe710458190>,\n",
       " 'these': <gensim.models.keyedvectors.Vocab at 0x7fe71044c0d0>,\n",
       " 'make': <gensim.models.keyedvectors.Vocab at 0x7fe710458210>,\n",
       " 'billion': <gensim.models.keyedvectors.Vocab at 0x7fe71044c150>,\n",
       " 'work': <gensim.models.keyedvectors.Vocab at 0x7fe710458290>,\n",
       " 'our': <gensim.models.keyedvectors.Vocab at 0x7fe71044c1d0>,\n",
       " 'home': <gensim.models.keyedvectors.Vocab at 0x7fe710458310>,\n",
       " 'school': <gensim.models.keyedvectors.Vocab at 0x7fe71044c250>,\n",
       " 'party': <gensim.models.keyedvectors.Vocab at 0x7fe710458390>,\n",
       " 'house': <gensim.models.keyedvectors.Vocab at 0x7fe71044c2d0>,\n",
       " 'old': <gensim.models.keyedvectors.Vocab at 0x7fe710458410>,\n",
       " 'later': <gensim.models.keyedvectors.Vocab at 0x7fe71044c350>,\n",
       " 'get': <gensim.models.keyedvectors.Vocab at 0x7fe710458490>,\n",
       " 'another': <gensim.models.keyedvectors.Vocab at 0x7fe71044c3d0>,\n",
       " 'tuesday': <gensim.models.keyedvectors.Vocab at 0x7fe710458510>,\n",
       " 'news': <gensim.models.keyedvectors.Vocab at 0x7fe71044c450>,\n",
       " 'long': <gensim.models.keyedvectors.Vocab at 0x7fe710458590>,\n",
       " 'five': <gensim.models.keyedvectors.Vocab at 0x7fe71044c4d0>,\n",
       " 'called': <gensim.models.keyedvectors.Vocab at 0x7fe710458610>,\n",
       " '1': <gensim.models.keyedvectors.Vocab at 0x7fe71044c550>,\n",
       " 'wednesday': <gensim.models.keyedvectors.Vocab at 0x7fe710458650>,\n",
       " 'military': <gensim.models.keyedvectors.Vocab at 0x7fe71044c5d0>,\n",
       " 'way': <gensim.models.keyedvectors.Vocab at 0x7fe7104586d0>,\n",
       " 'used': <gensim.models.keyedvectors.Vocab at 0x7fe71044c650>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x7fe710458750>,\n",
       " 'next': <gensim.models.keyedvectors.Vocab at 0x7fe71044c6d0>,\n",
       " 'monday': <gensim.models.keyedvectors.Vocab at 0x7fe7104587d0>,\n",
       " 'thursday': <gensim.models.keyedvectors.Vocab at 0x7fe71044c750>,\n",
       " 'friday': <gensim.models.keyedvectors.Vocab at 0x7fe710458850>,\n",
       " 'game': <gensim.models.keyedvectors.Vocab at 0x7fe71044c7d0>,\n",
       " 'here': <gensim.models.keyedvectors.Vocab at 0x7fe7104588d0>,\n",
       " '?': <gensim.models.keyedvectors.Vocab at 0x7fe71044c850>,\n",
       " 'should': <gensim.models.keyedvectors.Vocab at 0x7fe710458910>,\n",
       " 'take': <gensim.models.keyedvectors.Vocab at 0x7fe71044c8d0>,\n",
       " 'very': <gensim.models.keyedvectors.Vocab at 0x7fe710458990>,\n",
       " 'my': <gensim.models.keyedvectors.Vocab at 0x7fe71044c950>,\n",
       " 'north': <gensim.models.keyedvectors.Vocab at 0x7fe710458a10>,\n",
       " 'security': <gensim.models.keyedvectors.Vocab at 0x7fe71044c9d0>,\n",
       " 'season': <gensim.models.keyedvectors.Vocab at 0x7fe710458a90>,\n",
       " 'york': <gensim.models.keyedvectors.Vocab at 0x7fe71044ca50>,\n",
       " 'how': <gensim.models.keyedvectors.Vocab at 0x7fe710458b10>,\n",
       " 'public': <gensim.models.keyedvectors.Vocab at 0x7fe71044cad0>,\n",
       " 'early': <gensim.models.keyedvectors.Vocab at 0x7fe710458b90>,\n",
       " 'according': <gensim.models.keyedvectors.Vocab at 0x7fe71044cb50>,\n",
       " 'several': <gensim.models.keyedvectors.Vocab at 0x7fe710458c10>,\n",
       " 'court': <gensim.models.keyedvectors.Vocab at 0x7fe71044cbd0>,\n",
       " 'say': <gensim.models.keyedvectors.Vocab at 0x7fe710458c90>,\n",
       " 'around': <gensim.models.keyedvectors.Vocab at 0x7fe71044cc50>,\n",
       " 'foreign': <gensim.models.keyedvectors.Vocab at 0x7fe710458d10>,\n",
       " '10': <gensim.models.keyedvectors.Vocab at 0x7fe71044ccd0>,\n",
       " 'until': <gensim.models.keyedvectors.Vocab at 0x7fe710458d90>,\n",
       " 'set': <gensim.models.keyedvectors.Vocab at 0x7fe71044cd50>,\n",
       " 'political': <gensim.models.keyedvectors.Vocab at 0x7fe710458e10>,\n",
       " 'says': <gensim.models.keyedvectors.Vocab at 0x7fe71044cdd0>,\n",
       " 'market': <gensim.models.keyedvectors.Vocab at 0x7fe71044cf50>,\n",
       " 'however': <gensim.models.keyedvectors.Vocab at 0x7fe71044ce50>,\n",
       " 'family': <gensim.models.keyedvectors.Vocab at 0x7fe710460090>,\n",
       " 'life': <gensim.models.keyedvectors.Vocab at 0x7fe71044ced0>,\n",
       " 'same': <gensim.models.keyedvectors.Vocab at 0x7fe710460150>,\n",
       " 'general': <gensim.models.keyedvectors.Vocab at 0x7fe710460050>,\n",
       " '–': <gensim.models.keyedvectors.Vocab at 0x7fe710460250>,\n",
       " 'left': <gensim.models.keyedvectors.Vocab at 0x7fe710460110>,\n",
       " 'good': <gensim.models.keyedvectors.Vocab at 0x7fe710460350>,\n",
       " 'top': <gensim.models.keyedvectors.Vocab at 0x7fe710460210>,\n",
       " 'university': <gensim.models.keyedvectors.Vocab at 0x7fe710460450>,\n",
       " 'going': <gensim.models.keyedvectors.Vocab at 0x7fe710460310>,\n",
       " 'number': <gensim.models.keyedvectors.Vocab at 0x7fe710460550>,\n",
       " 'major': <gensim.models.keyedvectors.Vocab at 0x7fe710460410>,\n",
       " 'known': <gensim.models.keyedvectors.Vocab at 0x7fe710460650>,\n",
       " 'points': <gensim.models.keyedvectors.Vocab at 0x7fe710460510>,\n",
       " 'won': <gensim.models.keyedvectors.Vocab at 0x7fe710460710>,\n",
       " 'six': <gensim.models.keyedvectors.Vocab at 0x7fe710460610>,\n",
       " 'month': <gensim.models.keyedvectors.Vocab at 0x7fe710460810>,\n",
       " 'dollars': <gensim.models.keyedvectors.Vocab at 0x7fe710460750>,\n",
       " 'bank': <gensim.models.keyedvectors.Vocab at 0x7fe710460910>,\n",
       " '2': <gensim.models.keyedvectors.Vocab at 0x7fe710460850>,\n",
       " 'iraq': <gensim.models.keyedvectors.Vocab at 0x7fe7104609d0>,\n",
       " 'use': <gensim.models.keyedvectors.Vocab at 0x7fe710460950>,\n",
       " 'members': <gensim.models.keyedvectors.Vocab at 0x7fe7790d5450>,\n",
       " 'each': <gensim.models.keyedvectors.Vocab at 0x7fe710460a50>,\n",
       " 'area': <gensim.models.keyedvectors.Vocab at 0x7fe7790d5590>,\n",
       " 'found': <gensim.models.keyedvectors.Vocab at 0x7fe710460b50>,\n",
       " 'official': <gensim.models.keyedvectors.Vocab at 0x7fe7790d5150>,\n",
       " 'sunday': <gensim.models.keyedvectors.Vocab at 0x7fe710460c50>,\n",
       " 'place': <gensim.models.keyedvectors.Vocab at 0x7fe7790d5310>,\n",
       " 'go': <gensim.models.keyedvectors.Vocab at 0x7fe710460d50>,\n",
       " 'based': <gensim.models.keyedvectors.Vocab at 0x7fe7790d51d0>,\n",
       " 'among': <gensim.models.keyedvectors.Vocab at 0x7fe710460e50>,\n",
       " 'third': <gensim.models.keyedvectors.Vocab at 0x7fe7790d5110>,\n",
       " 'times': <gensim.models.keyedvectors.Vocab at 0x7fe710460f50>,\n",
       " 'took': <gensim.models.keyedvectors.Vocab at 0x7fe7790d5510>,\n",
       " 'right': <gensim.models.keyedvectors.Vocab at 0x7fe710461090>,\n",
       " 'days': <gensim.models.keyedvectors.Vocab at 0x7fe774490190>,\n",
       " 'local': <gensim.models.keyedvectors.Vocab at 0x7fe710461210>,\n",
       " 'economic': <gensim.models.keyedvectors.Vocab at 0x7fe7744903d0>,\n",
       " 'countries': <gensim.models.keyedvectors.Vocab at 0x7fe710461310>,\n",
       " 'see': <gensim.models.keyedvectors.Vocab at 0x7fe774490210>,\n",
       " 'best': <gensim.models.keyedvectors.Vocab at 0x7fe710461410>,\n",
       " 'report': <gensim.models.keyedvectors.Vocab at 0x7fe774490310>,\n",
       " 'killed': <gensim.models.keyedvectors.Vocab at 0x7fe710461510>,\n",
       " 'held': <gensim.models.keyedvectors.Vocab at 0x7fe774490410>,\n",
       " 'business': <gensim.models.keyedvectors.Vocab at 0x7fe710461610>,\n",
       " 'west': <gensim.models.keyedvectors.Vocab at 0x7fe774490390>,\n",
       " 'does': <gensim.models.keyedvectors.Vocab at 0x7fe710461710>,\n",
       " 'own': <gensim.models.keyedvectors.Vocab at 0x7fe777cc4050>,\n",
       " '%': <gensim.models.keyedvectors.Vocab at 0x7fe710461810>,\n",
       " 'came': <gensim.models.keyedvectors.Vocab at 0x7fe774450dd0>,\n",
       " 'law': <gensim.models.keyedvectors.Vocab at 0x7fe710461910>,\n",
       " 'months': <gensim.models.keyedvectors.Vocab at 0x7fe710460b10>,\n",
       " 'women': <gensim.models.keyedvectors.Vocab at 0x7fe710461a10>,\n",
       " \"'re\": <gensim.models.keyedvectors.Vocab at 0x7fe710460c10>,\n",
       " 'power': <gensim.models.keyedvectors.Vocab at 0x7fe710461b10>,\n",
       " 'think': <gensim.models.keyedvectors.Vocab at 0x7fe710460d10>,\n",
       " 'service': <gensim.models.keyedvectors.Vocab at 0x7fe710461c10>,\n",
       " 'children': <gensim.models.keyedvectors.Vocab at 0x7fe710460e10>,\n",
       " 'bush': <gensim.models.keyedvectors.Vocab at 0x7fe710461d10>,\n",
       " 'show': <gensim.models.keyedvectors.Vocab at 0x7fe710460f10>,\n",
       " '/': <gensim.models.keyedvectors.Vocab at 0x7fe710461e10>,\n",
       " 'help': <gensim.models.keyedvectors.Vocab at 0x7fe710460f90>,\n",
       " 'chief': <gensim.models.keyedvectors.Vocab at 0x7fe710461f10>,\n",
       " 'saturday': <gensim.models.keyedvectors.Vocab at 0x7fe7104610d0>,\n",
       " 'system': <gensim.models.keyedvectors.Vocab at 0x7fe710462050>,\n",
       " 'john': <gensim.models.keyedvectors.Vocab at 0x7fe710461190>,\n",
       " 'support': <gensim.models.keyedvectors.Vocab at 0x7fe710462150>,\n",
       " 'series': <gensim.models.keyedvectors.Vocab at 0x7fe710461290>,\n",
       " 'play': <gensim.models.keyedvectors.Vocab at 0x7fe710462250>,\n",
       " 'office': <gensim.models.keyedvectors.Vocab at 0x7fe710461390>,\n",
       " 'following': <gensim.models.keyedvectors.Vocab at 0x7fe710462310>,\n",
       " 'me': <gensim.models.keyedvectors.Vocab at 0x7fe710461490>,\n",
       " 'meeting': <gensim.models.keyedvectors.Vocab at 0x7fe710462410>,\n",
       " 'expected': <gensim.models.keyedvectors.Vocab at 0x7fe710461590>,\n",
       " 'late': <gensim.models.keyedvectors.Vocab at 0x7fe710462510>,\n",
       " 'washington': <gensim.models.keyedvectors.Vocab at 0x7fe710461690>,\n",
       " 'games': <gensim.models.keyedvectors.Vocab at 0x7fe710462610>,\n",
       " 'european': <gensim.models.keyedvectors.Vocab at 0x7fe710461750>,\n",
       " 'league': <gensim.models.keyedvectors.Vocab at 0x7fe710462790>,\n",
       " 'reported': <gensim.models.keyedvectors.Vocab at 0x7fe710461850>,\n",
       " 'final': <gensim.models.keyedvectors.Vocab at 0x7fe710462890>,\n",
       " 'added': <gensim.models.keyedvectors.Vocab at 0x7fe710461950>,\n",
       " 'without': <gensim.models.keyedvectors.Vocab at 0x7fe710462990>,\n",
       " 'british': <gensim.models.keyedvectors.Vocab at 0x7fe710461a50>,\n",
       " 'white': <gensim.models.keyedvectors.Vocab at 0x7fe710462a90>,\n",
       " 'history': <gensim.models.keyedvectors.Vocab at 0x7fe710461b50>,\n",
       " 'man': <gensim.models.keyedvectors.Vocab at 0x7fe710462b90>,\n",
       " 'men': <gensim.models.keyedvectors.Vocab at 0x7fe710461c50>,\n",
       " 'became': <gensim.models.keyedvectors.Vocab at 0x7fe710462c90>,\n",
       " 'want': <gensim.models.keyedvectors.Vocab at 0x7fe710461d50>,\n",
       " 'march': <gensim.models.keyedvectors.Vocab at 0x7fe710462d90>,\n",
       " 'case': <gensim.models.keyedvectors.Vocab at 0x7fe710461e50>,\n",
       " 'few': <gensim.models.keyedvectors.Vocab at 0x7fe710462e90>,\n",
       " 'run': <gensim.models.keyedvectors.Vocab at 0x7fe710461f50>,\n",
       " 'money': <gensim.models.keyedvectors.Vocab at 0x7fe710462f90>,\n",
       " 'began': <gensim.models.keyedvectors.Vocab at 0x7fe710462090>,\n",
       " 'open': <gensim.models.keyedvectors.Vocab at 0x7fe7104640d0>,\n",
       " 'name': <gensim.models.keyedvectors.Vocab at 0x7fe710462190>,\n",
       " 'trade': <gensim.models.keyedvectors.Vocab at 0x7fe7104641d0>,\n",
       " 'center': <gensim.models.keyedvectors.Vocab at 0x7fe710462290>,\n",
       " '3': <gensim.models.keyedvectors.Vocab at 0x7fe7104642d0>,\n",
       " 'israel': <gensim.models.keyedvectors.Vocab at 0x7fe710462350>,\n",
       " 'oil': <gensim.models.keyedvectors.Vocab at 0x7fe7104643d0>,\n",
       " 'too': <gensim.models.keyedvectors.Vocab at 0x7fe710462450>,\n",
       " 'al': <gensim.models.keyedvectors.Vocab at 0x7fe7104644d0>,\n",
       " 'film': <gensim.models.keyedvectors.Vocab at 0x7fe710462550>,\n",
       " 'win': <gensim.models.keyedvectors.Vocab at 0x7fe7104645d0>,\n",
       " 'led': <gensim.models.keyedvectors.Vocab at 0x7fe710462650>,\n",
       " 'east': <gensim.models.keyedvectors.Vocab at 0x7fe7104646d0>,\n",
       " 'central': <gensim.models.keyedvectors.Vocab at 0x7fe710462710>,\n",
       " '20': <gensim.models.keyedvectors.Vocab at 0x7fe7104647d0>,\n",
       " 'air': <gensim.models.keyedvectors.Vocab at 0x7fe710462810>,\n",
       " 'come': <gensim.models.keyedvectors.Vocab at 0x7fe7104648d0>,\n",
       " 'chinese': <gensim.models.keyedvectors.Vocab at 0x7fe710462910>,\n",
       " 'town': <gensim.models.keyedvectors.Vocab at 0x7fe7104649d0>,\n",
       " 'leader': <gensim.models.keyedvectors.Vocab at 0x7fe710462a10>,\n",
       " 'army': <gensim.models.keyedvectors.Vocab at 0x7fe710464ad0>,\n",
       " 'line': <gensim.models.keyedvectors.Vocab at 0x7fe710462ad0>,\n",
       " 'never': <gensim.models.keyedvectors.Vocab at 0x7fe710464bd0>,\n",
       " 'little': <gensim.models.keyedvectors.Vocab at 0x7fe710462bd0>,\n",
       " 'played': <gensim.models.keyedvectors.Vocab at 0x7fe710464cd0>,\n",
       " 'prime': <gensim.models.keyedvectors.Vocab at 0x7fe710462cd0>,\n",
       " 'death': <gensim.models.keyedvectors.Vocab at 0x7fe710464dd0>,\n",
       " 'companies': <gensim.models.keyedvectors.Vocab at 0x7fe710462dd0>,\n",
       " 'least': <gensim.models.keyedvectors.Vocab at 0x7fe710464ed0>,\n",
       " 'put': <gensim.models.keyedvectors.Vocab at 0x7fe710462ed0>,\n",
       " 'forces': <gensim.models.keyedvectors.Vocab at 0x7fe710464fd0>,\n",
       " 'past': <gensim.models.keyedvectors.Vocab at 0x7fe710462fd0>,\n",
       " 'de': <gensim.models.keyedvectors.Vocab at 0x7fe710465110>,\n",
       " 'half': <gensim.models.keyedvectors.Vocab at 0x7fe710464110>,\n",
       " 'june': <gensim.models.keyedvectors.Vocab at 0x7fe710465210>,\n",
       " 'saying': <gensim.models.keyedvectors.Vocab at 0x7fe710464210>,\n",
       " 'know': <gensim.models.keyedvectors.Vocab at 0x7fe710465310>,\n",
       " 'federal': <gensim.models.keyedvectors.Vocab at 0x7fe710464310>,\n",
       " 'french': <gensim.models.keyedvectors.Vocab at 0x7fe710465410>,\n",
       " 'peace': <gensim.models.keyedvectors.Vocab at 0x7fe710464410>,\n",
       " 'earlier': <gensim.models.keyedvectors.Vocab at 0x7fe710465510>,\n",
       " 'capital': <gensim.models.keyedvectors.Vocab at 0x7fe710464510>,\n",
       " 'force': <gensim.models.keyedvectors.Vocab at 0x7fe710465610>,\n",
       " 'great': <gensim.models.keyedvectors.Vocab at 0x7fe710464610>,\n",
       " 'union': <gensim.models.keyedvectors.Vocab at 0x7fe710465710>,\n",
       " 'near': <gensim.models.keyedvectors.Vocab at 0x7fe710464710>,\n",
       " 'released': <gensim.models.keyedvectors.Vocab at 0x7fe710465810>,\n",
       " 'small': <gensim.models.keyedvectors.Vocab at 0x7fe710464810>,\n",
       " 'department': <gensim.models.keyedvectors.Vocab at 0x7fe710465910>,\n",
       " 'every': <gensim.models.keyedvectors.Vocab at 0x7fe710464910>,\n",
       " 'health': <gensim.models.keyedvectors.Vocab at 0x7fe710465a10>,\n",
       " 'japan': <gensim.models.keyedvectors.Vocab at 0x7fe710464a10>,\n",
       " 'head': <gensim.models.keyedvectors.Vocab at 0x7fe710465b10>,\n",
       " 'ago': <gensim.models.keyedvectors.Vocab at 0x7fe710464b10>,\n",
       " 'night': <gensim.models.keyedvectors.Vocab at 0x7fe710465c10>,\n",
       " 'big': <gensim.models.keyedvectors.Vocab at 0x7fe710464c10>,\n",
       " 'cup': <gensim.models.keyedvectors.Vocab at 0x7fe710465d10>,\n",
       " 'election': <gensim.models.keyedvectors.Vocab at 0x7fe710464d10>,\n",
       " 'region': <gensim.models.keyedvectors.Vocab at 0x7fe710465e10>,\n",
       " 'director': <gensim.models.keyedvectors.Vocab at 0x7fe710464e10>,\n",
       " 'talks': <gensim.models.keyedvectors.Vocab at 0x7fe710465f10>,\n",
       " 'program': <gensim.models.keyedvectors.Vocab at 0x7fe710464f10>,\n",
       " 'far': <gensim.models.keyedvectors.Vocab at 0x7fe710466050>,\n",
       " 'today': <gensim.models.keyedvectors.Vocab at 0x7fe710465050>,\n",
       " 'statement': <gensim.models.keyedvectors.Vocab at 0x7fe710466150>,\n",
       " 'july': <gensim.models.keyedvectors.Vocab at 0x7fe710465150>,\n",
       " 'although': <gensim.models.keyedvectors.Vocab at 0x7fe710466250>,\n",
       " 'district': <gensim.models.keyedvectors.Vocab at 0x7fe710465250>,\n",
       " 'again': <gensim.models.keyedvectors.Vocab at 0x7fe710466310>,\n",
       " 'born': <gensim.models.keyedvectors.Vocab at 0x7fe710465350>,\n",
       " 'development': <gensim.models.keyedvectors.Vocab at 0x7fe710466410>,\n",
       " 'leaders': <gensim.models.keyedvectors.Vocab at 0x7fe710465450>,\n",
       " 'council': <gensim.models.keyedvectors.Vocab at 0x7fe710466510>,\n",
       " 'close': <gensim.models.keyedvectors.Vocab at 0x7fe710465550>,\n",
       " 'record': <gensim.models.keyedvectors.Vocab at 0x7fe710466610>,\n",
       " 'along': <gensim.models.keyedvectors.Vocab at 0x7fe710465650>,\n",
       " 'county': <gensim.models.keyedvectors.Vocab at 0x7fe710466710>,\n",
       " 'france': <gensim.models.keyedvectors.Vocab at 0x7fe710465750>,\n",
       " 'went': <gensim.models.keyedvectors.Vocab at 0x7fe710466810>,\n",
       " 'point': <gensim.models.keyedvectors.Vocab at 0x7fe710465850>,\n",
       " 'must': <gensim.models.keyedvectors.Vocab at 0x7fe7104668d0>,\n",
       " 'spokesman': <gensim.models.keyedvectors.Vocab at 0x7fe710465950>,\n",
       " 'your': <gensim.models.keyedvectors.Vocab at 0x7fe7104669d0>,\n",
       " 'member': <gensim.models.keyedvectors.Vocab at 0x7fe710465a50>,\n",
       " 'plan': <gensim.models.keyedvectors.Vocab at 0x7fe710466ad0>,\n",
       " 'financial': <gensim.models.keyedvectors.Vocab at 0x7fe710465b50>,\n",
       " 'april': <gensim.models.keyedvectors.Vocab at 0x7fe710466bd0>,\n",
       " 'recent': <gensim.models.keyedvectors.Vocab at 0x7fe710465c50>,\n",
       " 'campaign': <gensim.models.keyedvectors.Vocab at 0x7fe710466cd0>,\n",
       " 'become': <gensim.models.keyedvectors.Vocab at 0x7fe710465d50>,\n",
       " 'troops': <gensim.models.keyedvectors.Vocab at 0x7fe710466dd0>,\n",
       " 'whether': <gensim.models.keyedvectors.Vocab at 0x7fe710465e50>,\n",
       " 'lost': <gensim.models.keyedvectors.Vocab at 0x7fe710466ed0>,\n",
       " 'music': <gensim.models.keyedvectors.Vocab at 0x7fe710465f50>,\n",
       " '15': <gensim.models.keyedvectors.Vocab at 0x7fe710466fd0>,\n",
       " 'got': <gensim.models.keyedvectors.Vocab at 0x7fe710466090>,\n",
       " 'israeli': <gensim.models.keyedvectors.Vocab at 0x7fe710467110>,\n",
       " '30': <gensim.models.keyedvectors.Vocab at 0x7fe710466190>,\n",
       " 'need': <gensim.models.keyedvectors.Vocab at 0x7fe710467210>,\n",
       " '4': <gensim.models.keyedvectors.Vocab at 0x7fe710466290>,\n",
       " 'lead': <gensim.models.keyedvectors.Vocab at 0x7fe7104672d0>,\n",
       " 'already': <gensim.models.keyedvectors.Vocab at 0x7fe710466390>,\n",
       " 'russia': <gensim.models.keyedvectors.Vocab at 0x7fe7104673d0>,\n",
       " 'though': <gensim.models.keyedvectors.Vocab at 0x7fe710466490>,\n",
       " 'might': <gensim.models.keyedvectors.Vocab at 0x7fe7104674d0>,\n",
       " 'free': <gensim.models.keyedvectors.Vocab at 0x7fe710466590>,\n",
       " 'hit': <gensim.models.keyedvectors.Vocab at 0x7fe7104675d0>,\n",
       " 'rights': <gensim.models.keyedvectors.Vocab at 0x7fe710466690>,\n",
       " '11': <gensim.models.keyedvectors.Vocab at 0x7fe7104676d0>,\n",
       " 'information': <gensim.models.keyedvectors.Vocab at 0x7fe710466790>,\n",
       " 'away': <gensim.models.keyedvectors.Vocab at 0x7fe7104677d0>,\n",
       " '12': <gensim.models.keyedvectors.Vocab at 0x7fe710466910>,\n",
       " '5': <gensim.models.keyedvectors.Vocab at 0x7fe7104678d0>,\n",
       " 'others': <gensim.models.keyedvectors.Vocab at 0x7fe710466950>,\n",
       " 'control': <gensim.models.keyedvectors.Vocab at 0x7fe7104679d0>,\n",
       " 'within': <gensim.models.keyedvectors.Vocab at 0x7fe710466a50>,\n",
       " 'large': <gensim.models.keyedvectors.Vocab at 0x7fe710467ad0>,\n",
       " 'economy': <gensim.models.keyedvectors.Vocab at 0x7fe710466b50>,\n",
       " 'press': <gensim.models.keyedvectors.Vocab at 0x7fe710467bd0>,\n",
       " 'agency': <gensim.models.keyedvectors.Vocab at 0x7fe710466c50>,\n",
       " 'water': <gensim.models.keyedvectors.Vocab at 0x7fe710467cd0>,\n",
       " 'died': <gensim.models.keyedvectors.Vocab at 0x7fe710466d50>,\n",
       " 'career': <gensim.models.keyedvectors.Vocab at 0x7fe710467dd0>,\n",
       " 'making': <gensim.models.keyedvectors.Vocab at 0x7fe710466e50>,\n",
       " '...': <gensim.models.keyedvectors.Vocab at 0x7fe710467e90>,\n",
       " 'deal': <gensim.models.keyedvectors.Vocab at 0x7fe710466f50>,\n",
       " 'attack': <gensim.models.keyedvectors.Vocab at 0x7fe710467f90>,\n",
       " 'side': <gensim.models.keyedvectors.Vocab at 0x7fe710467090>,\n",
       " 'seven': <gensim.models.keyedvectors.Vocab at 0x7fe710467350>,\n",
       " 'better': <gensim.models.keyedvectors.Vocab at 0x7fe710467190>,\n",
       " 'less': <gensim.models.keyedvectors.Vocab at 0x7fe710467550>,\n",
       " 'september': <gensim.models.keyedvectors.Vocab at 0x7fe710467390>,\n",
       " 'once': <gensim.models.keyedvectors.Vocab at 0x7fe710467710>,\n",
       " 'clinton': <gensim.models.keyedvectors.Vocab at 0x7fe710467590>,\n",
       " 'main': <gensim.models.keyedvectors.Vocab at 0x7fe710467910>,\n",
       " 'due': <gensim.models.keyedvectors.Vocab at 0x7fe710467750>,\n",
       " 'committee': <gensim.models.keyedvectors.Vocab at 0x7fe710467b10>,\n",
       " 'building': <gensim.models.keyedvectors.Vocab at 0x7fe710467950>,\n",
       " 'conference': <gensim.models.keyedvectors.Vocab at 0x7fe710467d10>,\n",
       " 'club': <gensim.models.keyedvectors.Vocab at 0x7fe710467b50>,\n",
       " 'january': <gensim.models.keyedvectors.Vocab at 0x7fe710467f10>,\n",
       " 'decision': <gensim.models.keyedvectors.Vocab at 0x7fe710467d50>,\n",
       " 'stock': <gensim.models.keyedvectors.Vocab at 0x7fe71046b0d0>,\n",
       " 'america': <gensim.models.keyedvectors.Vocab at 0x7fe710467fd0>,\n",
       " 'given': <gensim.models.keyedvectors.Vocab at 0x7fe71046b1d0>,\n",
       " 'give': <gensim.models.keyedvectors.Vocab at 0x7fe71046b110>,\n",
       " 'often': <gensim.models.keyedvectors.Vocab at 0x7fe71046b2d0>,\n",
       " 'announced': <gensim.models.keyedvectors.Vocab at 0x7fe71046b210>,\n",
       " 'television': <gensim.models.keyedvectors.Vocab at 0x7fe71046b3d0>,\n",
       " 'industry': <gensim.models.keyedvectors.Vocab at 0x7fe71046b310>,\n",
       " 'order': <gensim.models.keyedvectors.Vocab at 0x7fe71046b4d0>,\n",
       " 'young': <gensim.models.keyedvectors.Vocab at 0x7fe71046b410>,\n",
       " \"'ve\": <gensim.models.keyedvectors.Vocab at 0x7fe71046b5d0>,\n",
       " 'palestinian': <gensim.models.keyedvectors.Vocab at 0x7fe71046b510>,\n",
       " 'age': <gensim.models.keyedvectors.Vocab at 0x7fe71046b690>,\n",
       " 'start': <gensim.models.keyedvectors.Vocab at 0x7fe71046b610>,\n",
       " 'administration': <gensim.models.keyedvectors.Vocab at 0x7fe71046b790>,\n",
       " 'russian': <gensim.models.keyedvectors.Vocab at 0x7fe71046b710>,\n",
       " 'prices': <gensim.models.keyedvectors.Vocab at 0x7fe71046b890>,\n",
       " 'round': <gensim.models.keyedvectors.Vocab at 0x7fe71046b810>,\n",
       " 'december': <gensim.models.keyedvectors.Vocab at 0x7fe71046b990>,\n",
       " 'nations': <gensim.models.keyedvectors.Vocab at 0x7fe71046b910>,\n",
       " \"'m\": <gensim.models.keyedvectors.Vocab at 0x7fe71046ba90>,\n",
       " 'human': <gensim.models.keyedvectors.Vocab at 0x7fe71046ba10>,\n",
       " 'india': <gensim.models.keyedvectors.Vocab at 0x7fe71046bb90>,\n",
       " 'defense': <gensim.models.keyedvectors.Vocab at 0x7fe71046bb10>,\n",
       " 'asked': <gensim.models.keyedvectors.Vocab at 0x7fe71046bc50>,\n",
       " 'total': <gensim.models.keyedvectors.Vocab at 0x7fe71046bc90>,\n",
       " 'october': <gensim.models.keyedvectors.Vocab at 0x7fe71046bd50>,\n",
       " 'players': <gensim.models.keyedvectors.Vocab at 0x7fe71046bd90>,\n",
       " 'bill': <gensim.models.keyedvectors.Vocab at 0x7fe71046be50>,\n",
       " 'important': <gensim.models.keyedvectors.Vocab at 0x7fe71046be90>,\n",
       " 'southern': <gensim.models.keyedvectors.Vocab at 0x7fe71046bf50>,\n",
       " 'move': <gensim.models.keyedvectors.Vocab at 0x7fe71046bf90>,\n",
       " 'fire': <gensim.models.keyedvectors.Vocab at 0x7fe71046c090>,\n",
       " 'population': <gensim.models.keyedvectors.Vocab at 0x7fe71046c0d0>,\n",
       " 'rose': <gensim.models.keyedvectors.Vocab at 0x7fe71046c190>,\n",
       " 'november': <gensim.models.keyedvectors.Vocab at 0x7fe71046c1d0>,\n",
       " 'include': <gensim.models.keyedvectors.Vocab at 0x7fe71046c290>,\n",
       " 'further': <gensim.models.keyedvectors.Vocab at 0x7fe71046c2d0>,\n",
       " 'nuclear': <gensim.models.keyedvectors.Vocab at 0x7fe71046c390>,\n",
       " 'street': <gensim.models.keyedvectors.Vocab at 0x7fe71046c3d0>,\n",
       " 'taken': <gensim.models.keyedvectors.Vocab at 0x7fe71046c490>,\n",
       " 'media': <gensim.models.keyedvectors.Vocab at 0x7fe71046c4d0>,\n",
       " 'different': <gensim.models.keyedvectors.Vocab at 0x7fe71046c590>,\n",
       " 'issue': <gensim.models.keyedvectors.Vocab at 0x7fe71046c5d0>,\n",
       " 'received': <gensim.models.keyedvectors.Vocab at 0x7fe71046c690>,\n",
       " 'secretary': <gensim.models.keyedvectors.Vocab at 0x7fe71046c6d0>,\n",
       " 'return': <gensim.models.keyedvectors.Vocab at 0x7fe71046c790>,\n",
       " 'college': <gensim.models.keyedvectors.Vocab at 0x7fe71046c7d0>,\n",
       " 'working': <gensim.models.keyedvectors.Vocab at 0x7fe71046c890>,\n",
       " 'community': <gensim.models.keyedvectors.Vocab at 0x7fe71046c8d0>,\n",
       " 'eight': <gensim.models.keyedvectors.Vocab at 0x7fe71046c990>,\n",
       " 'groups': <gensim.models.keyedvectors.Vocab at 0x7fe71046c9d0>,\n",
       " 'despite': <gensim.models.keyedvectors.Vocab at 0x7fe71046ca90>,\n",
       " 'level': <gensim.models.keyedvectors.Vocab at 0x7fe71046cad0>,\n",
       " 'largest': <gensim.models.keyedvectors.Vocab at 0x7fe71046cb90>,\n",
       " 'whose': <gensim.models.keyedvectors.Vocab at 0x7fe71046cbd0>,\n",
       " 'attacks': <gensim.models.keyedvectors.Vocab at 0x7fe71046cc90>,\n",
       " 'germany': <gensim.models.keyedvectors.Vocab at 0x7fe71046ccd0>,\n",
       " 'august': <gensim.models.keyedvectors.Vocab at 0x7fe71046cd90>,\n",
       " 'change': <gensim.models.keyedvectors.Vocab at 0x7fe71046cdd0>,\n",
       " 'church': <gensim.models.keyedvectors.Vocab at 0x7fe71046ce90>,\n",
       " 'nation': <gensim.models.keyedvectors.Vocab at 0x7fe71046ced0>,\n",
       " 'german': <gensim.models.keyedvectors.Vocab at 0x7fe71046cf90>,\n",
       " 'station': <gensim.models.keyedvectors.Vocab at 0x7fe71046cfd0>,\n",
       " 'london': <gensim.models.keyedvectors.Vocab at 0x7fe71046f090>,\n",
       " 'weeks': <gensim.models.keyedvectors.Vocab at 0x7fe71046f110>,\n",
       " 'having': <gensim.models.keyedvectors.Vocab at 0x7fe71046f190>,\n",
       " '18': <gensim.models.keyedvectors.Vocab at 0x7fe71046f210>,\n",
       " 'research': <gensim.models.keyedvectors.Vocab at 0x7fe71046f290>,\n",
       " 'black': <gensim.models.keyedvectors.Vocab at 0x7fe71046f310>,\n",
       " 'services': <gensim.models.keyedvectors.Vocab at 0x7fe71046f390>,\n",
       " 'story': <gensim.models.keyedvectors.Vocab at 0x7fe71046f410>,\n",
       " '6': <gensim.models.keyedvectors.Vocab at 0x7fe71046f490>,\n",
       " 'europe': <gensim.models.keyedvectors.Vocab at 0x7fe71046f4d0>,\n",
       " 'sales': <gensim.models.keyedvectors.Vocab at 0x7fe71046f590>,\n",
       " 'policy': <gensim.models.keyedvectors.Vocab at 0x7fe71046f5d0>,\n",
       " 'visit': <gensim.models.keyedvectors.Vocab at 0x7fe71046f690>,\n",
       " 'northern': <gensim.models.keyedvectors.Vocab at 0x7fe71046f6d0>,\n",
       " 'lot': <gensim.models.keyedvectors.Vocab at 0x7fe71046f790>,\n",
       " 'across': <gensim.models.keyedvectors.Vocab at 0x7fe71046f7d0>,\n",
       " 'per': <gensim.models.keyedvectors.Vocab at 0x7fe71046f890>,\n",
       " 'current': <gensim.models.keyedvectors.Vocab at 0x7fe71046f8d0>,\n",
       " 'board': <gensim.models.keyedvectors.Vocab at 0x7fe71046f990>,\n",
       " 'football': <gensim.models.keyedvectors.Vocab at 0x7fe71046f9d0>,\n",
       " 'ministry': <gensim.models.keyedvectors.Vocab at 0x7fe71046fa90>,\n",
       " 'workers': <gensim.models.keyedvectors.Vocab at 0x7fe71046fad0>,\n",
       " 'vote': <gensim.models.keyedvectors.Vocab at 0x7fe71046fb90>,\n",
       " 'book': <gensim.models.keyedvectors.Vocab at 0x7fe71046fbd0>,\n",
       " 'fell': <gensim.models.keyedvectors.Vocab at 0x7fe71046fc90>,\n",
       " 'seen': <gensim.models.keyedvectors.Vocab at 0x7fe71046fcd0>,\n",
       " 'role': <gensim.models.keyedvectors.Vocab at 0x7fe71046fd90>,\n",
       " 'students': <gensim.models.keyedvectors.Vocab at 0x7fe71046fdd0>,\n",
       " 'shares': <gensim.models.keyedvectors.Vocab at 0x7fe71046fe90>,\n",
       " 'iran': <gensim.models.keyedvectors.Vocab at 0x7fe71046fed0>,\n",
       " 'process': <gensim.models.keyedvectors.Vocab at 0x7fe71046ff90>,\n",
       " 'agreement': <gensim.models.keyedvectors.Vocab at 0x7fe71046ffd0>,\n",
       " 'quarter': <gensim.models.keyedvectors.Vocab at 0x7fe7104710d0>,\n",
       " 'full': <gensim.models.keyedvectors.Vocab at 0x7fe710471110>,\n",
       " 'match': <gensim.models.keyedvectors.Vocab at 0x7fe7104711d0>,\n",
       " 'started': <gensim.models.keyedvectors.Vocab at 0x7fe710471210>,\n",
       " 'growth': <gensim.models.keyedvectors.Vocab at 0x7fe7104712d0>,\n",
       " 'yet': <gensim.models.keyedvectors.Vocab at 0x7fe710471310>,\n",
       " 'moved': <gensim.models.keyedvectors.Vocab at 0x7fe7104713d0>,\n",
       " 'possible': <gensim.models.keyedvectors.Vocab at 0x7fe710471410>,\n",
       " 'western': <gensim.models.keyedvectors.Vocab at 0x7fe7104714d0>,\n",
       " 'special': <gensim.models.keyedvectors.Vocab at 0x7fe710471510>,\n",
       " '100': <gensim.models.keyedvectors.Vocab at 0x7fe7104715d0>,\n",
       " 'plans': <gensim.models.keyedvectors.Vocab at 0x7fe710471610>,\n",
       " 'interest': <gensim.models.keyedvectors.Vocab at 0x7fe7104716d0>,\n",
       " 'behind': <gensim.models.keyedvectors.Vocab at 0x7fe710471710>,\n",
       " 'strong': <gensim.models.keyedvectors.Vocab at 0x7fe7104717d0>,\n",
       " 'england': <gensim.models.keyedvectors.Vocab at 0x7fe710471810>,\n",
       " 'named': <gensim.models.keyedvectors.Vocab at 0x7fe7104718d0>,\n",
       " 'food': <gensim.models.keyedvectors.Vocab at 0x7fe710471910>,\n",
       " 'period': <gensim.models.keyedvectors.Vocab at 0x7fe7104719d0>,\n",
       " 'real': <gensim.models.keyedvectors.Vocab at 0x7fe710471a10>,\n",
       " 'authorities': <gensim.models.keyedvectors.Vocab at 0x7fe710471ad0>,\n",
       " 'car': <gensim.models.keyedvectors.Vocab at 0x7fe710471b10>,\n",
       " 'term': <gensim.models.keyedvectors.Vocab at 0x7fe710471bd0>,\n",
       " 'rate': <gensim.models.keyedvectors.Vocab at 0x7fe710471c10>,\n",
       " 'race': <gensim.models.keyedvectors.Vocab at 0x7fe710471cd0>,\n",
       " 'nearly': <gensim.models.keyedvectors.Vocab at 0x7fe710471d10>,\n",
       " 'korea': <gensim.models.keyedvectors.Vocab at 0x7fe710471dd0>,\n",
       " 'enough': <gensim.models.keyedvectors.Vocab at 0x7fe710471e10>,\n",
       " 'site': <gensim.models.keyedvectors.Vocab at 0x7fe710471ed0>,\n",
       " 'opposition': <gensim.models.keyedvectors.Vocab at 0x7fe710471f10>,\n",
       " 'keep': <gensim.models.keyedvectors.Vocab at 0x7fe710471fd0>,\n",
       " '25': <gensim.models.keyedvectors.Vocab at 0x7fe710474050>,\n",
       " 'call': <gensim.models.keyedvectors.Vocab at 0x7fe710474110>,\n",
       " 'future': <gensim.models.keyedvectors.Vocab at 0x7fe710474150>,\n",
       " 'taking': <gensim.models.keyedvectors.Vocab at 0x7fe710474210>,\n",
       " 'island': <gensim.models.keyedvectors.Vocab at 0x7fe710474250>,\n",
       " '2008': <gensim.models.keyedvectors.Vocab at 0x7fe710474310>,\n",
       " '2006': <gensim.models.keyedvectors.Vocab at 0x7fe710474350>,\n",
       " 'road': <gensim.models.keyedvectors.Vocab at 0x7fe710474410>,\n",
       " 'outside': <gensim.models.keyedvectors.Vocab at 0x7fe710474450>,\n",
       " 'really': <gensim.models.keyedvectors.Vocab at 0x7fe710474510>,\n",
       " 'century': <gensim.models.keyedvectors.Vocab at 0x7fe710474550>,\n",
       " 'democratic': <gensim.models.keyedvectors.Vocab at 0x7fe710474610>,\n",
       " 'almost': <gensim.models.keyedvectors.Vocab at 0x7fe710474650>,\n",
       " 'single': <gensim.models.keyedvectors.Vocab at 0x7fe710474710>,\n",
       " 'share': <gensim.models.keyedvectors.Vocab at 0x7fe710474750>,\n",
       " 'leading': <gensim.models.keyedvectors.Vocab at 0x7fe710474810>,\n",
       " 'trying': <gensim.models.keyedvectors.Vocab at 0x7fe710474850>,\n",
       " 'find': <gensim.models.keyedvectors.Vocab at 0x7fe710474910>,\n",
       " 'album': <gensim.models.keyedvectors.Vocab at 0x7fe710474950>,\n",
       " 'senior': <gensim.models.keyedvectors.Vocab at 0x7fe710474a10>,\n",
       " 'minutes': <gensim.models.keyedvectors.Vocab at 0x7fe710474a50>,\n",
       " 'together': <gensim.models.keyedvectors.Vocab at 0x7fe710474b10>,\n",
       " 'congress': <gensim.models.keyedvectors.Vocab at 0x7fe710474b50>,\n",
       " 'index': <gensim.models.keyedvectors.Vocab at 0x7fe710474c10>,\n",
       " 'australia': <gensim.models.keyedvectors.Vocab at 0x7fe710474c50>,\n",
       " 'results': <gensim.models.keyedvectors.Vocab at 0x7fe710474d10>,\n",
       " 'hard': <gensim.models.keyedvectors.Vocab at 0x7fe710474d50>,\n",
       " 'hours': <gensim.models.keyedvectors.Vocab at 0x7fe710474e10>,\n",
       " 'land': <gensim.models.keyedvectors.Vocab at 0x7fe710474e50>,\n",
       " 'action': <gensim.models.keyedvectors.Vocab at 0x7fe710474f10>,\n",
       " 'higher': <gensim.models.keyedvectors.Vocab at 0x7fe710474f50>,\n",
       " 'field': <gensim.models.keyedvectors.Vocab at 0x7fe7104770d0>,\n",
       " 'cut': <gensim.models.keyedvectors.Vocab at 0x7fe710477050>,\n",
       " 'coach': <gensim.models.keyedvectors.Vocab at 0x7fe7104771d0>,\n",
       " 'elections': <gensim.models.keyedvectors.Vocab at 0x7fe710477150>,\n",
       " 'san': <gensim.models.keyedvectors.Vocab at 0x7fe7104772d0>,\n",
       " 'issues': <gensim.models.keyedvectors.Vocab at 0x7fe710477250>,\n",
       " 'executive': <gensim.models.keyedvectors.Vocab at 0x7fe7104773d0>,\n",
       " 'february': <gensim.models.keyedvectors.Vocab at 0x7fe710477350>,\n",
       " 'production': <gensim.models.keyedvectors.Vocab at 0x7fe7104774d0>,\n",
       " 'areas': <gensim.models.keyedvectors.Vocab at 0x7fe710477450>,\n",
       " 'river': <gensim.models.keyedvectors.Vocab at 0x7fe7104775d0>,\n",
       " 'face': <gensim.models.keyedvectors.Vocab at 0x7fe710477550>,\n",
       " 'using': <gensim.models.keyedvectors.Vocab at 0x7fe710477690>,\n",
       " 'japanese': <gensim.models.keyedvectors.Vocab at 0x7fe710477650>,\n",
       " 'province': <gensim.models.keyedvectors.Vocab at 0x7fe710477790>,\n",
       " 'park': <gensim.models.keyedvectors.Vocab at 0x7fe710477750>,\n",
       " 'price': <gensim.models.keyedvectors.Vocab at 0x7fe710477890>,\n",
       " 'commission': <gensim.models.keyedvectors.Vocab at 0x7fe710477850>,\n",
       " 'california': <gensim.models.keyedvectors.Vocab at 0x7fe710477990>,\n",
       " 'father': <gensim.models.keyedvectors.Vocab at 0x7fe710477950>,\n",
       " 'son': <gensim.models.keyedvectors.Vocab at 0x7fe710477a90>,\n",
       " 'education': <gensim.models.keyedvectors.Vocab at 0x7fe710477a50>,\n",
       " '7': <gensim.models.keyedvectors.Vocab at 0x7fe710477b90>,\n",
       " 'village': <gensim.models.keyedvectors.Vocab at 0x7fe710477b10>,\n",
       " 'energy': <gensim.models.keyedvectors.Vocab at 0x7fe710477c90>,\n",
       " 'shot': <gensim.models.keyedvectors.Vocab at 0x7fe710477c10>,\n",
       " 'short': <gensim.models.keyedvectors.Vocab at 0x7fe710477d90>,\n",
       " 'africa': <gensim.models.keyedvectors.Vocab at 0x7fe710477d10>,\n",
       " 'key': <gensim.models.keyedvectors.Vocab at 0x7fe710477e90>,\n",
       " 'red': <gensim.models.keyedvectors.Vocab at 0x7fe710477e10>,\n",
       " 'association': <gensim.models.keyedvectors.Vocab at 0x7fe710477f90>,\n",
       " 'average': <gensim.models.keyedvectors.Vocab at 0x7fe710477f10>,\n",
       " 'pay': <gensim.models.keyedvectors.Vocab at 0x7fe7104790d0>,\n",
       " 'exchange': <gensim.models.keyedvectors.Vocab at 0x7fe710479050>,\n",
       " 'eu': <gensim.models.keyedvectors.Vocab at 0x7fe7104791d0>,\n",
       " 'something': <gensim.models.keyedvectors.Vocab at 0x7fe710479150>,\n",
       " 'gave': <gensim.models.keyedvectors.Vocab at 0x7fe7104792d0>,\n",
       " 'likely': <gensim.models.keyedvectors.Vocab at 0x7fe710479250>,\n",
       " 'player': <gensim.models.keyedvectors.Vocab at 0x7fe7104793d0>,\n",
       " 'george': <gensim.models.keyedvectors.Vocab at 0x7fe710479350>,\n",
       " '2007': <gensim.models.keyedvectors.Vocab at 0x7fe7104794d0>,\n",
       " 'victory': <gensim.models.keyedvectors.Vocab at 0x7fe710479450>,\n",
       " '8': <gensim.models.keyedvectors.Vocab at 0x7fe7104795d0>,\n",
       " 'low': <gensim.models.keyedvectors.Vocab at 0x7fe710479490>,\n",
       " 'things': <gensim.models.keyedvectors.Vocab at 0x7fe7104796d0>,\n",
       " '2010': <gensim.models.keyedvectors.Vocab at 0x7fe710479590>,\n",
       " 'pakistan': <gensim.models.keyedvectors.Vocab at 0x7fe7104797d0>,\n",
       " '14': <gensim.models.keyedvectors.Vocab at 0x7fe710479690>,\n",
       " 'post': <gensim.models.keyedvectors.Vocab at 0x7fe7104798d0>,\n",
       " 'social': <gensim.models.keyedvectors.Vocab at 0x7fe710479790>,\n",
       " 'continue': <gensim.models.keyedvectors.Vocab at 0x7fe7104799d0>,\n",
       " 'ever': <gensim.models.keyedvectors.Vocab at 0x7fe710479890>,\n",
       " 'look': <gensim.models.keyedvectors.Vocab at 0x7fe710479ad0>,\n",
       " 'chairman': <gensim.models.keyedvectors.Vocab at 0x7fe710479990>,\n",
       " 'job': <gensim.models.keyedvectors.Vocab at 0x7fe710479bd0>,\n",
       " '2000': <gensim.models.keyedvectors.Vocab at 0x7fe710479a90>,\n",
       " 'soldiers': <gensim.models.keyedvectors.Vocab at 0x7fe710479cd0>,\n",
       " 'able': <gensim.models.keyedvectors.Vocab at 0x7fe710479b90>,\n",
       " 'parliament': <gensim.models.keyedvectors.Vocab at 0x7fe710479dd0>,\n",
       " 'front': <gensim.models.keyedvectors.Vocab at 0x7fe710479c90>,\n",
       " 'himself': <gensim.models.keyedvectors.Vocab at 0x7fe710479ed0>,\n",
       " 'problems': <gensim.models.keyedvectors.Vocab at 0x7fe710479d90>,\n",
       " 'private': <gensim.models.keyedvectors.Vocab at 0x7fe710479fd0>,\n",
       " 'lower': <gensim.models.keyedvectors.Vocab at 0x7fe710479e90>,\n",
       " 'list': <gensim.models.keyedvectors.Vocab at 0x7fe71047b110>,\n",
       " 'built': <gensim.models.keyedvectors.Vocab at 0x7fe710479f90>,\n",
       " '13': <gensim.models.keyedvectors.Vocab at 0x7fe71047b210>,\n",
       " 'efforts': <gensim.models.keyedvectors.Vocab at 0x7fe71047b0d0>,\n",
       " 'dollar': <gensim.models.keyedvectors.Vocab at 0x7fe71047b310>,\n",
       " 'miles': <gensim.models.keyedvectors.Vocab at 0x7fe71047b1d0>,\n",
       " 'included': <gensim.models.keyedvectors.Vocab at 0x7fe71047b410>,\n",
       " 'radio': <gensim.models.keyedvectors.Vocab at 0x7fe71047b2d0>,\n",
       " 'live': <gensim.models.keyedvectors.Vocab at 0x7fe71047b510>,\n",
       " 'form': <gensim.models.keyedvectors.Vocab at 0x7fe71047b3d0>,\n",
       " 'david': <gensim.models.keyedvectors.Vocab at 0x7fe71047b610>,\n",
       " 'african': <gensim.models.keyedvectors.Vocab at 0x7fe71047b4d0>,\n",
       " 'increase': <gensim.models.keyedvectors.Vocab at 0x7fe71047b710>,\n",
       " 'reports': <gensim.models.keyedvectors.Vocab at 0x7fe71047b5d0>,\n",
       " 'sent': <gensim.models.keyedvectors.Vocab at 0x7fe71047b810>,\n",
       " 'fourth': <gensim.models.keyedvectors.Vocab at 0x7fe71047b6d0>,\n",
       " 'always': <gensim.models.keyedvectors.Vocab at 0x7fe71047b910>,\n",
       " 'king': <gensim.models.keyedvectors.Vocab at 0x7fe71047b7d0>,\n",
       " '50': <gensim.models.keyedvectors.Vocab at 0x7fe71047ba10>,\n",
       " 'tax': <gensim.models.keyedvectors.Vocab at 0x7fe71047b8d0>,\n",
       " 'taiwan': <gensim.models.keyedvectors.Vocab at 0x7fe71047bb10>,\n",
       " 'britain': <gensim.models.keyedvectors.Vocab at 0x7fe71047b9d0>,\n",
       " '16': <gensim.models.keyedvectors.Vocab at 0x7fe71047bc10>,\n",
       " 'playing': <gensim.models.keyedvectors.Vocab at 0x7fe71047bad0>,\n",
       " 'title': <gensim.models.keyedvectors.Vocab at 0x7fe71047bd10>,\n",
       " 'middle': <gensim.models.keyedvectors.Vocab at 0x7fe71047bbd0>,\n",
       " 'meet': <gensim.models.keyedvectors.Vocab at 0x7fe71047be10>,\n",
       " 'global': <gensim.models.keyedvectors.Vocab at 0x7fe71047bcd0>,\n",
       " 'wife': <gensim.models.keyedvectors.Vocab at 0x7fe71047bf10>,\n",
       " '2009': <gensim.models.keyedvectors.Vocab at 0x7fe71047bdd0>,\n",
       " 'position': <gensim.models.keyedvectors.Vocab at 0x7fe71047f050>,\n",
       " 'located': <gensim.models.keyedvectors.Vocab at 0x7fe71047bed0>,\n",
       " 'clear': <gensim.models.keyedvectors.Vocab at 0x7fe71047f150>,\n",
       " 'ahead': <gensim.models.keyedvectors.Vocab at 0x7fe71047bfd0>,\n",
       " '2004': <gensim.models.keyedvectors.Vocab at 0x7fe71047f250>,\n",
       " '2005': <gensim.models.keyedvectors.Vocab at 0x7fe71047f110>,\n",
       " 'iraqi': <gensim.models.keyedvectors.Vocab at 0x7fe71047f350>,\n",
       " 'english': <gensim.models.keyedvectors.Vocab at 0x7fe71047f210>,\n",
       " 'result': <gensim.models.keyedvectors.Vocab at 0x7fe71047f450>,\n",
       " 'release': <gensim.models.keyedvectors.Vocab at 0x7fe71047f310>,\n",
       " 'violence': <gensim.models.keyedvectors.Vocab at 0x7fe71047f550>,\n",
       " 'goal': <gensim.models.keyedvectors.Vocab at 0x7fe71047f410>,\n",
       " 'project': <gensim.models.keyedvectors.Vocab at 0x7fe71047f650>,\n",
       " 'closed': <gensim.models.keyedvectors.Vocab at 0x7fe71047f510>,\n",
       " 'border': <gensim.models.keyedvectors.Vocab at 0x7fe71047f750>,\n",
       " 'body': <gensim.models.keyedvectors.Vocab at 0x7fe71047f610>,\n",
       " 'soon': <gensim.models.keyedvectors.Vocab at 0x7fe71047f850>,\n",
       " 'crisis': <gensim.models.keyedvectors.Vocab at 0x7fe71047f710>,\n",
       " 'division': <gensim.models.keyedvectors.Vocab at 0x7fe71047f950>,\n",
       " '&amp;': <gensim.models.keyedvectors.Vocab at 0x7fe71047f810>,\n",
       " 'served': <gensim.models.keyedvectors.Vocab at 0x7fe71047fa10>,\n",
       " 'tour': <gensim.models.keyedvectors.Vocab at 0x7fe71047f910>,\n",
       " 'hospital': <gensim.models.keyedvectors.Vocab at 0x7fe71047fb10>,\n",
       " 'kong': <gensim.models.keyedvectors.Vocab at 0x7fe71047fa90>,\n",
       " 'test': <gensim.models.keyedvectors.Vocab at 0x7fe71047fc10>,\n",
       " 'hong': <gensim.models.keyedvectors.Vocab at 0x7fe71047fb90>,\n",
       " 'u.n.': <gensim.models.keyedvectors.Vocab at 0x7fe71047fd10>,\n",
       " 'inc.': <gensim.models.keyedvectors.Vocab at 0x7fe71047fc90>,\n",
       " 'technology': <gensim.models.keyedvectors.Vocab at 0x7fe71047fe10>,\n",
       " 'believe': <gensim.models.keyedvectors.Vocab at 0x7fe71047fd90>,\n",
       " 'organization': <gensim.models.keyedvectors.Vocab at 0x7fe71047ff10>,\n",
       " 'published': <gensim.models.keyedvectors.Vocab at 0x7fe71047fe90>,\n",
       " 'weapons': <gensim.models.keyedvectors.Vocab at 0x7fe710480050>,\n",
       " 'agreed': <gensim.models.keyedvectors.Vocab at 0x7fe71047ff90>,\n",
       " 'why': <gensim.models.keyedvectors.Vocab at 0x7fe710480150>,\n",
       " 'nine': <gensim.models.keyedvectors.Vocab at 0x7fe7104800d0>,\n",
       " 'summer': <gensim.models.keyedvectors.Vocab at 0x7fe710480250>,\n",
       " 'wanted': <gensim.models.keyedvectors.Vocab at 0x7fe7104801d0>,\n",
       " 'republican': <gensim.models.keyedvectors.Vocab at 0x7fe710480350>,\n",
       " 'act': <gensim.models.keyedvectors.Vocab at 0x7fe7104802d0>,\n",
       " 'recently': <gensim.models.keyedvectors.Vocab at 0x7fe710480450>,\n",
       " 'texas': <gensim.models.keyedvectors.Vocab at 0x7fe7104803d0>,\n",
       " 'course': <gensim.models.keyedvectors.Vocab at 0x7fe710480550>,\n",
       " 'problem': <gensim.models.keyedvectors.Vocab at 0x7fe7104804d0>,\n",
       " 'senate': <gensim.models.keyedvectors.Vocab at 0x7fe710480650>,\n",
       " 'medical': <gensim.models.keyedvectors.Vocab at 0x7fe7104805d0>,\n",
       " 'un': <gensim.models.keyedvectors.Vocab at 0x7fe710480750>,\n",
       " 'done': <gensim.models.keyedvectors.Vocab at 0x7fe7104806d0>,\n",
       " 'reached': <gensim.models.keyedvectors.Vocab at 0x7fe710480850>,\n",
       " 'star': <gensim.models.keyedvectors.Vocab at 0x7fe7104807d0>,\n",
       " 'continued': <gensim.models.keyedvectors.Vocab at 0x7fe710480950>,\n",
       " 'investors': <gensim.models.keyedvectors.Vocab at 0x7fe7104808d0>,\n",
       " 'living': <gensim.models.keyedvectors.Vocab at 0x7fe710480a50>,\n",
       " 'care': <gensim.models.keyedvectors.Vocab at 0x7fe7104809d0>,\n",
       " 'signed': <gensim.models.keyedvectors.Vocab at 0x7fe710480b50>,\n",
       " '17': <gensim.models.keyedvectors.Vocab at 0x7fe710480ad0>,\n",
       " 'art': <gensim.models.keyedvectors.Vocab at 0x7fe710480c50>,\n",
       " 'provide': <gensim.models.keyedvectors.Vocab at 0x7fe710480bd0>,\n",
       " 'worked': <gensim.models.keyedvectors.Vocab at 0x7fe710480d50>,\n",
       " 'presidential': <gensim.models.keyedvectors.Vocab at 0x7fe710480cd0>,\n",
       " 'gold': <gensim.models.keyedvectors.Vocab at 0x7fe710480e50>,\n",
       " 'obama': <gensim.models.keyedvectors.Vocab at 0x7fe710480dd0>,\n",
       " 'morning': <gensim.models.keyedvectors.Vocab at 0x7fe710480f50>,\n",
       " 'dead': <gensim.models.keyedvectors.Vocab at 0x7fe710480ed0>,\n",
       " 'opened': <gensim.models.keyedvectors.Vocab at 0x7fe710484090>,\n",
       " \"'ll\": <gensim.models.keyedvectors.Vocab at 0x7fe710480fd0>,\n",
       " 'event': <gensim.models.keyedvectors.Vocab at 0x7fe710484190>,\n",
       " 'previous': <gensim.models.keyedvectors.Vocab at 0x7fe710484110>,\n",
       " 'cost': <gensim.models.keyedvectors.Vocab at 0x7fe710484290>,\n",
       " 'instead': <gensim.models.keyedvectors.Vocab at 0x7fe710484210>,\n",
       " 'canada': <gensim.models.keyedvectors.Vocab at 0x7fe710484390>,\n",
       " 'band': <gensim.models.keyedvectors.Vocab at 0x7fe710484310>,\n",
       " 'teams': <gensim.models.keyedvectors.Vocab at 0x7fe710484490>,\n",
       " 'daily': <gensim.models.keyedvectors.Vocab at 0x7fe710484410>,\n",
       " '2001': <gensim.models.keyedvectors.Vocab at 0x7fe710484590>,\n",
       " 'available': <gensim.models.keyedvectors.Vocab at 0x7fe710484510>,\n",
       " 'drug': <gensim.models.keyedvectors.Vocab at 0x7fe710484690>,\n",
       " 'coming': <gensim.models.keyedvectors.Vocab at 0x7fe710484610>,\n",
       " '2003': <gensim.models.keyedvectors.Vocab at 0x7fe710484790>,\n",
       " 'investment': <gensim.models.keyedvectors.Vocab at 0x7fe710484710>,\n",
       " '’s': <gensim.models.keyedvectors.Vocab at 0x7fe710484890>,\n",
       " 'michael': <gensim.models.keyedvectors.Vocab at 0x7fe710484750>,\n",
       " 'civil': <gensim.models.keyedvectors.Vocab at 0x7fe710484990>,\n",
       " 'woman': <gensim.models.keyedvectors.Vocab at 0x7fe710484850>,\n",
       " 'training': <gensim.models.keyedvectors.Vocab at 0x7fe710484a90>,\n",
       " 'appeared': <gensim.models.keyedvectors.Vocab at 0x7fe710484950>,\n",
       " '9': <gensim.models.keyedvectors.Vocab at 0x7fe710484b90>,\n",
       " 'involved': <gensim.models.keyedvectors.Vocab at 0x7fe710484a10>,\n",
       " 'indian': <gensim.models.keyedvectors.Vocab at 0x7fe710484c90>,\n",
       " 'similar': <gensim.models.keyedvectors.Vocab at 0x7fe710484b10>,\n",
       " 'situation': <gensim.models.keyedvectors.Vocab at 0x7fe710484d90>,\n",
       " '24': <gensim.models.keyedvectors.Vocab at 0x7fe710484c10>,\n",
       " 'los': <gensim.models.keyedvectors.Vocab at 0x7fe710484e90>,\n",
       " 'running': <gensim.models.keyedvectors.Vocab at 0x7fe710484d10>,\n",
       " 'fighting': <gensim.models.keyedvectors.Vocab at 0x7fe710484f90>,\n",
       " 'mark': <gensim.models.keyedvectors.Vocab at 0x7fe710484e10>,\n",
       " '40': <gensim.models.keyedvectors.Vocab at 0x7fe7104850d0>,\n",
       " 'trial': <gensim.models.keyedvectors.Vocab at 0x7fe710484f10>,\n",
       " 'hold': <gensim.models.keyedvectors.Vocab at 0x7fe7104851d0>,\n",
       " 'australian': <gensim.models.keyedvectors.Vocab at 0x7fe710485050>,\n",
       " 'thought': <gensim.models.keyedvectors.Vocab at 0x7fe7104852d0>,\n",
       " '!': <gensim.models.keyedvectors.Vocab at 0x7fe710485150>,\n",
       " 'study': <gensim.models.keyedvectors.Vocab at 0x7fe710485310>,\n",
       " 'fall': <gensim.models.keyedvectors.Vocab at 0x7fe710485250>,\n",
       " 'mother': <gensim.models.keyedvectors.Vocab at 0x7fe710485410>,\n",
       " 'met': <gensim.models.keyedvectors.Vocab at 0x7fe710485350>,\n",
       " 'relations': <gensim.models.keyedvectors.Vocab at 0x7fe710485510>,\n",
       " 'anti': <gensim.models.keyedvectors.Vocab at 0x7fe710485450>,\n",
       " '2002': <gensim.models.keyedvectors.Vocab at 0x7fe710485610>,\n",
       " 'song': <gensim.models.keyedvectors.Vocab at 0x7fe710485550>,\n",
       " 'popular': <gensim.models.keyedvectors.Vocab at 0x7fe710485710>,\n",
       " 'base': <gensim.models.keyedvectors.Vocab at 0x7fe710485650>,\n",
       " 'tv': <gensim.models.keyedvectors.Vocab at 0x7fe710485810>,\n",
       " 'ground': <gensim.models.keyedvectors.Vocab at 0x7fe710485750>,\n",
       " 'markets': <gensim.models.keyedvectors.Vocab at 0x7fe710485910>,\n",
       " 'ii': <gensim.models.keyedvectors.Vocab at 0x7fe710485850>,\n",
       " 'newspaper': <gensim.models.keyedvectors.Vocab at 0x7fe710485a10>,\n",
       " 'staff': <gensim.models.keyedvectors.Vocab at 0x7fe710485950>,\n",
       " 'saw': <gensim.models.keyedvectors.Vocab at 0x7fe710485b10>,\n",
       " 'hand': <gensim.models.keyedvectors.Vocab at 0x7fe710485a50>,\n",
       " 'hope': <gensim.models.keyedvectors.Vocab at 0x7fe710485c10>,\n",
       " 'operations': <gensim.models.keyedvectors.Vocab at 0x7fe710485b50>,\n",
       " 'pressure': <gensim.models.keyedvectors.Vocab at 0x7fe710485d10>,\n",
       " 'americans': <gensim.models.keyedvectors.Vocab at 0x7fe710485c50>,\n",
       " 'eastern': <gensim.models.keyedvectors.Vocab at 0x7fe710485e10>,\n",
       " 'st.': <gensim.models.keyedvectors.Vocab at 0x7fe710485d50>,\n",
       " 'legal': <gensim.models.keyedvectors.Vocab at 0x7fe710485f10>,\n",
       " 'asia': <gensim.models.keyedvectors.Vocab at 0x7fe710485e50>,\n",
       " 'budget': <gensim.models.keyedvectors.Vocab at 0x7fe710488050>,\n",
       " 'returned': <gensim.models.keyedvectors.Vocab at 0x7fe710485f50>,\n",
       " 'considered': <gensim.models.keyedvectors.Vocab at 0x7fe710488110>,\n",
       " 'love': <gensim.models.keyedvectors.Vocab at 0x7fe710488090>,\n",
       " 'wrote': <gensim.models.keyedvectors.Vocab at 0x7fe710488210>,\n",
       " 'stop': <gensim.models.keyedvectors.Vocab at 0x7fe710488190>,\n",
       " 'fight': <gensim.models.keyedvectors.Vocab at 0x7fe710488310>,\n",
       " 'currently': <gensim.models.keyedvectors.Vocab at 0x7fe710488290>,\n",
       " 'charges': <gensim.models.keyedvectors.Vocab at 0x7fe710488410>,\n",
       " 'try': <gensim.models.keyedvectors.Vocab at 0x7fe710488390>,\n",
       " 'aid': <gensim.models.keyedvectors.Vocab at 0x7fe710488510>,\n",
       " 'ended': <gensim.models.keyedvectors.Vocab at 0x7fe710488490>,\n",
       " 'management': <gensim.models.keyedvectors.Vocab at 0x7fe710488610>,\n",
       " 'brought': <gensim.models.keyedvectors.Vocab at 0x7fe710488590>,\n",
       " 'cases': <gensim.models.keyedvectors.Vocab at 0x7fe710488710>,\n",
       " 'decided': <gensim.models.keyedvectors.Vocab at 0x7fe710488690>,\n",
       " 'failed': <gensim.models.keyedvectors.Vocab at 0x7fe710488810>,\n",
       " 'network': <gensim.models.keyedvectors.Vocab at 0x7fe710488790>,\n",
       " 'works': <gensim.models.keyedvectors.Vocab at 0x7fe710488910>,\n",
       " 'gas': <gensim.models.keyedvectors.Vocab at 0x7fe710488890>,\n",
       " 'turned': <gensim.models.keyedvectors.Vocab at 0x7fe710488a10>,\n",
       " 'fact': <gensim.models.keyedvectors.Vocab at 0x7fe710488990>,\n",
       " 'vice': <gensim.models.keyedvectors.Vocab at 0x7fe710488ad0>,\n",
       " 'ca': <gensim.models.keyedvectors.Vocab at 0x7fe710488b10>,\n",
       " 'mexico': <gensim.models.keyedvectors.Vocab at 0x7fe710488bd0>,\n",
       " 'trading': <gensim.models.keyedvectors.Vocab at 0x7fe710488c10>,\n",
       " 'especially': <gensim.models.keyedvectors.Vocab at 0x7fe710488cd0>,\n",
       " 'reporters': <gensim.models.keyedvectors.Vocab at 0x7fe710488d10>,\n",
       " 'afghanistan': <gensim.models.keyedvectors.Vocab at 0x7fe710488dd0>,\n",
       " 'common': <gensim.models.keyedvectors.Vocab at 0x7fe710488e10>,\n",
       " 'looking': <gensim.models.keyedvectors.Vocab at 0x7fe710488ed0>,\n",
       " 'space': <gensim.models.keyedvectors.Vocab at 0x7fe710488f10>,\n",
       " 'rates': <gensim.models.keyedvectors.Vocab at 0x7fe710488fd0>,\n",
       " 'manager': <gensim.models.keyedvectors.Vocab at 0x7fe70352f050>,\n",
       " 'loss': <gensim.models.keyedvectors.Vocab at 0x7fe70352f110>,\n",
       " '2011': <gensim.models.keyedvectors.Vocab at 0x7fe70352f150>,\n",
       " 'justice': <gensim.models.keyedvectors.Vocab at 0x7fe70352f210>,\n",
       " 'thousands': <gensim.models.keyedvectors.Vocab at 0x7fe70352f250>,\n",
       " 'james': <gensim.models.keyedvectors.Vocab at 0x7fe70352f310>,\n",
       " 'rather': <gensim.models.keyedvectors.Vocab at 0x7fe70352f350>,\n",
       " 'fund': <gensim.models.keyedvectors.Vocab at 0x7fe70352f410>,\n",
       " 'thing': <gensim.models.keyedvectors.Vocab at 0x7fe70352f450>,\n",
       " 'republic': <gensim.models.keyedvectors.Vocab at 0x7fe70352f510>,\n",
       " 'opening': <gensim.models.keyedvectors.Vocab at 0x7fe70352f550>,\n",
       " 'accused': <gensim.models.keyedvectors.Vocab at 0x7fe70352f610>,\n",
       " 'winning': <gensim.models.keyedvectors.Vocab at 0x7fe70352f650>,\n",
       " 'scored': <gensim.models.keyedvectors.Vocab at 0x7fe70352f710>,\n",
       " 'championship': <gensim.models.keyedvectors.Vocab at 0x7fe70352f750>,\n",
       " 'example': <gensim.models.keyedvectors.Vocab at 0x7fe70352f810>,\n",
       " 'getting': <gensim.models.keyedvectors.Vocab at 0x7fe70352f850>,\n",
       " 'biggest': <gensim.models.keyedvectors.Vocab at 0x7fe70352f910>,\n",
       " 'performance': <gensim.models.keyedvectors.Vocab at 0x7fe70352f950>,\n",
       " 'sports': <gensim.models.keyedvectors.Vocab at 0x7fe70352fa10>,\n",
       " '1998': <gensim.models.keyedvectors.Vocab at 0x7fe70352fa50>,\n",
       " 'let': <gensim.models.keyedvectors.Vocab at 0x7fe70352fb10>,\n",
       " 'allowed': <gensim.models.keyedvectors.Vocab at 0x7fe70352fb50>,\n",
       " 'schools': <gensim.models.keyedvectors.Vocab at 0x7fe70352fc10>,\n",
       " 'means': <gensim.models.keyedvectors.Vocab at 0x7fe70352fc50>,\n",
       " 'turn': <gensim.models.keyedvectors.Vocab at 0x7fe70352fd10>,\n",
       " 'leave': <gensim.models.keyedvectors.Vocab at 0x7fe70352fd50>,\n",
       " 'no.': <gensim.models.keyedvectors.Vocab at 0x7fe70352fe10>,\n",
       " 'robert': <gensim.models.keyedvectors.Vocab at 0x7fe70352fe50>,\n",
       " 'personal': <gensim.models.keyedvectors.Vocab at 0x7fe70352ff10>,\n",
       " 'stocks': <gensim.models.keyedvectors.Vocab at 0x7fe70352ff50>,\n",
       " 'showed': <gensim.models.keyedvectors.Vocab at 0x7fe703531050>,\n",
       " 'light': <gensim.models.keyedvectors.Vocab at 0x7fe703531090>,\n",
       " 'arrested': <gensim.models.keyedvectors.Vocab at 0x7fe703531150>,\n",
       " 'person': <gensim.models.keyedvectors.Vocab at 0x7fe703531190>,\n",
       " 'either': <gensim.models.keyedvectors.Vocab at 0x7fe703531250>,\n",
       " 'offer': <gensim.models.keyedvectors.Vocab at 0x7fe703531290>,\n",
       " 'majority': <gensim.models.keyedvectors.Vocab at 0x7fe703531350>,\n",
       " 'battle': <gensim.models.keyedvectors.Vocab at 0x7fe703531390>,\n",
       " '19': <gensim.models.keyedvectors.Vocab at 0x7fe703531450>,\n",
       " 'class': <gensim.models.keyedvectors.Vocab at 0x7fe703531490>,\n",
       " 'evidence': <gensim.models.keyedvectors.Vocab at 0x7fe703531550>,\n",
       " 'makes': <gensim.models.keyedvectors.Vocab at 0x7fe703531590>,\n",
       " 'society': <gensim.models.keyedvectors.Vocab at 0x7fe703531650>,\n",
       " 'products': <gensim.models.keyedvectors.Vocab at 0x7fe703531690>,\n",
       " 'regional': <gensim.models.keyedvectors.Vocab at 0x7fe703531750>,\n",
       " 'needed': <gensim.models.keyedvectors.Vocab at 0x7fe703531790>,\n",
       " 'stage': <gensim.models.keyedvectors.Vocab at 0x7fe703531850>,\n",
       " 'am': <gensim.models.keyedvectors.Vocab at 0x7fe703531890>,\n",
       " 'doing': <gensim.models.keyedvectors.Vocab at 0x7fe703531950>,\n",
       " 'families': <gensim.models.keyedvectors.Vocab at 0x7fe703531990>,\n",
       " 'construction': <gensim.models.keyedvectors.Vocab at 0x7fe703531a50>,\n",
       " 'various': <gensim.models.keyedvectors.Vocab at 0x7fe703531a90>,\n",
       " '1996': <gensim.models.keyedvectors.Vocab at 0x7fe703531b50>,\n",
       " 'sold': <gensim.models.keyedvectors.Vocab at 0x7fe703531b90>,\n",
       " 'independent': <gensim.models.keyedvectors.Vocab at 0x7fe703531c50>,\n",
       " 'kind': <gensim.models.keyedvectors.Vocab at 0x7fe703531c90>,\n",
       " 'airport': <gensim.models.keyedvectors.Vocab at 0x7fe703531d50>,\n",
       " 'paul': <gensim.models.keyedvectors.Vocab at 0x7fe703531d90>,\n",
       " 'judge': <gensim.models.keyedvectors.Vocab at 0x7fe703531e50>,\n",
       " 'internet': <gensim.models.keyedvectors.Vocab at 0x7fe703531e90>,\n",
       " 'movement': <gensim.models.keyedvectors.Vocab at 0x7fe703531f50>,\n",
       " 'room': <gensim.models.keyedvectors.Vocab at 0x7fe703531f90>,\n",
       " 'followed': <gensim.models.keyedvectors.Vocab at 0x7fe703533090>,\n",
       " 'original': <gensim.models.keyedvectors.Vocab at 0x7fe7035330d0>,\n",
       " 'angeles': <gensim.models.keyedvectors.Vocab at 0x7fe703533190>,\n",
       " 'italy': <gensim.models.keyedvectors.Vocab at 0x7fe7035331d0>,\n",
       " '`': <gensim.models.keyedvectors.Vocab at 0x7fe703533290>,\n",
       " 'data': <gensim.models.keyedvectors.Vocab at 0x7fe703533210>,\n",
       " 'comes': <gensim.models.keyedvectors.Vocab at 0x7fe703533390>,\n",
       " 'parties': <gensim.models.keyedvectors.Vocab at 0x7fe703533310>,\n",
       " 'nothing': <gensim.models.keyedvectors.Vocab at 0x7fe703533490>,\n",
       " 'sea': <gensim.models.keyedvectors.Vocab at 0x7fe703533410>,\n",
       " 'bring': <gensim.models.keyedvectors.Vocab at 0x7fe703533590>,\n",
       " '2012': <gensim.models.keyedvectors.Vocab at 0x7fe703533510>,\n",
       " 'annual': <gensim.models.keyedvectors.Vocab at 0x7fe703533690>,\n",
       " 'officer': <gensim.models.keyedvectors.Vocab at 0x7fe703533610>,\n",
       " 'beijing': <gensim.models.keyedvectors.Vocab at 0x7fe703533790>,\n",
       " 'present': <gensim.models.keyedvectors.Vocab at 0x7fe703533710>,\n",
       " 'remain': <gensim.models.keyedvectors.Vocab at 0x7fe703533890>,\n",
       " 'nato': <gensim.models.keyedvectors.Vocab at 0x7fe703533810>,\n",
       " '1999': <gensim.models.keyedvectors.Vocab at 0x7fe703533990>,\n",
       " '22': <gensim.models.keyedvectors.Vocab at 0x7fe703533910>,\n",
       " 'remains': <gensim.models.keyedvectors.Vocab at 0x7fe703533a90>,\n",
       " 'allow': <gensim.models.keyedvectors.Vocab at 0x7fe703533a10>,\n",
       " 'florida': <gensim.models.keyedvectors.Vocab at 0x7fe703533b90>,\n",
       " 'computer': <gensim.models.keyedvectors.Vocab at 0x7fe703533b10>,\n",
       " '21': <gensim.models.keyedvectors.Vocab at 0x7fe703533c90>,\n",
       " 'contract': <gensim.models.keyedvectors.Vocab at 0x7fe703533c10>,\n",
       " 'coast': <gensim.models.keyedvectors.Vocab at 0x7fe703533d90>,\n",
       " 'created': <gensim.models.keyedvectors.Vocab at 0x7fe703533d10>,\n",
       " 'demand': <gensim.models.keyedvectors.Vocab at 0x7fe703533e90>,\n",
       " 'operation': <gensim.models.keyedvectors.Vocab at 0x7fe703533e10>,\n",
       " 'events': <gensim.models.keyedvectors.Vocab at 0x7fe703537050>,\n",
       " 'islamic': <gensim.models.keyedvectors.Vocab at 0x7fe703533f10>,\n",
       " 'beat': <gensim.models.keyedvectors.Vocab at 0x7fe7035370d0>,\n",
       " 'analysts': <gensim.models.keyedvectors.Vocab at 0x7fe703533fd0>,\n",
       " 'interview': <gensim.models.keyedvectors.Vocab at 0x7fe7035371d0>,\n",
       " 'helped': <gensim.models.keyedvectors.Vocab at 0x7fe703537150>,\n",
       " 'child': <gensim.models.keyedvectors.Vocab at 0x7fe7035372d0>,\n",
       " 'probably': <gensim.models.keyedvectors.Vocab at 0x7fe703537250>,\n",
       " 'spent': <gensim.models.keyedvectors.Vocab at 0x7fe7035373d0>,\n",
       " 'asian': <gensim.models.keyedvectors.Vocab at 0x7fe703537350>,\n",
       " 'effort': <gensim.models.keyedvectors.Vocab at 0x7fe7035374d0>,\n",
       " 'cooperation': <gensim.models.keyedvectors.Vocab at 0x7fe703537450>,\n",
       " 'shows': <gensim.models.keyedvectors.Vocab at 0x7fe7035375d0>,\n",
       " 'calls': <gensim.models.keyedvectors.Vocab at 0x7fe703537550>,\n",
       " 'investigation': <gensim.models.keyedvectors.Vocab at 0x7fe7035376d0>,\n",
       " 'lives': <gensim.models.keyedvectors.Vocab at 0x7fe703537650>,\n",
       " 'video': <gensim.models.keyedvectors.Vocab at 0x7fe7035377d0>,\n",
       " 'yen': <gensim.models.keyedvectors.Vocab at 0x7fe703537750>,\n",
       " 'runs': <gensim.models.keyedvectors.Vocab at 0x7fe7035378d0>,\n",
       " 'tried': <gensim.models.keyedvectors.Vocab at 0x7fe703537850>,\n",
       " 'bad': <gensim.models.keyedvectors.Vocab at 0x7fe7035379d0>,\n",
       " 'described': <gensim.models.keyedvectors.Vocab at 0x7fe703537950>,\n",
       " '1994': <gensim.models.keyedvectors.Vocab at 0x7fe703537ad0>,\n",
       " 'toward': <gensim.models.keyedvectors.Vocab at 0x7fe703537a50>,\n",
       " 'written': <gensim.models.keyedvectors.Vocab at 0x7fe703537bd0>,\n",
       " 'throughout': <gensim.models.keyedvectors.Vocab at 0x7fe703537b50>,\n",
       " 'established': <gensim.models.keyedvectors.Vocab at 0x7fe703537cd0>,\n",
       " 'mission': <gensim.models.keyedvectors.Vocab at 0x7fe703537c50>,\n",
       " 'associated': <gensim.models.keyedvectors.Vocab at 0x7fe703537d90>,\n",
       " 'buy': <gensim.models.keyedvectors.Vocab at 0x7fe703537dd0>,\n",
       " 'growing': <gensim.models.keyedvectors.Vocab at 0x7fe703537e90>,\n",
       " 'green': <gensim.models.keyedvectors.Vocab at 0x7fe703537ed0>,\n",
       " 'forward': <gensim.models.keyedvectors.Vocab at 0x7fe703537f90>,\n",
       " 'competition': <gensim.models.keyedvectors.Vocab at 0x7fe703537fd0>,\n",
       " 'poor': <gensim.models.keyedvectors.Vocab at 0x7fe703538090>,\n",
       " 'latest': <gensim.models.keyedvectors.Vocab at 0x7fe703538110>,\n",
       " 'banks': <gensim.models.keyedvectors.Vocab at 0x7fe703538190>,\n",
       " 'question': <gensim.models.keyedvectors.Vocab at 0x7fe703538210>,\n",
       " '1997': <gensim.models.keyedvectors.Vocab at 0x7fe703538290>,\n",
       " 'prison': <gensim.models.keyedvectors.Vocab at 0x7fe703538310>,\n",
       " 'feel': <gensim.models.keyedvectors.Vocab at 0x7fe703538390>,\n",
       " 'attention': <gensim.models.keyedvectors.Vocab at 0x7fe703538410>,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4acd0a8cc22446d9b61b36518d6ec14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=624181.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximebonnin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/Users/maximebonnin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:74: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(624181, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    #print(f\"For n_clusters = {k}\")\n",
    "    #print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    #print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=vectorized_docs,\n",
    "    k=50,\n",
    "    mb=500,\n",
    "    print_silhouette_values=False,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": df[\"text\"],\n",
    "    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n",
    "    \"cluster\": cluster_labels\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = MiniBatchKMeans(n_clusters=50, batch_size=500).fit(vectorized_docs)\n",
    "predictions = clustering.predict(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2445831   0.21664964  0.17667985 ...  0.06620023 -0.00455149\n",
      "  -0.0507665 ]\n",
      " [ 0.2019616   0.21072896  0.08947049 ...  0.06433008 -0.03125115\n",
      "  -0.03070114]\n",
      " [ 0.15431985  0.22170708  0.0817633  ...  0.00710959 -0.06185143\n",
      "  -0.01639712]\n",
      " ...\n",
      " [ 0.02618253  0.14670855  0.01277673 ...  0.10819402  0.05224674\n",
      "  -0.00990068]\n",
      " [ 0.21577488  0.17654657  0.08149476 ...  0.0534032  -0.0493921\n",
      "   0.00304876]\n",
      " [ 0.1964553   0.20334814  0.12340484 ... -0.05235037 -0.01298244\n",
      "  -0.03646562]]\n"
     ]
    }
   ],
   "source": [
    "print(clustering.cluster_centers_)\n",
    "np.save(\"cluster_centers.npy\", clustering.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(data)[:,0]\n",
    "del data, tokenized_docs, model, texts\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a876073a094640f4a50fea3a3c86ea00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#results[\"cluster_centers\"] = clustering.cluster_centers_\n",
    "results = {}\n",
    "\n",
    "for i,id_ in tqdm(enumerate(ids)):\n",
    "    results[id_] = {}\n",
    "    results[id_][\"vector\"] = str(list(vectorized_docs[i]))\n",
    "    results[id_][\"cluster\"] = int(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Vectors_and_clusters.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'vector': '[2.00304e-06, 1.0]', 'pred': 1}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "s = {\"1\":{\"vector\":str(list(np.array([0.00000200304002001000001000000000000001,1],dtype=np.float32))),\"pred\":1}}\n",
    "print(s)\n",
    "with open('Vectors_and_clusters.json', 'w') as f:\n",
    "    json.dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: example specific particular instance similar \n",
      "Cluster 1: example instance same this well \n",
      "Cluster 2: en que la con de \n",
      "Cluster 3: system using data use example \n",
      "Cluster 4: example instance same this use \n",
      "Cluster 5: particular specific example certain instance \n",
      "Cluster 6: any specific instance example means \n",
      "Cluster 7: und eine der über „ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximebonnin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 8: example instance particular similar specific \n",
      "Cluster 9: provide example instance provided use \n",
      "Cluster 10: using uses system allows use \n",
      "Cluster 11: system example provide use creating \n",
      "Cluster 12: la de et du en \n",
      "Cluster 13: using instance specific data allows \n",
      "Cluster 14: using allows available use system \n",
      "Cluster 15: specific example particular instance use \n",
      "Cluster 16: using system allows directly use \n",
      "Cluster 17: example specific i.e. particular function \n",
      "Cluster 18: example specific using uses particular \n",
      "Cluster 19: example similar usually using typically \n",
      "Cluster 20: example specific particular instance certain \n",
      "Cluster 21: example specific use using system \n",
      "Cluster 22: provide specific example system particular \n",
      "Cluster 23: focus particular well example important \n",
      "Cluster 24: specific furthermore particular certain similar \n",
      "Cluster 25: provided provide instance system use \n",
      "Cluster 26: system allows direct using directly \n",
      "Cluster 27: same example this instance similar \n",
      "Cluster 28: system using use allows uses \n",
      "Cluster 29: example well work particular focus \n",
      "Cluster 30: example particular rather this instance \n",
      "Cluster 31: cells proteins function e.g. protein \n",
      "Cluster 32: example instance using particular similar \n",
      "Cluster 33: example focus particular provide creating \n",
      "Cluster 34: system use using example same \n",
      "Cluster 35: con que y mas por \n",
      "Cluster 36: example instance specific particular system \n",
      "Cluster 37: example particular rather instance any \n",
      "Cluster 38: specific example particular basic context \n",
      "Cluster 39: instance example using same use \n",
      "Cluster 40: for also work based new \n",
      "Cluster 41: this well which example result \n",
      "Cluster 42: use using instance example specific \n",
      "Cluster 43: example particular specific rather instance \n",
      "Cluster 44: uses system using use applications \n",
      "Cluster 45: example system use provide specific \n",
      "Cluster 46: using system signal uses input \n",
      "Cluster 47: well focus its provide example \n",
      "Cluster 48: system provide use using example \n",
      "Cluster 49: method specific parameters methods processes \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(50):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------10-------------------\n",
      "Multimedia applications targeting batteryoperated wearable devices must be poweraware to exploit the capabilities of variable voltage processors. This paper presents a feedback (FB) controller for video decoding that regulates the voltage for individual frames. The decoding complexity of (parts of) individual frames is estimated using a simple frame length decoding time correlation obtained from statistics gathered on the target hardware (StrongARM processor). Experiments with a modified H.263 decoder show that the FB controller closely approaches ( 10%) the optimal case in which each frame is decoded at the minimal frequency/voltage. Furthermore, we observe that incorporating additional complexity information in the video stream will only be useful when the energy consumption of the (fixed) memory subsystem is significantly reduced.\n",
      "-------------\n",
      "A system for videoconferencing that offers, among other features, extremely low endtoend delay as well as very high scalability. The system accommodates heterogeneous receivers and networks, as well as the besteffort nature of networks such as those based on the Internet Protocol. The system relies on scalable video coding to provide a coded representation of a source video signal at multiple temporal, quality, and spatial resolutions. These resolutions are represented by distinct bitstream components that are created at each enduser encoder. System architecture and processes called SVC Thinning allow the separation of data into data used for prediction in other pictures and data not used for prediction in other pictures. SVC Thinning processes, which can be performed at video conferencing endpoints or at MCUs, can selectively remove or replace with fewer bits the data not used for prediction in other pictures from transmitted bit streams. This separation and selective removal or replacement of data for transmission allows a tradeoff between scalability support (i.e. number of decodable video resolutions), error resiliency and coding efficiency.\n",
      "-------------\n",
      "A user device having a plurality of microphones and an operating method thereof are provided. The user device includes a plurality of microphones for converting sounds into electrical signals, an analysis unit for analyzing acoustic characteristics of the electrical signals outputted from the plurality of microphones, a switch unit for electrically connecting a specific microphone and a host device according to the analysis result of the analysis unit, and the host device for executing a function using an electrical signal outputted from the specific microphone. Various other exemplary embodiments are possible.\n",
      "-------------\n",
      "A system and method for extracting acoustic features and speech activity on a device and transmitting them in a distributed voice recognition system. The distributed voice recognition system includes a local VR engine in a subscriber unit and a server VR engine on a server . The local VR engine comprises a feature extraction (FE) module that extracts features from a speech signal, and a voice activity detection module (VAD) that detects voice activity within a speech signal. The system includes filters, framing and windowing modules, power spectrum analyzers, a neural network, a nonlinear element, and other components to selectively provide an advanced front end vector including predetermined portions of the voice activity detection indication and extracted features from the subscriber unit to the server . The system also includes a module to generate additional feature vectors on the server from the received features using a feedforward multilayer perceptron (MLP) and providing the same to the speech server.\n",
      "-------------\n",
      "Systems and methods of encoding a video signal that includes a succession of images are disclosed. A system may include a plurality of independently programmable processing elements (PEs), an input interface device adapted to receive, buffer, and divide the input video signal in a manner appropriate to the plurality of PEs, and an output interface device adapted to receive encoded bitstreams generated by the plurality of PEs and provide an encoded video signal. Each PE is configurable to carry out the steps of a selected encoding algorithm and includes a digital processor and a memory in communication with the digital processor. The memories are independently accessible, and PEs communicate with each another during the encoding.\n",
      "-------------\n",
      "Standard deep neural networkbased acoustic models for automatic speech recognition (ASR) rely on handengineered input features, typically logmel filterbank magnitudes. In this paper, we describe a convolutional neural network  deep neural network (CNNDNN) acoustic model which takes raw multichannel waveforms as input, i.e. without any preceding feature extraction, and learns a similar feature representation through supervised training. By operating directly in the time domain, the network is able to take advantage of the signal's fine time structure that is discarded when computing filterbank magnitude features. This structure is especially useful when analyzing multichannel inputs, where timing differences between input channels can be used to localize a signal in space. The first convolutional layer of the proposed model naturally learns a filterbank that is selective in both frequency and direction of arrival, i.e. a bank of bandpass beamformers with an auditorylike frequency scale. When trained on data corrupted with noise coming from different spatial locations, the network learns to filter them out by steering nulls in the directions corresponding to the noise sources. Experiments on a simulated multichannel dataset show that the proposed acoustic model outperforms a DNN that uses logmel filterbank magnitude features under noisy and reverberant conditions.\n",
      "-------------\n",
      "A method for synthesizing a binaural audio signal, the method comprising: inputting a parametrically encoded audio signal comprising at least one combined signal of a plurality of audio channels and one or more corresponding sets of side information describing a multichannel sound image; and applying a predetermined set of headrelated transfer function filters to the at least one combined signal in proportion determined by the corresponding set of side information to synthesize a binaural audio signal. A corresponding parametric audio decoder, parametric audio encoder, computer program product, and apparatus for synthesizing a binaural audio signal are also described.\n",
      "-------------\n",
      "An audio signal encoding method and apparatus and an audio signal decoding method and apparatus are provided. The audio signal encoding method includes: transforming an audio signal into a signal of a frequency domain; extracting semantic information from the audio signal; variably reconfiguring one or more subbands included in the audio signal by segmenting or grouping the one or more subbands using the extracted semantic information; and generating a quantized bitstream by calculating a quantization step size and a scale factor with respect to a reconfigured subband of the one or more subbands.\n",
      "-------------\n",
      "Modern passive mobile radar systems equipped with circular arrays, require to elaborate simultaneously and in real time a certain number of passive signals. The signals are corresponding to, as an example, eight element of the circular array and ten carrier frequencies for each element in FM band. Further data are acquired if the system is intended for deploying at the same time DAB and/or DVBT signals. This sizing implies that the processing architecture has to sustain the load corresponding to, at least, more than ten adaptive cancellers (in time or space or both), pulse compressors, detection logics and bistatic and Cartesian trackers. Some of those algorithms map pretty well on GPU, thus giving a sizeable advantage in computational power with respect to standard Central Processing Unit (CPU)based devices. This paper contains a rationale for the selection of algorithms to be mapped on GPU, some rules followed for the mapping strategy and comparisons of computational time obtained with GPU and CPUbased devices operating on the same input data. A computational architecture is proposed that consists in a wise mixture of GPU and CPU devices that exploits, at their best, the available equipment.\n",
      "-------------\n",
      "An apparatus for synthesizing a rendered output signal having a first audio channel and a second audio channel includes a decorrelator stage for generating a decorrelator signal based on a downmix signal, and a combiner for performing a weighted combination of the downmix signal and a decorrelated signal based on parametric audio object information, downmix information and target rendering information. The combiner solves the problem of optimally combining matrixing with decorrelation for a high quality stereo scene reproduction of a number of individual audio objects using a multichannel downmix.\n",
      "-------------\n",
      "-------------------1-------------------\n",
      " The thousands of specialized structured file formats in use today present a substantial barrier to freely exchanging information between applications programs. In Chapter 8, we consider the problem of deducing such basic features as the whitespace characters, bracketing delimiter symbols, and selfdelimiter characters of a given file format from one or more example files. We demonstrate that for sufficiently large example files, we can typically identify the basic features of interest.\n",
      "-------------\n",
      " system should perform the translation and search in a single step without any user’s supervision.\n",
      "-------------\n",
      "The availability of large amounts of data is a fundamental prerequisite for building handwriting recognition systems. Any system needs a test set of labelled samples for measuring its performance along its development and guiding it. Moreover, there are systems that need additional samples for learning the recognition task they have to cope with later, i.e. a training set. Thus, the acquisition and distribution of standard databases has become an important issue in the handwriting recognition research community. Examples of widely used databases in the online domain are UNIPEN, IRONOFF, and Pendigits. This paper describes the current state of our own database, UJIpenchars, whose first version contains online representations of 1 364 isolated handwritten characters produced by 11 writers and is freely available at the UCI Machine Learning Repository. Moreover, we have recently concluded a second acquisition phase, totalling more than 11 000 samples from 60 writers to be made available in short as UJIpenchars2.\n",
      "-------------\n",
      "In this paper we study the problem of identifying systems that automatically inject nonpersonal messages in microblogging message streams, thus potentially biasing results of certain information extraction procedures, such as opinionmining and trend analysis. We also study several classes of features, namely features based on the time of posting, the client used to post, the presence of links, the user interaction and the writing style. This last class of features, that we introduce here for the first time, is proved to be a top performer, achieving accuracy near the 90%, on par with the best features previously used for this task.\n",
      "-------------\n",
      "Instance unification determines whether two instances in an ontology refer to the same object in the real world. More specifically, this paper addresses the instance unification problem for person names. The approach combines the use of citation information (i.e., abstract, initials, titles and coauthorship information) with web mining, in order to gather additional evidence for the instance unification algorithm. The method is evaluated on two datasets – one from the BT digital library and one used in previous work on name disambiguation. The results show that the information mined from the web contributes substantially towards the successful handling of highly ambiguous cases which lowered the performance of previous methods.\n",
      "-------------\n",
      "This paper describes a novel statistical namedentity (i.e. \"proper name\") recognition system built around a maximum entity framework. By working v,ithin the framework of maximum entropy theory and utilizing a flexible objectbased architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multiword terms. The purely statistical system contains no handgenerated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thusfar published. 1 I N T R O D U C T I O N Named entity recognition is one of the simplest of the common message understanding tasks. The objective is to identify and categorize all members of certain categories of \"proper names\" from a given corpus. The specific test bed which will be the subject of this paper is that of the Seventh Message Understanding Conference (MUC7), in which the task was to identify \"names\" falling into one of seven categories: person, organization, location, date, time, percentage, and monetary amount. This paper describes a new system called \"Maximum Entropy Named Entity\" or \"MENE\" (pronounced \"meanie\"). By working within the framework of maximum entropy theory and utilizing a flexible objectbased architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decision. These knowledge sources include capitalization features, lexical features, and features indicating the current section of text. It makes use of a broad array of dictionaries of useful single or multiword terms such as first names, company names, and corporate suffixes, and automatically handles cases where words are in more than one dictionary. Our dictio152 naries required no manual editing and were either downloaded from the web or were simply \"obvious\" lists entered by hand. This system, built from offtheshelf knowledge sources, contained no handgenerated pat terns and achieved a result which is comparable with that of the best statistical systems. Further experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thusfar reported by any system on a MUC evaluation. Given appropriate training data, we believe that this system is highly portable to other domains and languages and have already achieved good results on uppercase English. We also feel that there are plenty of avenues to explore in enhancing the system's performance on Englishlanguage newspaper\n",
      "-------------\n",
      "We describe the use of a recommender system to enable continuous knowledge acquisition and individualized tutoring of application software across an organization. Installing such systems will result in the capture of evolving expertise and in organizationwide learning (OWL). We present the results of a yearlong naturalistic inquiry into application’s usage patterns, based on logging users’ actions. We analyze the data to develop user models, individualized expert models, confidence intervals, and instructional indicators. We show how this information could be used to tutor users. Introduction Recommender Systems typically help people select products, services, and information. A novel application of recommender systems is to help individuals select ’what to learn next’ by recommending knowledge that their peers have found useful. For example, people typically utilize only a small portion of a software application’s functionality (one study shows users applying less than 10% of Microsoft Word’s commands). A recommender system can unobtrusively note which portions of an application’s functionality that the members of an organization find useful, group the organization’s members into sets of similar users, or peers (based on similar demographic factors such as job title, or similarities in command usage patterns), and produce recommendations for learning that are specific to the individual in the context of his/her organization, peers, and current activities. This paper reports research on a recommender system (Resnick & Varian, 1997) intended to promote gradual but perpetual performance improvement in the use of application software. We present our rationale, an analysis of a year’s collected data, and a vision of how users might learn from the system. We have worked with one commercial application, and believe our approach is generally applicable. The research explores the potential of a new sort of user modeling based on summaries of logged user data. This method of user modeling enables the observation of a large number of users over a long period of time, enables concurrent development of student models and individualized expert models, and applies recommender system techniques to onthejob instruction. Earlier work is reported in Linton (1990), and Linton (1996). Kay and Thomas (1995), Thomas (1996) report on related work with a text editor in an academic environment. A recommender system to enhance the organizationwide learning of application software is a means of promoting organizational learning (Senge, 1990). By pooling and sharing expertise, recommender systems augment and assist the natural social process of people learning from each other. This approach is quite distinct from systems, such as Microsoft’s Office Assistant, which recommend new commands based on their logical equivalence to the lessefficient way a user may be performing a task. The system presented here will (1) capture evolving expertise from community of practice (Lave & Wenger 1991), (2) support lessskilled members of the community in acquiring expertise, and (3) serve as an organizational memory for the expertise it captures. In many workplaces ... mastery is in short supply and what is required is a kind of collaborative bootstrapping of expertise. (Eales & Welch, 1995, p. 100) The main goal of the approach taken in this work is to continuously improve the performance of application users by providing individualized modeling and coaching based on the automated comparison of user models to expert models. The system described here would be applicable in any situation where a number of application users perform similar tasks on networked computers 65 From: AAAI Technical Report WS9808. Compilation copyright © 1998, AAAI (www.aaai.org). All rights reserved. In the remainder of this section we describe the logging process and make some initial remarks about modeling and coaching software users. We then present an analysis of the data we have logged and our process of creating individual models of expertise. In the final section we describe further work and close with a summary. Each time a user issues a Word command such as Cut or Paste, the command is written to the log, together with a time stamp, and then executed. The logger, called OWL for OrganizationWide Learning, comes up when the user opens Word; it creates a separate log for each file the user edits, and when the user quits Word, it sends the logs to a server where they are periodically loaded into a database for analysis. A toolbar button labeled ’OWL is ON’ (or OFF) informs users of OWL’s tate and gives them control. Individual models of expertise We have selected the Edit commands for further analysis. A similar analysis could be performed for each type of command. The first of the three tables in Figure 1 presents data on the Edit commands for each of our 16 users. In the table, each column contains data for one user and each row contains data for one command (Edit commands that were not used have been omitted). A cell then, contains the count of the number of times the individual has used the command. The columns have been sorted so that the person using the most commands is on the left and the person using the fewest is on the right. Similarly, the rows have been sorted so that the most frequently used command is in the top row and the least frequently used command is in the bottom row. Consequently the cells with the largest values are in the upper left corner and those with the smallest values are in the lower right comer. The table has been shaded to make the contours of the numbers visible: the largest numbers have the darkest shading and the smallest numbers have no shading, each shade indicates an order of magnitude. Inspection of the first table reveals that users tend to acquire the Edit commands in a specific sequence, i.e., those that know fewer commands know a subset of the commands used by their moreknowledgeable peers. If instead, users acquired commands in an idiosyncratic order, the data would not sort as it does. And if they acquired commands in a manner that strongly reflected their job tasks or their writing tasks, there would be subgroups of users who shared common commands. Also, the moreknowledgeable users do not replace commands learned early on with more powerful commands, but instead keep adding new commands to their repertoire. Finally, the sequence of command acquisition corresponds to the commands’ frequency of use. While this last point is not necessarily a surprise, neither is it a given. There are some peaks and valleys in the data as sorted, and a fairly rough edge where commands transition from being used rarely to being used not at all. These peaks, valleys, and rough edges may represent periods of repetitive tasks or lack of data, respectively, or they may represent overdependence on some command that has a more powerful substitute or ignorance of a command or of a task (a sequence of commands) that uses the command. In other words, some of the peaks, valleys, and rough edges may represent opportunities to learn more effective use of the software. In the second table in Figure 1 the data have been smoothed. The observed value in each cell has been replaced by an expected value, the most likely value for the cell, using a method taken from statistics, based on the row, column and grand totals for the table (Howell, 1982). In the case of software use, the row effect is the overall relative utility of the command (for all users) and the column effect is the usage of related commands by the individual user. The expected value is the usage the command would have if the individual used it in a manner consistent with his/her usage of related commands and consistent with his/her peers’ usage of the command. These expected values are a new kind of expert model, one that is unique to each individual and each moment in time; the expected value in each cell reflects the individual’s use of related commands, and one’s peers’ use of the same command. The reason for differences between observed and expected values, between one’s actual and expert model, might have several explanations such as the individual’s tasks, preferences, experiences, or hardware, but we are most interested when the difference indicates the lack of knowledge or skill.\n",
      "-------------\n",
      "Nowadays with the proliferation of smartphones and tablets on the market, almost everyone has access to mobile devices that offer better processing capabilities and access to new information and services. The Web is undoubtedly the best tool for sharing content, especially through social networks. One of the most useful information that can be extracted is the geographical one. Current navigation systems lack in several ways to satisfy the need to process and reason upon such volumes of data, namely, to accurately provide information about urban traffic in realtime and the possibility to personalize the information used by such systems. This paper describes an approach to integrate and fuse tweet messages from traffic agencies in UK, with the objective of detecting the geographical focus of traffic events. Tweet messages are considered in this work given its uniqueness, the real time nature of tweets which may be used to quickly detect a traffic event and its simplicity; it only cost 140 characters to generate a message (called “tweet”) for any user. The approach presented here is composed by several steps: tweet classification, event type classification, name entity recognition, geolocation and event tracking. Finally, we do an experimental study on a real dataset composed by traffic related tweet messages to access the accuracy of proposed approach. We present some inaccuracies ranging from lack of geographical information, imprecise and ambiguous toponyms, overlaps and repetitions as well as visualization to our data set in UK. We finally give an outlook into potential corrections. The work presented here is still part of ongoing work. Results achieved so far do not address the final conclusions but form the basis for the formalization of a domain knowledge along with the services.\n",
      "-------------\n",
      "The world wide web is growing continuously and rapidly; it is quickly facilitating the migration of tasks of the daily life into webbased. This trend shows time will come when everyone is forced to use the web for daily activities. Naive users arc the major concern of such a shift; so, it is necessary to have the web ready to serve them. We argue that this requires well optimized websites for users to quickly locate the information they arc looking for. This, on the other hand, becomes more and more important due to the widespread reliance on the many services available on the Internet nowadays. It is true that search engines can facilitate the task of finding the information one is looking for. However, search engines will never replace but do complement the optimization of a website’s internal structure based on previously recorded user behavior. In this chapter, wc will present a novel approach for identifying problematic structures in websites. This method consists of two phases. The first phase compares user behavior, derived via web log mining techniques, to a combined analysis of the website’s link structure obtained by applying three methods leading to more robust framework and hence strong and consistent outcome: (1) constructing and analyzing a social network of the pages constituting the website by considering both the structure and the usage information; (2) applying the Weighted PageRank algorithm; and (3) applying the Hypertext Induced Topic Selection (HITS) method. In the second phase, we use the term frequencyinverse document frequency (TFIDF) measure to investigate further the correlation between the page that contains the link and the linked to pages in order to further support the findings of the first phase of our approach. We will then show how to use these intermediate results in order to point out problematic website structures to the website owner.\n",
      "-------------\n",
      "This study aims at developing a news surveillance system able to address multilingual web corpora. As an example of a domain where multilingual capacity is crucial, we focus on Epidemic Surveillance. This task necessitates worldwide coverage of news in order to detect new events as quickly as possible, anywhere, whatever the language it is first reported in. In this study, textgenre is used rather than sentence analysis. The newsgenre properties allow us to assess the thematic relevance of news, filtered with the help of a specialised lexicon that is automatically collected on Wikipedia. Afterwards, a more detailed analysis of text specific properties is applied to relevant documents to better characterize the epidemic event (i.e., which disease spreads where?). Results from 400 documents in each language demonstrate the interest of this multilingual approach with light resources. DAnIEL achieves an F 1measure score around 85%. Two issues are addressed: the first is morphology rich languages, e.g. Greek, Polish and Russian as compared to English. The second is event location detection as related to disease detection. This system provides a reliable alternative to the generic IE architecture that is constrained by the lack of numerous components in many languages.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for test_cluster in [10,1]:\n",
    "    print(f\"-------------------{test_cluster}-------------------\")\n",
    "    most_representative_docs = np.argsort(\n",
    "        np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    "    )\n",
    "    for d in most_representative_docs[:10]:\n",
    "        print(df[\"text\"][d])\n",
    "        print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41603511,  0.02993672,  0.08477567, ...,  0.42157972,\n",
       "         0.05389575,  0.08879889],\n",
       "       [ 0.2547176 ,  0.12962069,  0.05133368, ...,  0.41265002,\n",
       "         0.03695938,  0.02680716],\n",
       "       [ 0.21775689,  0.58536683, -0.57444827, ..., -0.38275401,\n",
       "         0.88557918,  1.01202863],\n",
       "       ...,\n",
       "       [ 0.32258523,  0.02653912,  0.13975534, ...,  0.44815915,\n",
       "         0.07009276,  0.15324835],\n",
       "       [ 0.39768368, -0.00493577,  0.35454605, ...,  0.29803471,\n",
       "         0.02942713,  0.12025491],\n",
       "       [ 0.38876871, -0.02372196,  0.03457274, ...,  0.50404936,\n",
       "         0.14686806,  0.10216383]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Clusters.npy\",clustering.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clustering.predict(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame()\n",
    "df_clusters[\"id\"] = data[:,0]\n",
    "df_clusters[\"prediction\"] = predictions\n",
    "pd.DataFrame(df_clusters).to_csv(\"Clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c77158d38858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a27dfe8365d0>\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(file_str)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnew_file_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_file_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a27dfe8365d0>\u001b[0m in \u001b[0;36mget_id\u001b[0;34m(file_str)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnew_file_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from json import JSONDecodeError\n",
    "for text in texts:\n",
    "    try:\n",
    "        j = get_json(text)\n",
    "    except JSONDecodeError:\n",
    "        print(text)\n",
    "        break\n",
    "    if j['IndexLength']==0:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeb980a7680441eac42ec2d85ab9999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=624181.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files_str = [get_descritpion(file_str) for file_str in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=files_str,size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'min_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0fc73ed17709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'min_count'"
     ]
    }
   ],
   "source": [
    "model.train(sentences=files_str, total_examples=len(files_str), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |  \n",
      " |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |  \n",
      " |  Once you're finished training a model (=no more updates, only querying)\n",
      " |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      " |  \n",
      " |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |  \n",
      " |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |  \n",
      " |  Some important attributes are the following:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      " |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      " |  \n",
      " |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      " |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      " |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      " |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      " |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use `self.wv.__contains__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |      Deprecated. Use `self.wv.accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      " |      \n",
      " |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      " |      Discard parameters that are used in training and scoring, to save memory.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Use only if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace_word_vectors_with_normalized : bool, optional\n",
      " |          If True, forget the original (not normalized) word vectors and only keep\n",
      " |          the L2-normalized word vectors, to save even more memory.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Deprecated. Use `self.wv.init_sims` instead.\n",
      " |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of str\n",
      " |          List of context words.\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      " |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      " |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated, use self.wv.doesnt_match() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated, use self.wv.most_similar() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated, use self.wv.n_similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_word() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated, use self.wv.similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated, use self.wv.wmdistance() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vecPaperDescription.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maximebonnin/Notebooks/3A Notebook/INF554/INF554_Kaggle_Project/Exploration'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/maximebonnin/Notebooks/3A Notebook/INF554/INF554_Kaggle_Project/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 217801\n",
      "Number of edges: 1718164\n"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]\n",
    "\n",
    "# load the graph    \n",
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges() \n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)\n",
    "\n",
    "\n",
    "# computes structural features for each node\n",
    "core_number = nx.core_number(G)\n",
    "\n",
    "# create the training matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number \n",
    "X_train = np.zeros((n_train, 2))\n",
    "y_train = np.zeros(n_train)\n",
    "for i,row in df_train.iterrows():\n",
    "    node = row['author']\n",
    "    X_train[i,0] = G.degree(node)\n",
    "    X_train[i,1] = core_number[node]\n",
    "    y_train[i] = row['hindex']\n",
    "\n",
    "# create the test matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number\n",
    "X_test = np.zeros((n_test, 2))\n",
    "for i,row in df_test.iterrows():\n",
    "    node = row['author']\n",
    "    X_test[i,0] = G.degree(node)\n",
    "    X_test[i,1] = core_number[node]\n",
    "    \n",
    "# train a regression model and make predictions\n",
    "reg = Lasso(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# write the predictions to file\n",
    "df_test['hindex'] = pd.Series(np.round_(y_pred, decimals=3))\n",
    "\n",
    "\n",
    "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(G, node_size=10)\n",
    "plt.title(\"Raw graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"data/abstracts.txt\") as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8289804\n",
      "{'IndexLength': 82, 'InvertedIndex': {'For': [0], 'the': [1, 48, 51, 73], 'feature': [2], 'analysis': [3], 'of': [4, 47, 50, 65, 71], 'vector': [5, 11, 23, 30, 59], 'fields': [6, 60], 'we': [7], 'decompose': [8], 'a': [9, 16, 18, 21, 29, 33, 79], 'given': [10], 'field': [12, 31], 'into': [13], 'three': [14], 'components:': [15], 'divergence-free,': [17], 'rotation-free,': [19], 'and': [20, 36, 42, 63, 78], 'harmonic': [22], 'field.': [24], 'This': [25], 'Hodge-type': [26], 'decomposition': [27], 'splits': [28], 'using': [32], 'variational': [34], 'approach,': [35], 'allows': [37], 'to': [38, 56, 75], 'locate': [39], 'sources,': [40], 'sinks,': [41], 'vortices': [43], 'as': [44], 'extremal': [45], 'points': [46], 'potentials': [49], 'components.': [52], 'Our': [53], 'method': [54, 74], 'applies': [55], 'discrete': [57], 'tangential': [58], 'on': [61], 'surfaces,': [62], 'is': [64], 'global': [66], 'nature.': [67], 'Results': [68], 'are': [69], 'presented': [70], 'applying': [72], 'test': [76], 'cases': [77], 'CFD': [80], 'flow.': [81]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'paper,',\n",
       " 'we',\n",
       " 'describe',\n",
       " 'a',\n",
       " 'new',\n",
       " 'bitmap',\n",
       " 'indexing',\n",
       " 'technique',\n",
       " 'to',\n",
       " 'cluster',\n",
       " 'XML',\n",
       " 'documents.',\n",
       " 'XML',\n",
       " 'is',\n",
       " 'a',\n",
       " 'new',\n",
       " 'standard',\n",
       " 'for',\n",
       " 'exchanging',\n",
       " 'and',\n",
       " 'representing',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Internet.',\n",
       " 'Documents',\n",
       " 'can',\n",
       " 'be',\n",
       " 'hierarchically',\n",
       " 'represented',\n",
       " 'by',\n",
       " 'XML-elements.',\n",
       " 'XML',\n",
       " 'documents',\n",
       " 'are',\n",
       " 'represented',\n",
       " 'and',\n",
       " 'indexed',\n",
       " 'using',\n",
       " 'a',\n",
       " 'bitmap',\n",
       " 'indexing',\n",
       " 'technique.',\n",
       " 'We',\n",
       " 'define',\n",
       " 'the',\n",
       " 'similarity',\n",
       " 'and',\n",
       " 'popularity',\n",
       " 'operations',\n",
       " 'available',\n",
       " 'in',\n",
       " 'bitmap',\n",
       " 'indexes',\n",
       " 'and',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'method',\n",
       " 'for',\n",
       " 'partitioning',\n",
       " 'a',\n",
       " 'XML',\n",
       " 'document',\n",
       " 'set.',\n",
       " 'Furthermore,',\n",
       " 'a',\n",
       " '2-dimensional',\n",
       " 'bitmap',\n",
       " 'index',\n",
       " 'is',\n",
       " 'extended',\n",
       " 'to',\n",
       " 'a',\n",
       " '3dimensional',\n",
       " 'bitmap',\n",
       " 'index,',\n",
       " 'called',\n",
       " 'BitCube.',\n",
       " 'We',\n",
       " 'define',\n",
       " 'statistical',\n",
       " 'measurements',\n",
       " 'in',\n",
       " 'the',\n",
       " 'BitCube:',\n",
       " 'mean,',\n",
       " 'mode,',\n",
       " 'standard',\n",
       " 'derivation,',\n",
       " 'and',\n",
       " 'correlation',\n",
       " 'coefficient.',\n",
       " 'Based',\n",
       " 'on',\n",
       " 'these',\n",
       " 'measurements,',\n",
       " 'we',\n",
       " 'also',\n",
       " 'define',\n",
       " 'the',\n",
       " 'slice,',\n",
       " 'project,',\n",
       " 'and',\n",
       " 'dice',\n",
       " 'operations',\n",
       " 'on',\n",
       " 'a',\n",
       " 'BitCube.',\n",
       " 'BitCube',\n",
       " 'can',\n",
       " 'be',\n",
       " 'manipulated',\n",
       " 'efficiently',\n",
       " 'and',\n",
       " 'improves',\n",
       " 'the',\n",
       " 'performance',\n",
       " 'of',\n",
       " 'document',\n",
       " 'retrieval.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(get_id(texts[1000]))\n",
    "print(get_json(texts[1000]))\n",
    "get_descritpion(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-15d36fd2391f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_descritpion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-15d36fd2391f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_descritpion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-e00f98af3848>\u001b[0m in \u001b[0;36mget_descritpion\u001b[0;34m(file_str)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_descritpion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IndexLength\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-e00f98af3848>\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(file_str)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_descritpion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "files_str = [get_descritpion(file_str) for file_str in texts]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "287892eb55904b50337f16055d7cf7eaa1f5a3cda4903bb2a548d1e286ec920a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
