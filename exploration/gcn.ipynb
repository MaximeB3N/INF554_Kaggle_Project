{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98f18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from random import randint\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00177d8",
   "metadata": {},
   "source": [
    "# INF 554 Lab 9: Deep Learning for Graph Data\n",
    "\n",
    "\n",
    "We want to acknowledge _Dr. Giannis Nikolentzos_ for his large contributions to the content of this lab.\n",
    "\n",
    "In this lab we will be working with the DeepWalk algorithm, an unsupervised method for the embedding of nodes in a  graph, and Graph Neural Networks, which are semi-supervised methods used for node classification and several other tasks on attributed graphs. Semi-supervised learning methods are trained on datasets, which include both labeled and unlabeled data points, and therefore are often said to fall inbetween supervised and unsupervised learning methods. \n",
    "\n",
    "\n",
    "As in the assessment we will be working with the Cora dataset, which we load below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b4a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174241, 2)\n",
      "(100000,) (10000,)\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18650612  0.26633653  0.20926507 ...  0.11414053 -0.02610091\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "[105831  83363  34553 ... 107648  83100  43012]\n",
      "[25671 46599  5386 ... 91507  7006 39079]\n",
      "100000 10000\n",
      "0 15\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "  11.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   2.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   2.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   9.        ]\n",
      " [ 0.18650612  0.26633653  0.20926507 ...  0.11414053 -0.02610091\n",
      "  14.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "  11.        ]]\n",
      "110000 429199\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#cora_dataset = cora_dataset[0].to(device)\n",
    "    \n",
    "# load the graph\n",
    "G = nx.read_edgelist('../data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "nodes = np.array(list(G.nodes))\n",
    "#G = G.subgraph(nodes[:100])\n",
    "#nodes = np.array(list(G.nodes))\n",
    "adjacency = nx.linalg.graphmatrix.adjacency_matrix(G)\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "\n",
    "# Read h indexes\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "print(df_train.shape)\n",
    "\n",
    "indices = np.arange(110000)\n",
    "indices = np.random.permutation(indices)\n",
    "y_train = df_train['hindex'][indices[:100000]]\n",
    "authors_train = df_train['author'][indices[:100000]]\n",
    "y_test = df_train['hindex'][indices[-10000:]].to_numpy()\n",
    "authors_test = df_train['author'][indices[-10000:]]\n",
    "print(authors_train.shape, authors_test.shape)\n",
    "\n",
    "G = G.subgraph(np.array([nodes[nodes==auth] for auth in authors_train] + [nodes[nodes==auth] for auth in authors_test]).reshape(len(y_train)+len(y_test)))\n",
    "nodes = np.array(list(G.nodes))\n",
    "adjacency = nx.convert_matrix.to_scipy_sparse_matrix(G)\n",
    "N = np.load('../data/N.npy')\n",
    "print(N[:10])\n",
    "#features = np.expand_dims(np.copy(nodes), axis=1)\n",
    "idx_train = np.array([np.argwhere(nodes==auth) for auth in authors_train])\n",
    "idx_test = np.array([np.argwhere(nodes==auth) for auth in authors_test])\n",
    "idx_train = idx_train.reshape(len(authors_train))\n",
    "idx_test = idx_test.reshape(len(authors_test))\n",
    "print(idx_train)\n",
    "print(idx_test)\n",
    "print(len(authors_train), len(authors_test))\n",
    "print(np.min(idx_train), np.min(idx_test))\n",
    "for i in idx_train:\n",
    "    N[i,-1] = df_train[df_train['author'] == nodes[i]]['hindex']\n",
    "for i in idx_test:\n",
    "    N[i,-1] = -1.0\n",
    "print(N[:10])\n",
    "idx = np.concatenate((idx_train, idx_test))\n",
    "features = N[idx]\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d3aac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26423      3.0\n",
      "81322     17.0\n",
      "89589     78.0\n",
      "13372     15.0\n",
      "48617      5.0\n",
      "          ... \n",
      "29503      3.0\n",
      "108588    16.0\n",
      "16659      5.0\n",
      "106192     4.0\n",
      "12255      7.0\n",
      "Name: hindex, Length: 100000, dtype: float64\n",
      "[16.  8. 16. ...  7.  7.  3.]\n"
     ]
    }
   ],
   "source": [
    "#y_train = y_train.to_numpy()\n",
    "#y_test = y_test.to_numpy()\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58f95bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515524\n"
     ]
    }
   ],
   "source": [
    "print(np.min(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372db6a",
   "metadata": {},
   "source": [
    "## 2) Graph Convolutional Networks\n",
    "\n",
    "In the second part of the lab, we will focus on the problem of semi-supervised node classification using Graph Neural Networks (GNNs). Generally, speaking GNNs are neural networks that process graph data, in contrast to MLPs processing vector data, CNNs processing structured data such as images and RNNs processing sequences.\n",
    "\n",
    "GNNs follow a recursive neighborhood aggregation (or message passing) scheme, where each node aggregates feature vectors of its neighbors to compute its new feature vector.\n",
    "After $k$ iterations of aggregation, a node is represented by its transformed feature vector, which captures the structural information within the node's $k$-hop neighborhood.\n",
    "\n",
    "\n",
    "In node classification, we are given the class labels of some nodes, and the goal is to predict the class labels of the nodes of the test set using information from both the graph structure and the attributes of the nodes.\n",
    "\n",
    "Given the adjacency matrix $\\mathbf{A}$ of a graph, we will first normalise it as follows:\n",
    "\\begin{equation*}\n",
    "   \\qquad\\qquad\\qquad\\qquad \\hat{\\mathbf{A}} = (\\mathbf{D}+\\mathbf{I})^{-\\frac{1}{2}} \\; (\\mathbf{A} +\\mathbf{I}) \\; (\\mathbf{D}+\\mathbf{I})^{-\\frac{1}{2}}, \\qquad\\qquad\\qquad\\qquad(1)\n",
    "\\end{equation*}\n",
    "where $\\mathbf{D}$ is a diagonal matrix such that $\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}$.\n",
    "The above formula adds self-loops to the graph, and produces a symmetric normalised matrix.\n",
    "This normalization trick, among other things, addresses numerical instabilities, which may lead to exploding/vanishing gradients when used in a deep neural network model.\n",
    "\n",
    "\n",
    ">**Task 4:** Fill in the body of the ``normalise_adjacency()`` function that applies the normalisation trick in Equation (1). Note that the adjacency matrix is stored as a sparse matrix. Use operations of the NumPy and SciPy libraries (e.g., ``identity()`` function of SciPy) to also produce a sparse normalised matrix.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85ec5e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "def normalise_adjacency(A):\n",
    "    #Please insert your code for Task 4 here\n",
    "    diag = np.asarray(sp.csr_matrix.sum(A, axis=1)).squeeze()\n",
    "    D = sp.diags(diag, format='csr')\n",
    "    I = sp.identity(D.shape[0], format='csr')\n",
    "    fac = sp.linalg.inv(sp.csr_matrix.sqrt(D + I))\n",
    "    A_normalised = fac @ (A + I) @ fac\n",
    "    return A_normalised\n",
    "\n",
    "\n",
    "def sparse_to_torch_sparse(M):\n",
    "    \"\"\"Converts a sparse SciPy matrix to a sparse PyTorch tensor\"\"\"\n",
    "    M = M.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((M.row, M.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(M.data)\n",
    "    shape = torch.Size(M.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "print(type(adjacency))\n",
    "#type(normalise_adjacency(sp.csr_matrix(adjacency)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689e581",
   "metadata": {},
   "source": [
    "You will next implement a [Graph Convolutional Network (GCN)](https://arxiv.org/pdf/1609.02907.pdf) model that consists of three layers.\n",
    "Let $\\hat{\\mathbf{A}}$ be the normalised adjacency matrix of the graph, and $\\mathbf{X}$ a matrix whose $i^{th}$ row contains the feature vector of node $i$.\n",
    "The first layer of the model is a message passing layer, and is defined as follows:\n",
    "\\begin{equation*}\n",
    "    \\mathbf{Z}^0 = f(\\hat{\\mathbf{A}} \\; \\mathbf{X} \\; \\mathbf{W}^0 ),\n",
    "\\end{equation*}\n",
    "where $\\mathbf{W}^0$ is a matrix of trainable weights and $f$ is an activation function (e.g., ReLU, sigmoid, tanh).\n",
    "Clearly, the new feature vector of each node is the sum of the feature vectors of its neighbors.\n",
    "The second layer of the model is again a message passing layer:\n",
    "\\begin{equation*}\n",
    "    \\mathbf{Z}^1 = f(\\hat{\\mathbf{A}} \\; \\mathbf{Z}^0 \\; \\mathbf{W}^1),\n",
    "\\end{equation*}\n",
    "where $\\mathbf{W}^1$ is a second matrix of trainable weights and $f$ is an activation function.\n",
    "The two message passing layers are followed by a fully-connected layer which makes use of the softmax function to produce a probability distribution over the class labels:\n",
    "\\begin{equation*}\n",
    "    \\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{Z}^1 \\; \\mathbf{W}^2),\n",
    "\\end{equation*}\n",
    "where $\\mathbf{W}^2$ is a third matrix of trainable weights.\n",
    "Note that for clarity of presentation we have omitted biases.\n",
    "\n",
    "We next discuss some practical implementation details.\n",
    "Let $\\mathbf{H} = f(\\hat{\\mathbf{A}} \\; \\mathbf{Z} \\; \\mathbf{W}^0 )$ be a message passing layer.\n",
    "Clearly, to compute $\\mathbf{H}$, we need to perform two matrix-matrix multiplications.\n",
    "Since $\\mathbf{W}$ corresponds to a matrix of trainable parameters, the first matrix-matrix multiplication (between $\\mathbf{Z}$ and $\\mathbf{W}$) can be defined as a fully-connected layer of the network.\n",
    "Then, we can multiply $\\hat{\\mathbf{A}}$ with the output of the above operation and apply the nonlinear function to compute $\\mathbf{H}$.\n",
    "\n",
    ">**Task 5:** Implement the architecture presented above in the ``GCN()`` class.\n",
    "More specifically, add the following layers:\n",
    ">* a message passing layer with $h_1$ hidden units (i.e., $\\mathbf{W}^0 \\in \\mathbb{R}^{d \\times h_1}$) followed by a ReLU activation function\n",
    ">* a dropout layer with with $p_d$ ratio of dropped outputs\n",
    ">* a message passing layer with $h_2$ hidden units (i.e., $\\mathbf{W}^1 \\in \\mathbb{R}^{h_1 \\times h_2}$) followed by a ReLU activation function\n",
    ">* a fully-connected layer with $n_{class}$ units (i.e., $\\mathbf{W}^2 \\in \\mathbb{R}^{h_2 \\times n_{class}}$) followed by the log_softmax activation function\n",
    "\n",
    ">Please return both the log_softmaxed output and the hidden states after the second layer, so that they can be visualised later on. Then, make use of our provided code to train this model on the cora dataset and visualise its hidden states.\n",
    "\n",
    ">(Hint: You can perform a matrix-matrix multiplication using the ``torch.mm()`` function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b18d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"Simple GCN model\"\"\"\n",
    "    def __init__(self, n_feat, n_hidden_1, n_hidden_2, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj):\n",
    "        \n",
    "        #Please insert your code for Task 5 here\n",
    "        z0 = self.relu(self.fc1(adj.matmul(torch.tensor(x_in, dtype=torch.float32))))\n",
    "        z0 = self.dropout(z0)\n",
    "        z1 = self.relu(self.fc2(adj @ z0))\n",
    "        x = self.fc3(z1)\n",
    "\n",
    "        return x, z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b20187bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erwan/.local/lib/python3.8/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:318: SparseEfficiencyWarning: splu requires CSC matrix format\n",
      "  warn('splu requires CSC matrix format', SparseEfficiencyWarning)\n",
      "/home/erwan/.local/lib/python3.8/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:215: SparseEfficiencyWarning: spsolve is more efficient when sparse b is in the CSC matrix format\n",
      "  warn('spsolve is more efficient when sparse b '\n"
     ]
    }
   ],
   "source": [
    "adj = normalise_adjacency(sp.csr_matrix(adjacency)) \n",
    "adj = sparse_to_torch_sparse(sp.csr_matrix(adjacency)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da375ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "221fe69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n",
      "Epoch: 001 loss_train: 224.9492 acc_train: 0.0000 loss_val: 224.3655 acc_val: 0.0000 time: 1.9505s\n",
      "Epoch: 002 loss_train: 223.6556 acc_train: 0.0000 loss_val: 228.4854 acc_val: 0.0000 time: 1.7091s\n",
      "Epoch: 003 loss_train: 223.8230 acc_train: 0.0000 loss_val: 230.6032 acc_val: 0.0000 time: 1.7410s\n",
      "Epoch: 004 loss_train: 224.7843 acc_train: 0.0000 loss_val: 230.2570 acc_val: 0.0000 time: 1.9321s\n",
      "Epoch: 005 loss_train: 225.6424 acc_train: 0.0000 loss_val: 228.2746 acc_val: 0.0000 time: 1.9787s\n",
      "Epoch: 006 loss_train: 223.6491 acc_train: 0.0000 loss_val: 224.5224 acc_val: 0.0000 time: 1.8510s\n",
      "Epoch: 007 loss_train: 222.6242 acc_train: 0.0000 loss_val: 219.6008 acc_val: 0.0000 time: 1.8602s\n",
      "Epoch: 008 loss_train: 229.8526 acc_train: 0.0000 loss_val: 232.2337 acc_val: 0.0000 time: 1.7494s\n",
      "Epoch: 009 loss_train: 227.3819 acc_train: 0.0000 loss_val: 240.9110 acc_val: 0.0000 time: 1.7210s\n",
      "Epoch: 010 loss_train: 230.9195 acc_train: 0.0000 loss_val: 244.9938 acc_val: 0.0000 time: 1.7409s\n",
      "Epoch: 011 loss_train: 234.5974 acc_train: 0.0000 loss_val: 245.5806 acc_val: 0.0000 time: 1.7934s\n",
      "Epoch: 012 loss_train: 236.1839 acc_train: 0.0000 loss_val: 243.1124 acc_val: 0.0000 time: 1.7525s\n",
      "Epoch: 013 loss_train: 233.0524 acc_train: 0.0000 loss_val: 238.7015 acc_val: 0.0000 time: 1.8673s\n",
      "Epoch: 014 loss_train: 230.6612 acc_train: 0.0000 loss_val: 233.4021 acc_val: 0.0000 time: 1.7558s\n",
      "Epoch: 015 loss_train: 232.7969 acc_train: 0.0000 loss_val: 233.0902 acc_val: 0.0000 time: 1.6009s\n",
      "Epoch: 016 loss_train: 227.4348 acc_train: 0.0000 loss_val: 230.6922 acc_val: 0.0000 time: 1.5943s\n",
      "Epoch: 017 loss_train: 227.7958 acc_train: 0.0000 loss_val: 227.4048 acc_val: 0.0000 time: 1.5878s\n",
      "Epoch: 018 loss_train: 226.5877 acc_train: 0.0000 loss_val: 224.4074 acc_val: 0.0000 time: 1.6449s\n",
      "Epoch: 019 loss_train: 228.7052 acc_train: 0.0000 loss_val: 223.8775 acc_val: 0.0000 time: 1.6057s\n",
      "Epoch: 020 loss_train: 227.6395 acc_train: 0.0000 loss_val: 224.6145 acc_val: 0.0000 time: 1.5841s\n",
      "Epoch: 021 loss_train: 226.0550 acc_train: 0.0000 loss_val: 226.9731 acc_val: 0.0000 time: 1.5848s\n",
      "Epoch: 022 loss_train: 226.4596 acc_train: 0.0000 loss_val: 228.5064 acc_val: 0.0000 time: 1.6345s\n",
      "Epoch: 023 loss_train: 226.9974 acc_train: 0.0000 loss_val: 229.1551 acc_val: 0.0000 time: 1.6225s\n",
      "Epoch: 024 loss_train: 227.6411 acc_train: 0.0000 loss_val: 228.7805 acc_val: 0.0000 time: 1.6188s\n",
      "Epoch: 025 loss_train: 227.0496 acc_train: 0.0000 loss_val: 227.4277 acc_val: 0.0000 time: 1.7461s\n",
      "Epoch: 026 loss_train: 226.5505 acc_train: 0.0000 loss_val: 224.9456 acc_val: 0.0000 time: 1.6333s\n",
      "Epoch: 027 loss_train: 225.4266 acc_train: 0.0000 loss_val: 222.7507 acc_val: 0.0000 time: 1.5956s\n",
      "Epoch: 028 loss_train: 227.1185 acc_train: 0.0000 loss_val: 222.4016 acc_val: 0.0000 time: 1.6136s\n",
      "Epoch: 029 loss_train: 227.9541 acc_train: 0.0000 loss_val: 225.0990 acc_val: 0.0000 time: 1.7195s\n",
      "Epoch: 030 loss_train: 225.5662 acc_train: 0.0000 loss_val: 227.4912 acc_val: 0.0000 time: 1.9729s\n",
      "Epoch: 031 loss_train: 226.7116 acc_train: 0.0000 loss_val: 228.6920 acc_val: 0.0000 time: 1.9414s\n",
      "Epoch: 032 loss_train: 227.5673 acc_train: 0.0000 loss_val: 227.4112 acc_val: 0.0000 time: 1.9806s\n",
      "Epoch: 033 loss_train: 226.4639 acc_train: 0.0000 loss_val: 224.6678 acc_val: 0.0000 time: 1.9645s\n",
      "Epoch: 034 loss_train: 226.3699 acc_train: 0.0000 loss_val: 221.5565 acc_val: 0.0000 time: 1.9808s\n",
      "Epoch: 035 loss_train: 224.8383 acc_train: 0.0000 loss_val: 218.6742 acc_val: 0.0000 time: 1.6501s\n",
      "Epoch: 036 loss_train: 227.6448 acc_train: 0.0000 loss_val: 221.5757 acc_val: 0.0000 time: 1.5963s\n",
      "Epoch: 037 loss_train: 224.8570 acc_train: 0.0000 loss_val: 225.2618 acc_val: 0.0000 time: 1.9435s\n",
      "Epoch: 038 loss_train: 225.0648 acc_train: 0.0000 loss_val: 227.7278 acc_val: 0.0000 time: 1.8338s\n",
      "Epoch: 039 loss_train: 225.9579 acc_train: 0.0000 loss_val: 228.1269 acc_val: 0.0000 time: 1.9334s\n",
      "Epoch: 040 loss_train: 225.9380 acc_train: 0.0000 loss_val: 226.5253 acc_val: 0.0000 time: 1.9641s\n",
      "Epoch: 041 loss_train: 225.3391 acc_train: 0.0000 loss_val: 223.7087 acc_val: 0.0000 time: 1.8477s\n",
      "Epoch: 042 loss_train: 225.9270 acc_train: 0.0000 loss_val: 224.4940 acc_val: 0.0000 time: 1.8783s\n",
      "Epoch: 043 loss_train: 224.7488 acc_train: 0.0000 loss_val: 224.5779 acc_val: 0.0000 time: 1.9462s\n",
      "Epoch: 044 loss_train: 223.4448 acc_train: 0.0000 loss_val: 223.4962 acc_val: 0.0000 time: 1.9435s\n",
      "Epoch: 045 loss_train: 225.0181 acc_train: 0.0000 loss_val: 223.0916 acc_val: 0.0000 time: 1.9672s\n",
      "Epoch: 046 loss_train: 223.5354 acc_train: 0.0000 loss_val: 223.2371 acc_val: 0.0000 time: 1.8575s\n",
      "Epoch: 047 loss_train: 222.7398 acc_train: 0.0000 loss_val: 224.2992 acc_val: 0.0000 time: 1.6937s\n",
      "Epoch: 048 loss_train: 224.3960 acc_train: 0.0000 loss_val: 225.0166 acc_val: 0.0000 time: 1.6864s\n",
      "Epoch: 049 loss_train: 224.3891 acc_train: 0.0000 loss_val: 224.2969 acc_val: 0.0000 time: 1.8086s\n",
      "Epoch: 050 loss_train: 224.3278 acc_train: 0.0000 loss_val: 222.8042 acc_val: 0.0000 time: 1.8329s\n",
      "Epoch: 051 loss_train: 223.0997 acc_train: 0.0000 loss_val: 221.0011 acc_val: 0.0000 time: 1.8567s\n",
      "Epoch: 052 loss_train: 223.8036 acc_train: 0.0000 loss_val: 223.6896 acc_val: 0.0000 time: 1.9331s\n",
      "Epoch: 053 loss_train: 222.9886 acc_train: 0.0000 loss_val: 221.7703 acc_val: 0.0000 time: 1.9485s\n",
      "Epoch: 054 loss_train: 223.1765 acc_train: 0.0000 loss_val: 219.0636 acc_val: 0.0000 time: 1.9571s\n",
      "Epoch: 055 loss_train: 221.6591 acc_train: 0.0000 loss_val: 218.8900 acc_val: 0.0000 time: 1.9272s\n",
      "Epoch: 056 loss_train: 221.2264 acc_train: 0.0000 loss_val: 223.1812 acc_val: 0.0000 time: 1.8601s\n",
      "Epoch: 057 loss_train: 221.5722 acc_train: 0.0000 loss_val: 227.7090 acc_val: 0.0000 time: 1.7645s\n",
      "Epoch: 058 loss_train: 224.2312 acc_train: 0.0000 loss_val: 228.8067 acc_val: 0.0000 time: 1.6587s\n",
      "Epoch: 059 loss_train: 223.3447 acc_train: 0.0000 loss_val: 226.7077 acc_val: 0.0000 time: 1.6670s\n",
      "Epoch: 060 loss_train: 223.3300 acc_train: 0.0000 loss_val: 222.2260 acc_val: 0.0000 time: 1.6633s\n",
      "Epoch: 061 loss_train: 222.1249 acc_train: 0.0000 loss_val: 217.9801 acc_val: 0.0000 time: 1.7134s\n",
      "Epoch: 062 loss_train: 222.0404 acc_train: 0.0000 loss_val: 219.9758 acc_val: 0.0000 time: 1.7585s\n",
      "Epoch: 063 loss_train: 220.2688 acc_train: 0.0000 loss_val: 224.2457 acc_val: 0.0000 time: 1.7294s\n",
      "Epoch: 064 loss_train: 222.2394 acc_train: 0.0000 loss_val: 225.9805 acc_val: 0.0000 time: 1.8505s\n",
      "Epoch: 065 loss_train: 221.6704 acc_train: 0.0000 loss_val: 225.4984 acc_val: 0.0000 time: 1.7370s\n",
      "Epoch: 066 loss_train: 220.5343 acc_train: 0.0000 loss_val: 222.9268 acc_val: 0.0000 time: 1.6509s\n",
      "Epoch: 067 loss_train: 220.8915 acc_train: 0.0000 loss_val: 221.0538 acc_val: 0.0000 time: 1.6780s\n",
      "Epoch: 068 loss_train: 221.4318 acc_train: 0.0000 loss_val: 219.0601 acc_val: 0.0000 time: 1.8532s\n",
      "Epoch: 069 loss_train: 222.8653 acc_train: 0.0000 loss_val: 226.5125 acc_val: 0.0000 time: 1.6895s\n",
      "Epoch: 070 loss_train: 221.1520 acc_train: 0.0000 loss_val: 230.7574 acc_val: 0.0000 time: 1.8293s\n",
      "Epoch: 071 loss_train: 224.4846 acc_train: 0.0000 loss_val: 231.9363 acc_val: 0.0000 time: 1.8361s\n",
      "Epoch: 072 loss_train: 224.3475 acc_train: 0.0000 loss_val: 229.8371 acc_val: 0.0000 time: 1.8109s\n",
      "Epoch: 073 loss_train: 223.3441 acc_train: 0.0000 loss_val: 225.8734 acc_val: 0.0000 time: 1.8938s\n",
      "Epoch: 074 loss_train: 223.9941 acc_train: 0.0000 loss_val: 222.1996 acc_val: 0.0000 time: 1.8412s\n",
      "Epoch: 075 loss_train: 220.9405 acc_train: 0.0000 loss_val: 219.1328 acc_val: 0.0000 time: 1.7477s\n",
      "Epoch: 076 loss_train: 222.0737 acc_train: 0.0000 loss_val: 220.7530 acc_val: 0.0000 time: 1.6866s\n",
      "Epoch: 077 loss_train: 220.3291 acc_train: 0.0000 loss_val: 222.3453 acc_val: 0.0000 time: 1.6870s\n",
      "Epoch: 078 loss_train: 221.1227 acc_train: 0.0000 loss_val: 224.8978 acc_val: 0.0000 time: 1.8050s\n",
      "Epoch: 079 loss_train: 222.6162 acc_train: 0.0000 loss_val: 225.8190 acc_val: 0.0000 time: 1.8587s\n",
      "Epoch: 080 loss_train: 223.0709 acc_train: 0.0000 loss_val: 224.6472 acc_val: 0.0000 time: 1.7396s\n",
      "Epoch: 081 loss_train: 222.2288 acc_train: 0.0000 loss_val: 222.9265 acc_val: 0.0000 time: 1.8675s\n",
      "Epoch: 082 loss_train: 221.5060 acc_train: 0.0000 loss_val: 219.7920 acc_val: 0.0000 time: 1.6807s\n",
      "Epoch: 083 loss_train: 220.4962 acc_train: 0.0000 loss_val: 222.4843 acc_val: 0.0000 time: 1.7464s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 084 loss_train: 218.9583 acc_train: 0.0000 loss_val: 223.1877 acc_val: 0.0000 time: 1.8053s\n",
      "Epoch: 085 loss_train: 219.5648 acc_train: 0.0000 loss_val: 221.0125 acc_val: 0.0000 time: 1.7546s\n",
      "Epoch: 086 loss_train: 221.0573 acc_train: 0.0000 loss_val: 218.9377 acc_val: 0.0000 time: 1.7089s\n",
      "Epoch: 087 loss_train: 219.3339 acc_train: 0.0000 loss_val: 222.2783 acc_val: 0.0000 time: 1.6354s\n",
      "Epoch: 088 loss_train: 219.8356 acc_train: 0.0000 loss_val: 224.8689 acc_val: 0.0000 time: 1.6325s\n",
      "Epoch: 089 loss_train: 221.4472 acc_train: 0.0000 loss_val: 225.2429 acc_val: 0.0000 time: 1.7115s\n",
      "Epoch: 090 loss_train: 221.1520 acc_train: 0.0000 loss_val: 224.1464 acc_val: 0.0000 time: 1.9375s\n",
      "Epoch: 091 loss_train: 219.1220 acc_train: 0.0000 loss_val: 220.2904 acc_val: 0.0000 time: 1.8273s\n",
      "Epoch: 092 loss_train: 219.8068 acc_train: 0.0000 loss_val: 216.6062 acc_val: 0.0000 time: 1.6997s\n",
      "Epoch: 093 loss_train: 220.8568 acc_train: 0.0000 loss_val: 221.1796 acc_val: 0.0000 time: 1.4481s\n",
      "Epoch: 094 loss_train: 218.9041 acc_train: 0.0000 loss_val: 224.8752 acc_val: 0.0000 time: 1.5347s\n",
      "Epoch: 095 loss_train: 219.7635 acc_train: 0.0000 loss_val: 225.4150 acc_val: 0.0000 time: 1.4488s\n",
      "Epoch: 096 loss_train: 220.1112 acc_train: 0.0000 loss_val: 223.8253 acc_val: 0.0000 time: 1.4414s\n",
      "Epoch: 097 loss_train: 219.3049 acc_train: 0.0000 loss_val: 220.0153 acc_val: 0.0000 time: 1.4529s\n",
      "Epoch: 098 loss_train: 219.3468 acc_train: 0.0000 loss_val: 216.1059 acc_val: 0.0000 time: 1.4507s\n",
      "Epoch: 099 loss_train: 223.6534 acc_train: 0.0000 loss_val: 232.1217 acc_val: 0.0000 time: 1.4482s\n",
      "Epoch: 100 loss_train: 224.0089 acc_train: 0.0000 loss_val: 241.4760 acc_val: 0.0000 time: 1.4618s\n",
      "Epoch: 101 loss_train: 230.4711 acc_train: 0.0000 loss_val: 244.6516 acc_val: 0.0000 time: 1.5924s\n",
      "Epoch: 102 loss_train: 232.1059 acc_train: 0.0000 loss_val: 242.5835 acc_val: 0.0000 time: 1.6405s\n",
      "Epoch: 103 loss_train: 233.1829 acc_train: 0.0000 loss_val: 238.6149 acc_val: 0.0000 time: 1.5757s\n",
      "Epoch: 104 loss_train: 240.0899 acc_train: 0.0000 loss_val: 242.9533 acc_val: 0.0000 time: 1.6286s\n",
      "Epoch: 105 loss_train: 231.4819 acc_train: 0.0000 loss_val: 242.9048 acc_val: 0.0000 time: 1.4614s\n",
      "Epoch: 106 loss_train: 232.4378 acc_train: 0.0000 loss_val: 239.8756 acc_val: 0.0000 time: 1.4410s\n",
      "Epoch: 107 loss_train: 231.5705 acc_train: 0.0000 loss_val: 234.9183 acc_val: 0.0000 time: 1.4472s\n",
      "Epoch: 108 loss_train: 229.1089 acc_train: 0.0000 loss_val: 228.6014 acc_val: 0.0000 time: 1.4659s\n",
      "Epoch: 109 loss_train: 224.5626 acc_train: 0.0000 loss_val: 222.9118 acc_val: 0.0000 time: 1.4616s\n",
      "Epoch: 110 loss_train: 225.9574 acc_train: 0.0000 loss_val: 221.2042 acc_val: 0.0000 time: 1.4665s\n",
      "Epoch: 111 loss_train: 233.6480 acc_train: 0.0000 loss_val: 221.6969 acc_val: 0.0000 time: 1.4643s\n",
      "Epoch: 112 loss_train: 229.8839 acc_train: 0.0000 loss_val: 226.0301 acc_val: 0.0000 time: 1.4704s\n",
      "Epoch: 113 loss_train: 224.8768 acc_train: 0.0000 loss_val: 231.0239 acc_val: 0.0000 time: 1.4605s\n",
      "Epoch: 114 loss_train: 225.3850 acc_train: 0.0000 loss_val: 234.2198 acc_val: 0.0000 time: 1.4693s\n",
      "Epoch: 115 loss_train: 228.8188 acc_train: 0.0000 loss_val: 235.4288 acc_val: 0.0000 time: 1.4466s\n",
      "Epoch: 116 loss_train: 229.7840 acc_train: 0.0000 loss_val: 234.7988 acc_val: 0.0000 time: 1.4506s\n",
      "Epoch: 117 loss_train: 228.6599 acc_train: 0.0000 loss_val: 232.5804 acc_val: 0.0000 time: 1.6032s\n",
      "Epoch: 118 loss_train: 227.4707 acc_train: 0.0000 loss_val: 229.3478 acc_val: 0.0000 time: 1.5628s\n",
      "Epoch: 119 loss_train: 225.9373 acc_train: 0.0000 loss_val: 225.8501 acc_val: 0.0000 time: 1.5970s\n",
      "Epoch: 120 loss_train: 224.4388 acc_train: 0.0000 loss_val: 222.9841 acc_val: 0.0000 time: 1.6049s\n",
      "Epoch: 121 loss_train: 225.3021 acc_train: 0.0000 loss_val: 222.0147 acc_val: 0.0000 time: 1.6679s\n",
      "Epoch: 122 loss_train: 229.6814 acc_train: 0.0000 loss_val: 223.3402 acc_val: 0.0000 time: 1.5036s\n",
      "Epoch: 123 loss_train: 221.7121 acc_train: 0.0000 loss_val: 225.1884 acc_val: 0.0000 time: 1.4889s\n",
      "Epoch: 124 loss_train: 222.4346 acc_train: 0.0000 loss_val: 225.9806 acc_val: 0.0000 time: 1.4950s\n",
      "Epoch: 125 loss_train: 224.4130 acc_train: 0.0000 loss_val: 225.6855 acc_val: 0.0000 time: 1.5539s\n",
      "Epoch: 126 loss_train: 221.9853 acc_train: 0.0000 loss_val: 224.3880 acc_val: 0.0000 time: 1.4889s\n",
      "Epoch: 127 loss_train: 221.8115 acc_train: 0.0000 loss_val: 221.9000 acc_val: 0.0000 time: 1.5587s\n",
      "Epoch: 128 loss_train: 221.3713 acc_train: 0.0000 loss_val: 219.4678 acc_val: 0.0000 time: 1.5375s\n",
      "Epoch: 129 loss_train: 223.8568 acc_train: 0.0000 loss_val: 219.3992 acc_val: 0.0000 time: 1.5252s\n",
      "Epoch: 130 loss_train: 222.3518 acc_train: 0.0000 loss_val: 221.0213 acc_val: 0.0000 time: 1.5550s\n",
      "Epoch: 131 loss_train: 223.3680 acc_train: 0.0000 loss_val: 223.1818 acc_val: 0.0000 time: 1.6187s\n",
      "Epoch: 132 loss_train: 219.1804 acc_train: 0.0000 loss_val: 223.8118 acc_val: 0.0000 time: 1.5072s\n",
      "Epoch: 133 loss_train: 220.0243 acc_train: 0.0000 loss_val: 222.9399 acc_val: 0.0000 time: 1.6077s\n",
      "Epoch: 134 loss_train: 220.5388 acc_train: 0.0000 loss_val: 221.7305 acc_val: 0.0000 time: 1.6565s\n",
      "Epoch: 135 loss_train: 221.7986 acc_train: 0.0000 loss_val: 221.3434 acc_val: 0.0000 time: 1.9179s\n",
      "Epoch: 136 loss_train: 219.8427 acc_train: 0.0000 loss_val: 220.7326 acc_val: 0.0000 time: 1.9937s\n",
      "Epoch: 137 loss_train: 219.8249 acc_train: 0.0000 loss_val: 219.5915 acc_val: 0.0000 time: 1.8357s\n",
      "Epoch: 138 loss_train: 220.1921 acc_train: 0.0000 loss_val: 219.4043 acc_val: 0.0000 time: 1.9563s\n",
      "Epoch: 139 loss_train: 218.6435 acc_train: 0.0000 loss_val: 218.9554 acc_val: 0.0000 time: 1.8039s\n",
      "Epoch: 140 loss_train: 219.0668 acc_train: 0.0000 loss_val: 218.6292 acc_val: 0.0000 time: 1.6948s\n",
      "Epoch: 141 loss_train: 220.3955 acc_train: 0.0000 loss_val: 219.2059 acc_val: 0.0000 time: 1.8835s\n",
      "Epoch: 142 loss_train: 221.8004 acc_train: 0.0000 loss_val: 222.6229 acc_val: 0.0000 time: 1.6862s\n",
      "Epoch: 143 loss_train: 219.6012 acc_train: 0.0000 loss_val: 224.2906 acc_val: 0.0000 time: 1.6836s\n",
      "Epoch: 144 loss_train: 222.4700 acc_train: 0.0000 loss_val: 224.2150 acc_val: 0.0000 time: 1.7393s\n",
      "Epoch: 145 loss_train: 221.2677 acc_train: 0.0000 loss_val: 222.1986 acc_val: 0.0000 time: 1.7033s\n",
      "Epoch: 146 loss_train: 220.4349 acc_train: 0.0000 loss_val: 218.7986 acc_val: 0.0000 time: 1.6938s\n",
      "Epoch: 147 loss_train: 221.6894 acc_train: 0.0000 loss_val: 220.7475 acc_val: 0.0000 time: 1.7336s\n",
      "Epoch: 148 loss_train: 218.5910 acc_train: 0.0000 loss_val: 221.1608 acc_val: 0.0000 time: 1.6681s\n",
      "Epoch: 149 loss_train: 218.8987 acc_train: 0.0000 loss_val: 220.9256 acc_val: 0.0000 time: 1.6854s\n",
      "Epoch: 150 loss_train: 219.1582 acc_train: 0.0000 loss_val: 220.0289 acc_val: 0.0000 time: 1.8542s\n",
      "Epoch: 151 loss_train: 216.9288 acc_train: 0.0000 loss_val: 217.9593 acc_val: 0.0000 time: 1.8618s\n",
      "Epoch: 152 loss_train: 219.5072 acc_train: 0.0000 loss_val: 216.4332 acc_val: 0.0000 time: 1.8642s\n",
      "Epoch: 153 loss_train: 216.6867 acc_train: 0.0000 loss_val: 216.2363 acc_val: 0.0000 time: 1.7540s\n",
      "Epoch: 154 loss_train: 217.8404 acc_train: 0.0000 loss_val: 217.6562 acc_val: 0.0000 time: 1.8653s\n",
      "Epoch: 155 loss_train: 216.3683 acc_train: 0.0000 loss_val: 219.1102 acc_val: 0.0000 time: 1.9569s\n",
      "Epoch: 156 loss_train: 217.4351 acc_train: 0.0000 loss_val: 220.1429 acc_val: 0.0000 time: 1.9646s\n",
      "Epoch: 157 loss_train: 216.8072 acc_train: 0.0000 loss_val: 220.2437 acc_val: 0.0000 time: 1.9394s\n",
      "Epoch: 158 loss_train: 219.0802 acc_train: 0.0000 loss_val: 219.8025 acc_val: 0.0000 time: 1.8710s\n",
      "Epoch: 159 loss_train: 218.5710 acc_train: 0.0000 loss_val: 219.0065 acc_val: 0.0000 time: 1.7164s\n",
      "Epoch: 160 loss_train: 217.4618 acc_train: 0.0000 loss_val: 218.9533 acc_val: 0.0000 time: 1.6759s\n",
      "Epoch: 161 loss_train: 216.3386 acc_train: 0.0000 loss_val: 216.8000 acc_val: 0.0000 time: 1.7408s\n",
      "Epoch: 162 loss_train: 217.5092 acc_train: 0.0000 loss_val: 214.2569 acc_val: 0.0000 time: 1.7548s\n",
      "Epoch: 163 loss_train: 218.9081 acc_train: 0.0000 loss_val: 222.8826 acc_val: 0.0000 time: 1.7006s\n",
      "Epoch: 164 loss_train: 218.3380 acc_train: 0.0000 loss_val: 227.8564 acc_val: 0.0000 time: 1.8588s\n",
      "Epoch: 165 loss_train: 221.6868 acc_train: 0.0000 loss_val: 228.6950 acc_val: 0.0000 time: 1.7527s\n",
      "Epoch: 166 loss_train: 222.8661 acc_train: 0.0000 loss_val: 225.7517 acc_val: 0.0000 time: 1.7538s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 167 loss_train: 220.7253 acc_train: 0.0000 loss_val: 219.0495 acc_val: 0.0000 time: 1.7969s\n",
      "Epoch: 168 loss_train: 215.9820 acc_train: 0.0000 loss_val: 211.9776 acc_val: 0.0000 time: 1.7024s\n",
      "Epoch: 169 loss_train: 240.1435 acc_train: 0.0000 loss_val: 234.1701 acc_val: 0.0000 time: 1.9039s\n",
      "Epoch: 170 loss_train: 227.2797 acc_train: 0.0000 loss_val: 247.7998 acc_val: 0.0000 time: 1.9998s\n",
      "Epoch: 171 loss_train: 238.0425 acc_train: 0.0000 loss_val: 253.4273 acc_val: 0.0000 time: 2.0061s\n",
      "Epoch: 172 loss_train: 242.7414 acc_train: 0.0000 loss_val: 255.2662 acc_val: 0.0000 time: 1.9742s\n",
      "Epoch: 173 loss_train: 244.8840 acc_train: 0.0000 loss_val: 255.1846 acc_val: 0.0000 time: 1.9542s\n",
      "Epoch: 174 loss_train: 243.5557 acc_train: 0.0000 loss_val: 254.0665 acc_val: 0.0000 time: 1.9284s\n",
      "Epoch: 175 loss_train: 244.0533 acc_train: 0.0000 loss_val: 252.1124 acc_val: 0.0000 time: 2.0398s\n",
      "Epoch: 176 loss_train: 242.5779 acc_train: 0.0000 loss_val: 249.4178 acc_val: 0.0000 time: 1.8817s\n",
      "Epoch: 177 loss_train: 240.3021 acc_train: 0.0000 loss_val: 245.6338 acc_val: 0.0000 time: 1.7692s\n",
      "Epoch: 178 loss_train: 237.9328 acc_train: 0.0000 loss_val: 240.3358 acc_val: 0.0000 time: 1.8079s\n",
      "Epoch: 179 loss_train: 238.9009 acc_train: 0.0000 loss_val: 240.7769 acc_val: 0.0000 time: 1.7780s\n",
      "Epoch: 180 loss_train: 235.4309 acc_train: 0.0000 loss_val: 239.8402 acc_val: 0.0000 time: 1.8250s\n",
      "Epoch: 181 loss_train: 233.9262 acc_train: 0.0000 loss_val: 237.7187 acc_val: 0.0000 time: 1.9189s\n",
      "Epoch: 182 loss_train: 232.1172 acc_train: 0.0000 loss_val: 234.6856 acc_val: 0.0000 time: 1.7742s\n",
      "Epoch: 183 loss_train: 231.0313 acc_train: 0.0000 loss_val: 230.6732 acc_val: 0.0000 time: 1.7329s\n",
      "Epoch: 184 loss_train: 230.9044 acc_train: 0.0000 loss_val: 226.9997 acc_val: 0.0000 time: 1.7166s\n",
      "Epoch: 185 loss_train: 241.7023 acc_train: 0.0000 loss_val: 238.9725 acc_val: 0.0000 time: 1.7806s\n",
      "Epoch: 186 loss_train: 233.4308 acc_train: 0.0000 loss_val: 244.0392 acc_val: 0.0000 time: 1.8377s\n",
      "Epoch: 187 loss_train: 237.3594 acc_train: 0.0000 loss_val: 246.3419 acc_val: 0.0000 time: 1.7987s\n",
      "Epoch: 188 loss_train: 239.3709 acc_train: 0.0000 loss_val: 247.1295 acc_val: 0.0000 time: 1.7762s\n",
      "Epoch: 189 loss_train: 239.0034 acc_train: 0.0000 loss_val: 247.2535 acc_val: 0.0000 time: 1.8862s\n",
      "Epoch: 190 loss_train: 239.7275 acc_train: 0.0000 loss_val: 246.9755 acc_val: 0.0000 time: 1.8244s\n",
      "Epoch: 191 loss_train: 238.8048 acc_train: 0.0000 loss_val: 246.4524 acc_val: 0.0000 time: 1.7210s\n",
      "Epoch: 192 loss_train: 238.8015 acc_train: 0.0000 loss_val: 245.7058 acc_val: 0.0000 time: 1.7709s\n",
      "Epoch: 193 loss_train: 238.9086 acc_train: 0.0000 loss_val: 244.7721 acc_val: 0.0000 time: 1.7194s\n",
      "Epoch: 194 loss_train: 237.3961 acc_train: 0.0000 loss_val: 243.6747 acc_val: 0.0000 time: 1.7176s\n",
      "Epoch: 195 loss_train: 236.5387 acc_train: 0.0000 loss_val: 242.4263 acc_val: 0.0000 time: 1.8783s\n",
      "Epoch: 196 loss_train: 234.6374 acc_train: 0.0000 loss_val: 241.0345 acc_val: 0.0000 time: 1.8951s\n",
      "Epoch: 197 loss_train: 234.3533 acc_train: 0.0000 loss_val: 239.4948 acc_val: 0.0000 time: 1.7339s\n",
      "Epoch: 198 loss_train: 233.1400 acc_train: 0.0000 loss_val: 237.7778 acc_val: 0.0000 time: 1.7092s\n",
      "Epoch: 199 loss_train: 232.8066 acc_train: 0.0000 loss_val: 236.2218 acc_val: 0.0000 time: 1.9016s\n",
      "Epoch: 200 loss_train: 231.2360 acc_train: 0.0000 loss_val: 234.9169 acc_val: 0.0000 time: 1.8524s\n",
      "Epoch: 201 loss_train: 229.6586 acc_train: 0.0000 loss_val: 233.7916 acc_val: 0.0000 time: 2.0065s\n",
      "Epoch: 202 loss_train: 230.8212 acc_train: 0.0000 loss_val: 233.0609 acc_val: 0.0000 time: 1.7861s\n",
      "Epoch: 203 loss_train: 229.8364 acc_train: 0.0000 loss_val: 232.5121 acc_val: 0.0000 time: 1.8757s\n",
      "Epoch: 204 loss_train: 229.3709 acc_train: 0.0000 loss_val: 232.2072 acc_val: 0.0000 time: 1.9139s\n",
      "Epoch: 205 loss_train: 228.6104 acc_train: 0.0000 loss_val: 232.1682 acc_val: 0.0000 time: 1.7164s\n",
      "Epoch: 206 loss_train: 226.1633 acc_train: 0.0000 loss_val: 232.2923 acc_val: 0.0000 time: 1.9178s\n",
      "Epoch: 207 loss_train: 226.9759 acc_train: 0.0000 loss_val: 232.6582 acc_val: 0.0000 time: 1.8092s\n",
      "Epoch: 208 loss_train: 226.7090 acc_train: 0.0000 loss_val: 233.2586 acc_val: 0.0000 time: 1.7070s\n",
      "Epoch: 209 loss_train: 228.2663 acc_train: 0.0000 loss_val: 234.0875 acc_val: 0.0000 time: 1.6881s\n",
      "Epoch: 210 loss_train: 233.3491 acc_train: 0.0000 loss_val: 235.9147 acc_val: 0.0000 time: 1.8696s\n",
      "Epoch: 211 loss_train: 226.5264 acc_train: 0.0000 loss_val: 237.1680 acc_val: 0.0000 time: 1.9115s\n",
      "Epoch: 212 loss_train: 229.5866 acc_train: 0.0000 loss_val: 237.9539 acc_val: 0.0000 time: 1.8755s\n",
      "Epoch: 213 loss_train: 228.6192 acc_train: 0.0000 loss_val: 237.8957 acc_val: 0.0000 time: 1.6993s\n",
      "Epoch: 214 loss_train: 229.2379 acc_train: 0.0000 loss_val: 237.0822 acc_val: 0.0000 time: 1.7121s\n",
      "Epoch: 215 loss_train: 227.7294 acc_train: 0.0000 loss_val: 235.6246 acc_val: 0.0000 time: 1.7147s\n",
      "Epoch: 216 loss_train: 229.0706 acc_train: 0.0000 loss_val: 233.6528 acc_val: 0.0000 time: 1.7059s\n",
      "Epoch: 217 loss_train: 249.4028 acc_train: 0.0000 loss_val: 234.0243 acc_val: 0.0000 time: 1.7237s\n",
      "Epoch: 218 loss_train: 225.3855 acc_train: 0.0000 loss_val: 233.7114 acc_val: 0.0000 time: 1.7209s\n",
      "Epoch: 219 loss_train: 224.6414 acc_train: 0.0000 loss_val: 232.4928 acc_val: 0.0000 time: 1.8106s\n",
      "Epoch: 220 loss_train: 224.7215 acc_train: 0.0000 loss_val: 221.6443 acc_val: 0.0000 time: 1.9124s\n",
      "Epoch: 221 loss_train: 227.5222 acc_train: 0.0000 loss_val: 285.5723 acc_val: 0.0000 time: 1.6910s\n",
      "Epoch: 222 loss_train: 552.9086 acc_train: 0.0000 loss_val: 239.8524 acc_val: 0.0000 time: 1.7059s\n",
      "Epoch: 223 loss_train: 230.4911 acc_train: 0.0000 loss_val: 243.0927 acc_val: 0.0000 time: 1.7272s\n",
      "Epoch: 224 loss_train: 232.8139 acc_train: 0.0000 loss_val: 243.8366 acc_val: 0.0000 time: 1.6929s\n",
      "Epoch: 225 loss_train: 234.1916 acc_train: 0.0000 loss_val: 243.2786 acc_val: 0.0000 time: 1.7185s\n",
      "Epoch: 226 loss_train: 233.9161 acc_train: 0.0000 loss_val: 241.9721 acc_val: 0.0000 time: 1.7030s\n",
      "Epoch: 227 loss_train: 235.3117 acc_train: 0.0000 loss_val: 240.1693 acc_val: 0.0000 time: 1.6809s\n",
      "Epoch: 228 loss_train: 232.7552 acc_train: 0.0000 loss_val: 237.7725 acc_val: 0.0000 time: 1.8412s\n",
      "Epoch: 229 loss_train: 233.3174 acc_train: 0.0000 loss_val: 234.9223 acc_val: 0.0000 time: 1.8848s\n",
      "Epoch: 230 loss_train: 230.5583 acc_train: 0.0000 loss_val: 231.6466 acc_val: 0.0000 time: 1.8230s\n",
      "Epoch: 231 loss_train: 232.1207 acc_train: 0.0000 loss_val: 228.4273 acc_val: 0.0000 time: 1.7116s\n",
      "Epoch: 232 loss_train: 231.6322 acc_train: 0.0000 loss_val: 225.1169 acc_val: 0.0000 time: 1.8198s\n",
      "Epoch: 233 loss_train: 228.3332 acc_train: 0.0000 loss_val: 222.4210 acc_val: 0.0000 time: 1.8451s\n",
      "Epoch: 234 loss_train: 229.1279 acc_train: 0.0000 loss_val: 221.7070 acc_val: 0.0000 time: 1.7838s\n",
      "Epoch: 235 loss_train: 228.3693 acc_train: 0.0000 loss_val: 226.1107 acc_val: 0.0000 time: 1.6233s\n",
      "Epoch: 236 loss_train: 226.6695 acc_train: 0.0000 loss_val: 229.0867 acc_val: 0.0000 time: 1.7092s\n",
      "Epoch: 237 loss_train: 226.9673 acc_train: 0.0000 loss_val: 230.1398 acc_val: 0.0000 time: 1.9157s\n",
      "Epoch: 238 loss_train: 227.6334 acc_train: 0.0000 loss_val: 230.0432 acc_val: 0.0000 time: 1.8158s\n",
      "Epoch: 239 loss_train: 227.5117 acc_train: 0.0000 loss_val: 229.3085 acc_val: 0.0000 time: 1.8469s\n",
      "Epoch: 240 loss_train: 226.6584 acc_train: 0.0000 loss_val: 227.0812 acc_val: 0.0000 time: 1.8884s\n",
      "Epoch: 241 loss_train: 225.8845 acc_train: 0.0000 loss_val: 224.2576 acc_val: 0.0000 time: 1.6978s\n",
      "Epoch: 242 loss_train: 224.9776 acc_train: 0.0000 loss_val: 221.4164 acc_val: 0.0000 time: 1.7094s\n",
      "Epoch: 243 loss_train: 225.9486 acc_train: 0.0000 loss_val: 220.8567 acc_val: 0.0000 time: 1.9249s\n",
      "Epoch: 244 loss_train: 225.5953 acc_train: 0.0000 loss_val: 221.6552 acc_val: 0.0000 time: 1.9033s\n",
      "Epoch: 245 loss_train: 226.0511 acc_train: 0.0000 loss_val: 222.4678 acc_val: 0.0000 time: 1.8629s\n",
      "Epoch: 246 loss_train: 225.0690 acc_train: 0.0000 loss_val: 222.6420 acc_val: 0.0000 time: 1.7319s\n",
      "Epoch: 247 loss_train: 225.1534 acc_train: 0.0000 loss_val: 222.5138 acc_val: 0.0000 time: 1.9563s\n",
      "Epoch: 248 loss_train: 227.7006 acc_train: 0.0000 loss_val: 222.4317 acc_val: 0.0000 time: 1.7472s\n",
      "Epoch: 249 loss_train: 225.1317 acc_train: 0.0000 loss_val: 221.7957 acc_val: 0.0000 time: 2.0065s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250 loss_train: 223.6787 acc_train: 0.0000 loss_val: 221.1017 acc_val: 0.0000 time: 1.9410s\n",
      "Epoch: 251 loss_train: 224.6094 acc_train: 0.0000 loss_val: 220.3677 acc_val: 0.0000 time: 1.8873s\n",
      "Epoch: 252 loss_train: 226.7854 acc_train: 0.0000 loss_val: 220.1692 acc_val: 0.0000 time: 1.7566s\n",
      "Epoch: 253 loss_train: 224.2514 acc_train: 0.0000 loss_val: 219.9637 acc_val: 0.0000 time: 1.8540s\n",
      "Epoch: 254 loss_train: 228.5865 acc_train: 0.0000 loss_val: 226.9635 acc_val: 0.0000 time: 1.8839s\n",
      "Epoch: 255 loss_train: 227.0460 acc_train: 0.0000 loss_val: 231.3051 acc_val: 0.0000 time: 1.9851s\n",
      "Epoch: 256 loss_train: 227.3246 acc_train: 0.0000 loss_val: 233.2226 acc_val: 0.0000 time: 1.8298s\n",
      "Epoch: 257 loss_train: 228.9275 acc_train: 0.0000 loss_val: 233.7478 acc_val: 0.0000 time: 1.9039s\n",
      "Epoch: 258 loss_train: 230.1133 acc_train: 0.0000 loss_val: 233.3367 acc_val: 0.0000 time: 1.7910s\n",
      "Epoch: 259 loss_train: 231.4871 acc_train: 0.0000 loss_val: 232.1754 acc_val: 0.0000 time: 1.8291s\n",
      "Epoch: 260 loss_train: 228.7682 acc_train: 0.0000 loss_val: 230.1856 acc_val: 0.0000 time: 1.8882s\n",
      "Epoch: 261 loss_train: 227.6155 acc_train: 0.0000 loss_val: 227.2719 acc_val: 0.0000 time: 1.6972s\n",
      "Epoch: 262 loss_train: 227.5329 acc_train: 0.0000 loss_val: 225.8268 acc_val: 0.0000 time: 1.6974s\n",
      "Epoch: 263 loss_train: 226.0643 acc_train: 0.0000 loss_val: 223.8035 acc_val: 0.0000 time: 1.7596s\n",
      "Epoch: 264 loss_train: 224.7701 acc_train: 0.0000 loss_val: 222.3345 acc_val: 0.0000 time: 1.8827s\n",
      "Epoch: 265 loss_train: 227.3794 acc_train: 0.0000 loss_val: 222.0694 acc_val: 0.0000 time: 1.9724s\n",
      "Epoch: 266 loss_train: 224.1764 acc_train: 0.0000 loss_val: 221.9076 acc_val: 0.0000 time: 1.7893s\n",
      "Epoch: 267 loss_train: 231.7389 acc_train: 0.0000 loss_val: 223.1454 acc_val: 0.0000 time: 1.7106s\n",
      "Epoch: 268 loss_train: 222.6197 acc_train: 0.0000 loss_val: 220.7341 acc_val: 0.0000 time: 1.7150s\n",
      "Epoch: 269 loss_train: 217.2427 acc_train: 0.0000 loss_val: 296.6942 acc_val: 0.0000 time: 1.6874s\n",
      "Epoch: 270 loss_train: 323.3482 acc_train: 0.0000 loss_val: 213.3208 acc_val: 0.0000 time: 1.7900s\n",
      "Epoch: 271 loss_train: 213.0135 acc_train: 0.0000 loss_val: 225.7410 acc_val: 0.0000 time: 1.9564s\n",
      "Epoch: 272 loss_train: 224.2015 acc_train: 0.0000 loss_val: 227.2213 acc_val: 0.0000 time: 1.8357s\n",
      "Epoch: 273 loss_train: 227.9468 acc_train: 0.0000 loss_val: 227.8253 acc_val: 0.0000 time: 1.8565s\n",
      "Epoch: 274 loss_train: 224.9887 acc_train: 0.0000 loss_val: 227.8602 acc_val: 0.0000 time: 1.9444s\n",
      "Epoch: 275 loss_train: 224.3622 acc_train: 0.0000 loss_val: 227.6299 acc_val: 0.0000 time: 1.8744s\n",
      "Epoch: 276 loss_train: 227.9500 acc_train: 0.0000 loss_val: 227.4013 acc_val: 0.0000 time: 1.8901s\n",
      "Epoch: 277 loss_train: 228.8401 acc_train: 0.0000 loss_val: 227.2274 acc_val: 0.0000 time: 1.8659s\n",
      "Epoch: 278 loss_train: 226.9034 acc_train: 0.0000 loss_val: 226.8808 acc_val: 0.0000 time: 1.9661s\n",
      "Epoch: 279 loss_train: 224.5725 acc_train: 0.0000 loss_val: 226.0438 acc_val: 0.0000 time: 1.9408s\n",
      "Epoch: 280 loss_train: 229.5789 acc_train: 0.0000 loss_val: 225.1183 acc_val: 0.0000 time: 1.7775s\n",
      "Epoch: 281 loss_train: 224.5379 acc_train: 0.0000 loss_val: 223.5671 acc_val: 0.0000 time: 1.6788s\n",
      "Epoch: 282 loss_train: 223.8118 acc_train: 0.0000 loss_val: 221.1709 acc_val: 0.0000 time: 1.6142s\n",
      "Epoch: 283 loss_train: 221.9439 acc_train: 0.0000 loss_val: 217.9426 acc_val: 0.0000 time: 1.6086s\n",
      "Epoch: 284 loss_train: 221.5531 acc_train: 0.0000 loss_val: 215.9944 acc_val: 0.0000 time: 1.6108s\n",
      "Epoch: 285 loss_train: 221.6463 acc_train: 0.0000 loss_val: 215.7566 acc_val: 0.0000 time: 1.6144s\n",
      "Epoch: 286 loss_train: 223.0816 acc_train: 0.0000 loss_val: 217.3490 acc_val: 0.0000 time: 1.8167s\n",
      "Epoch: 287 loss_train: 221.4804 acc_train: 0.0000 loss_val: 218.8843 acc_val: 0.0000 time: 1.7976s\n",
      "Epoch: 288 loss_train: 222.5652 acc_train: 0.0000 loss_val: 220.6867 acc_val: 0.0000 time: 1.8059s\n",
      "Epoch: 289 loss_train: 221.2523 acc_train: 0.0000 loss_val: 221.7506 acc_val: 0.0000 time: 1.6411s\n",
      "Epoch: 290 loss_train: 223.1223 acc_train: 0.0000 loss_val: 222.2070 acc_val: 0.0000 time: 1.6178s\n",
      "Epoch: 291 loss_train: 221.8311 acc_train: 0.0000 loss_val: 221.8304 acc_val: 0.0000 time: 1.6065s\n",
      "Epoch: 292 loss_train: 220.8629 acc_train: 0.0000 loss_val: 220.4113 acc_val: 0.0000 time: 1.5952s\n",
      "Epoch: 293 loss_train: 223.4198 acc_train: 0.0000 loss_val: 219.0454 acc_val: 0.0000 time: 1.9661s\n",
      "Epoch: 294 loss_train: 220.6936 acc_train: 0.0000 loss_val: 217.4202 acc_val: 0.0000 time: 1.9984s\n",
      "Epoch: 295 loss_train: 223.6335 acc_train: 0.0000 loss_val: 217.5045 acc_val: 0.0000 time: 1.9768s\n",
      "Epoch: 296 loss_train: 222.5903 acc_train: 0.0000 loss_val: 219.5928 acc_val: 0.0000 time: 1.9232s\n",
      "Epoch: 297 loss_train: 222.2016 acc_train: 0.0000 loss_val: 221.6830 acc_val: 0.0000 time: 1.8800s\n",
      "Epoch: 298 loss_train: 221.7072 acc_train: 0.0000 loss_val: 222.7754 acc_val: 0.0000 time: 1.9528s\n",
      "Epoch: 299 loss_train: 222.0011 acc_train: 0.0000 loss_val: 222.7347 acc_val: 0.0000 time: 1.7867s\n",
      "Epoch: 300 loss_train: 222.3361 acc_train: 0.0000 loss_val: 221.6918 acc_val: 0.0000 time: 1.5597s\n",
      "Epoch: 301 loss_train: 219.6897 acc_train: 0.0000 loss_val: 219.8541 acc_val: 0.0000 time: 1.9342s\n",
      "Epoch: 302 loss_train: 221.3948 acc_train: 0.0000 loss_val: 217.2287 acc_val: 0.0000 time: 1.4966s\n",
      "Epoch: 303 loss_train: 220.3024 acc_train: 0.0000 loss_val: 214.6368 acc_val: 0.0000 time: 1.5099s\n",
      "Epoch: 304 loss_train: 222.9884 acc_train: 0.0000 loss_val: 214.0302 acc_val: 0.0000 time: 1.5668s\n",
      "Epoch: 305 loss_train: 217.4884 acc_train: 0.0000 loss_val: 214.1555 acc_val: 0.0000 time: 1.4875s\n",
      "Epoch: 306 loss_train: 218.8957 acc_train: 0.0000 loss_val: 214.5806 acc_val: 0.0000 time: 1.5032s\n",
      "Epoch: 307 loss_train: 224.9388 acc_train: 0.0000 loss_val: 221.0268 acc_val: 0.0000 time: 1.5026s\n",
      "Epoch: 308 loss_train: 220.2260 acc_train: 0.0000 loss_val: 225.3423 acc_val: 0.0000 time: 1.4782s\n",
      "Epoch: 309 loss_train: 221.9830 acc_train: 0.0000 loss_val: 227.5506 acc_val: 0.0000 time: 1.4931s\n",
      "Epoch: 310 loss_train: 222.7674 acc_train: 0.0000 loss_val: 228.4676 acc_val: 0.0000 time: 1.6000s\n",
      "Epoch: 311 loss_train: 224.5345 acc_train: 0.0000 loss_val: 228.5354 acc_val: 0.0000 time: 1.9345s\n",
      "Epoch: 312 loss_train: 225.0724 acc_train: 0.0000 loss_val: 227.9737 acc_val: 0.0000 time: 1.8674s\n",
      "Epoch: 313 loss_train: 224.4590 acc_train: 0.0000 loss_val: 227.0029 acc_val: 0.0000 time: 1.8030s\n",
      "Epoch: 314 loss_train: 223.8006 acc_train: 0.0000 loss_val: 225.6680 acc_val: 0.0000 time: 1.9852s\n",
      "Epoch: 315 loss_train: 223.5860 acc_train: 0.0000 loss_val: 224.2674 acc_val: 0.0000 time: 1.8955s\n",
      "Epoch: 316 loss_train: 223.2560 acc_train: 0.0000 loss_val: 222.7585 acc_val: 0.0000 time: 1.9217s\n",
      "Epoch: 317 loss_train: 221.6213 acc_train: 0.0000 loss_val: 221.1553 acc_val: 0.0000 time: 1.7941s\n",
      "Epoch: 318 loss_train: 221.5531 acc_train: 0.0000 loss_val: 219.7736 acc_val: 0.0000 time: 1.8955s\n",
      "Epoch: 319 loss_train: 219.3692 acc_train: 0.0000 loss_val: 218.6558 acc_val: 0.0000 time: 1.8335s\n",
      "Epoch: 320 loss_train: 249.7026 acc_train: 0.0000 loss_val: 223.5744 acc_val: 0.0000 time: 1.8033s\n",
      "Epoch: 321 loss_train: 221.6950 acc_train: 0.0000 loss_val: 226.9435 acc_val: 0.0000 time: 1.9252s\n",
      "Epoch: 322 loss_train: 223.1198 acc_train: 0.0000 loss_val: 229.1012 acc_val: 0.0000 time: 1.9244s\n",
      "Epoch: 323 loss_train: 225.4527 acc_train: 0.0000 loss_val: 230.3146 acc_val: 0.0000 time: 1.8448s\n",
      "Epoch: 324 loss_train: 227.0004 acc_train: 0.0000 loss_val: 230.7779 acc_val: 0.0000 time: 1.9513s\n",
      "Epoch: 325 loss_train: 226.9301 acc_train: 0.0000 loss_val: 230.5966 acc_val: 0.0000 time: 1.7464s\n",
      "Epoch: 326 loss_train: 227.1098 acc_train: 0.0000 loss_val: 229.7804 acc_val: 0.0000 time: 1.6956s\n",
      "Epoch: 327 loss_train: 227.1885 acc_train: 0.0000 loss_val: 228.1622 acc_val: 0.0000 time: 1.7396s\n",
      "Epoch: 328 loss_train: 226.9874 acc_train: 0.0000 loss_val: 225.5676 acc_val: 0.0000 time: 1.7068s\n",
      "Epoch: 329 loss_train: 224.2772 acc_train: 0.0000 loss_val: 222.0139 acc_val: 0.0000 time: 1.7233s\n",
      "Epoch: 330 loss_train: 222.8845 acc_train: 0.0000 loss_val: 217.7952 acc_val: 0.0000 time: 1.7374s\n",
      "Epoch: 331 loss_train: 220.1264 acc_train: 0.0000 loss_val: 214.4016 acc_val: 0.0000 time: 1.7759s\n",
      "Epoch: 332 loss_train: 224.1447 acc_train: 0.0000 loss_val: 213.3447 acc_val: 0.0000 time: 1.9134s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333 loss_train: 228.0537 acc_train: 0.0000 loss_val: 218.8140 acc_val: 0.0000 time: 1.7040s\n",
      "Epoch: 334 loss_train: 221.0452 acc_train: 0.0000 loss_val: 224.6430 acc_val: 0.0000 time: 1.7412s\n",
      "Epoch: 335 loss_train: 222.7742 acc_train: 0.0000 loss_val: 228.6420 acc_val: 0.0000 time: 1.9141s\n",
      "Epoch: 336 loss_train: 225.8670 acc_train: 0.0000 loss_val: 230.8076 acc_val: 0.0000 time: 1.8807s\n",
      "Epoch: 337 loss_train: 227.2422 acc_train: 0.0000 loss_val: 231.5358 acc_val: 0.0000 time: 1.8827s\n",
      "Epoch: 338 loss_train: 227.6932 acc_train: 0.0000 loss_val: 231.2684 acc_val: 0.0000 time: 1.9323s\n",
      "Epoch: 339 loss_train: 227.9356 acc_train: 0.0000 loss_val: 230.1118 acc_val: 0.0000 time: 1.8226s\n",
      "Epoch: 340 loss_train: 226.7055 acc_train: 0.0000 loss_val: 227.9421 acc_val: 0.0000 time: 2.1219s\n",
      "Epoch: 341 loss_train: 224.9820 acc_train: 0.0000 loss_val: 224.8085 acc_val: 0.0000 time: 2.2602s\n",
      "Epoch: 342 loss_train: 222.9247 acc_train: 0.0000 loss_val: 220.6010 acc_val: 0.0000 time: 1.8484s\n",
      "Epoch: 343 loss_train: 219.6663 acc_train: 0.0000 loss_val: 215.5077 acc_val: 0.0000 time: 1.8118s\n",
      "Epoch: 344 loss_train: 221.0434 acc_train: 0.0000 loss_val: 210.0047 acc_val: 0.0000 time: 1.7087s\n",
      "Epoch: 345 loss_train: 219.1954 acc_train: 0.0000 loss_val: 1101.1034 acc_val: 0.0000 time: 1.7290s\n",
      "Epoch: 346 loss_train: 1524.1229 acc_train: 0.0000 loss_val: 229.5146 acc_val: 0.0000 time: 1.7033s\n",
      "Epoch: 347 loss_train: 226.6526 acc_train: 0.0000 loss_val: 240.8542 acc_val: 0.0000 time: 1.7607s\n",
      "Epoch: 348 loss_train: 236.3581 acc_train: 0.0000 loss_val: 244.9891 acc_val: 0.0000 time: 1.7291s\n",
      "Epoch: 349 loss_train: 240.5793 acc_train: 0.0000 loss_val: 246.9660 acc_val: 0.0000 time: 1.7348s\n",
      "Epoch: 350 loss_train: 242.8616 acc_train: 0.0000 loss_val: 247.9994 acc_val: 0.0000 time: 1.7415s\n",
      "Epoch: 351 loss_train: 244.4399 acc_train: 0.0000 loss_val: 248.4578 acc_val: 0.0000 time: 1.6827s\n",
      "Epoch: 352 loss_train: 244.3841 acc_train: 0.0000 loss_val: 248.6005 acc_val: 0.0000 time: 1.7069s\n",
      "Epoch: 353 loss_train: 245.0586 acc_train: 0.0000 loss_val: 248.5217 acc_val: 0.0000 time: 1.7619s\n",
      "Epoch: 354 loss_train: 245.4474 acc_train: 0.0000 loss_val: 248.2482 acc_val: 0.0000 time: 1.6809s\n",
      "Epoch: 355 loss_train: 245.6037 acc_train: 0.0000 loss_val: 247.8748 acc_val: 0.0000 time: 1.7311s\n",
      "Epoch: 356 loss_train: 244.7840 acc_train: 0.0000 loss_val: 247.4126 acc_val: 0.0000 time: 1.9305s\n",
      "Epoch: 357 loss_train: 244.9942 acc_train: 0.0000 loss_val: 246.8997 acc_val: 0.0000 time: 1.9501s\n",
      "Epoch: 358 loss_train: 244.9730 acc_train: 0.0000 loss_val: 246.3515 acc_val: 0.0000 time: 1.8154s\n",
      "Epoch: 359 loss_train: 244.0590 acc_train: 0.0000 loss_val: 245.7711 acc_val: 0.0000 time: 1.9513s\n",
      "Epoch: 360 loss_train: 243.8913 acc_train: 0.0000 loss_val: 245.1557 acc_val: 0.0000 time: 1.9639s\n",
      "Epoch: 361 loss_train: 243.5376 acc_train: 0.0000 loss_val: 244.5089 acc_val: 0.0000 time: 2.0557s\n",
      "Epoch: 362 loss_train: 242.7778 acc_train: 0.0000 loss_val: 243.8302 acc_val: 0.0000 time: 1.8850s\n",
      "Epoch: 363 loss_train: 242.2076 acc_train: 0.0000 loss_val: 243.1114 acc_val: 0.0000 time: 1.9653s\n",
      "Epoch: 364 loss_train: 241.5648 acc_train: 0.0000 loss_val: 242.3556 acc_val: 0.0000 time: 1.8096s\n",
      "Epoch: 365 loss_train: 241.0707 acc_train: 0.0000 loss_val: 241.5786 acc_val: 0.0000 time: 1.9778s\n",
      "Epoch: 366 loss_train: 240.3530 acc_train: 0.0000 loss_val: 240.7650 acc_val: 0.0000 time: 1.9964s\n",
      "Epoch: 367 loss_train: 239.9924 acc_train: 0.0000 loss_val: 239.9137 acc_val: 0.0000 time: 1.8513s\n",
      "Epoch: 368 loss_train: 239.1874 acc_train: 0.0000 loss_val: 239.0112 acc_val: 0.0000 time: 1.8774s\n",
      "Epoch: 369 loss_train: 238.1985 acc_train: 0.0000 loss_val: 238.0446 acc_val: 0.0000 time: 1.8960s\n",
      "Epoch: 370 loss_train: 237.5862 acc_train: 0.0000 loss_val: 237.0014 acc_val: 0.0000 time: 1.6980s\n",
      "Epoch: 371 loss_train: 236.2060 acc_train: 0.0000 loss_val: 235.9139 acc_val: 0.0000 time: 1.7405s\n",
      "Epoch: 372 loss_train: 236.0328 acc_train: 0.0000 loss_val: 234.7802 acc_val: 0.0000 time: 1.9310s\n",
      "Epoch: 373 loss_train: 235.2716 acc_train: 0.0000 loss_val: 233.6025 acc_val: 0.0000 time: 1.9294s\n",
      "Epoch: 374 loss_train: 233.9896 acc_train: 0.0000 loss_val: 232.3494 acc_val: 0.0000 time: 2.2022s\n",
      "Epoch: 375 loss_train: 233.1895 acc_train: 0.0000 loss_val: 230.9539 acc_val: 0.0000 time: 2.2419s\n",
      "Epoch: 376 loss_train: 232.2888 acc_train: 0.0000 loss_val: 229.4133 acc_val: 0.0000 time: 2.0997s\n",
      "Epoch: 377 loss_train: 231.2820 acc_train: 0.0000 loss_val: 227.8513 acc_val: 0.0000 time: 1.6843s\n",
      "Epoch: 378 loss_train: 229.9703 acc_train: 0.0000 loss_val: 226.1859 acc_val: 0.0000 time: 1.7081s\n",
      "Epoch: 379 loss_train: 229.8669 acc_train: 0.0000 loss_val: 224.5417 acc_val: 0.0000 time: 1.6863s\n",
      "Epoch: 380 loss_train: 228.3743 acc_train: 0.0000 loss_val: 222.9872 acc_val: 0.0000 time: 1.8123s\n",
      "Epoch: 381 loss_train: 227.9278 acc_train: 0.0000 loss_val: 221.6784 acc_val: 0.0000 time: 1.8475s\n",
      "Epoch: 382 loss_train: 227.3798 acc_train: 0.0000 loss_val: 220.7329 acc_val: 0.0000 time: 2.0612s\n",
      "Epoch: 383 loss_train: 227.6278 acc_train: 0.0000 loss_val: 220.0325 acc_val: 0.0000 time: 1.8924s\n",
      "Epoch: 384 loss_train: 225.2666 acc_train: 0.0000 loss_val: 219.4015 acc_val: 0.0000 time: 1.8737s\n",
      "Epoch: 385 loss_train: 225.7120 acc_train: 0.0000 loss_val: 218.8044 acc_val: 0.0000 time: 1.8417s\n",
      "Epoch: 386 loss_train: 225.8523 acc_train: 0.0000 loss_val: 218.3858 acc_val: 0.0000 time: 1.9200s\n",
      "Epoch: 387 loss_train: 222.8807 acc_train: 0.0000 loss_val: 218.1938 acc_val: 0.0000 time: 1.8524s\n",
      "Epoch: 388 loss_train: 222.8822 acc_train: 0.0000 loss_val: 218.2177 acc_val: 0.0000 time: 1.9715s\n",
      "Epoch: 389 loss_train: 222.6739 acc_train: 0.0000 loss_val: 218.2336 acc_val: 0.0000 time: 1.8509s\n",
      "Epoch: 390 loss_train: 223.3063 acc_train: 0.0000 loss_val: 218.4586 acc_val: 0.0000 time: 1.8528s\n",
      "Epoch: 391 loss_train: 222.5508 acc_train: 0.0000 loss_val: 218.6560 acc_val: 0.0000 time: 1.7802s\n",
      "Epoch: 392 loss_train: 222.4106 acc_train: 0.0000 loss_val: 218.7356 acc_val: 0.0000 time: 1.8309s\n",
      "Epoch: 393 loss_train: 221.5343 acc_train: 0.0000 loss_val: 218.5502 acc_val: 0.0000 time: 1.7687s\n",
      "Epoch: 394 loss_train: 223.1436 acc_train: 0.0000 loss_val: 218.2468 acc_val: 0.0000 time: 1.7203s\n",
      "Epoch: 395 loss_train: 222.0893 acc_train: 0.0000 loss_val: 217.5508 acc_val: 0.0000 time: 1.7576s\n",
      "Epoch: 396 loss_train: 222.1022 acc_train: 0.0000 loss_val: 216.6507 acc_val: 0.0000 time: 1.7198s\n",
      "Epoch: 397 loss_train: 223.4745 acc_train: 0.0000 loss_val: 215.9262 acc_val: 0.0000 time: 1.8058s\n",
      "Epoch: 398 loss_train: 221.0274 acc_train: 0.0000 loss_val: 215.2738 acc_val: 0.0000 time: 1.9342s\n",
      "Epoch: 399 loss_train: 222.1407 acc_train: 0.0000 loss_val: 214.8400 acc_val: 0.0000 time: 1.9558s\n",
      "Epoch: 400 loss_train: 221.2314 acc_train: 0.0000 loss_val: 214.6660 acc_val: 0.0000 time: 1.7255s\n",
      "Epoch: 401 loss_train: 221.0942 acc_train: 0.0000 loss_val: 214.5862 acc_val: 0.0000 time: 1.8046s\n",
      "Epoch: 402 loss_train: 219.3618 acc_train: 0.0000 loss_val: 214.5911 acc_val: 0.0000 time: 1.9159s\n",
      "Epoch: 403 loss_train: 219.6834 acc_train: 0.0000 loss_val: 214.7439 acc_val: 0.0000 time: 1.8584s\n",
      "Epoch: 404 loss_train: 219.5974 acc_train: 0.0000 loss_val: 214.8408 acc_val: 0.0000 time: 1.7371s\n",
      "Epoch: 405 loss_train: 220.8936 acc_train: 0.0000 loss_val: 214.9311 acc_val: 0.0000 time: 1.7934s\n",
      "Epoch: 406 loss_train: 218.5605 acc_train: 0.0000 loss_val: 214.8069 acc_val: 0.0000 time: 1.8778s\n",
      "Epoch: 407 loss_train: 218.6172 acc_train: 0.0000 loss_val: 214.6968 acc_val: 0.0000 time: 1.7156s\n",
      "Epoch: 408 loss_train: 220.0829 acc_train: 0.0000 loss_val: 214.7728 acc_val: 0.0000 time: 1.8438s\n",
      "Epoch: 409 loss_train: 220.3837 acc_train: 0.0000 loss_val: 215.1009 acc_val: 0.0000 time: 1.8140s\n",
      "Epoch: 410 loss_train: 218.9431 acc_train: 0.0000 loss_val: 215.2994 acc_val: 0.0000 time: 1.8725s\n",
      "Epoch: 411 loss_train: 218.7731 acc_train: 0.0000 loss_val: 215.2892 acc_val: 0.0000 time: 2.0317s\n",
      "Epoch: 412 loss_train: 217.9602 acc_train: 0.0000 loss_val: 215.0769 acc_val: 0.0000 time: 1.9032s\n",
      "Epoch: 413 loss_train: 220.3421 acc_train: 0.0000 loss_val: 214.8984 acc_val: 0.0000 time: 1.7626s\n",
      "Epoch: 414 loss_train: 219.4939 acc_train: 0.0000 loss_val: 214.6653 acc_val: 0.0000 time: 1.8104s\n",
      "Epoch: 415 loss_train: 217.6098 acc_train: 0.0000 loss_val: 214.2629 acc_val: 0.0000 time: 1.9325s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 416 loss_train: 217.8719 acc_train: 0.0000 loss_val: 213.8882 acc_val: 0.0000 time: 1.8607s\n",
      "Epoch: 417 loss_train: 216.9557 acc_train: 0.0000 loss_val: 213.4088 acc_val: 0.0000 time: 1.7293s\n",
      "Epoch: 418 loss_train: 217.1376 acc_train: 0.0000 loss_val: 212.9779 acc_val: 0.0000 time: 1.9402s\n",
      "Epoch: 419 loss_train: 217.6504 acc_train: 0.0000 loss_val: 212.7615 acc_val: 0.0000 time: 1.9859s\n",
      "Epoch: 420 loss_train: 217.7086 acc_train: 0.0000 loss_val: 212.4994 acc_val: 0.0000 time: 1.9704s\n",
      "Epoch: 421 loss_train: 215.9342 acc_train: 0.0000 loss_val: 212.2685 acc_val: 0.0000 time: 1.9127s\n",
      "Epoch: 422 loss_train: 218.0382 acc_train: 0.0000 loss_val: 212.4765 acc_val: 0.0000 time: 1.8110s\n",
      "Epoch: 423 loss_train: 216.7388 acc_train: 0.0000 loss_val: 212.9423 acc_val: 0.0000 time: 1.8603s\n",
      "Epoch: 424 loss_train: 215.3428 acc_train: 0.0000 loss_val: 212.9619 acc_val: 0.0000 time: 1.8648s\n",
      "Epoch: 425 loss_train: 214.3826 acc_train: 0.0000 loss_val: 212.3537 acc_val: 0.0000 time: 1.9264s\n",
      "Epoch: 426 loss_train: 215.9740 acc_train: 0.0000 loss_val: 211.6720 acc_val: 0.0000 time: 1.7472s\n",
      "Epoch: 427 loss_train: 216.1736 acc_train: 0.0000 loss_val: 211.1196 acc_val: 0.0000 time: 1.7802s\n",
      "Epoch: 428 loss_train: 214.9741 acc_train: 0.0000 loss_val: 210.5755 acc_val: 0.0000 time: 1.9506s\n",
      "Epoch: 429 loss_train: 215.9520 acc_train: 0.0000 loss_val: 210.1982 acc_val: 0.0000 time: 1.7526s\n",
      "Epoch: 430 loss_train: 214.7858 acc_train: 0.0000 loss_val: 211.3696 acc_val: 0.0000 time: 1.7739s\n",
      "Epoch: 431 loss_train: 215.5905 acc_train: 0.0000 loss_val: 212.6971 acc_val: 0.0000 time: 1.9094s\n",
      "Epoch: 432 loss_train: 216.4325 acc_train: 0.0000 loss_val: 213.4667 acc_val: 0.0000 time: 2.0196s\n",
      "Epoch: 433 loss_train: 214.4332 acc_train: 0.0000 loss_val: 213.3953 acc_val: 0.0000 time: 1.9426s\n",
      "Epoch: 434 loss_train: 216.4848 acc_train: 0.0000 loss_val: 212.6900 acc_val: 0.0000 time: 1.8908s\n",
      "Epoch: 435 loss_train: 215.6772 acc_train: 0.0000 loss_val: 211.1871 acc_val: 0.0000 time: 1.8646s\n",
      "Epoch: 436 loss_train: 214.2045 acc_train: 0.0000 loss_val: 209.5473 acc_val: 0.0000 time: 1.8953s\n",
      "Epoch: 437 loss_train: 213.5124 acc_train: 0.0000 loss_val: 204.2576 acc_val: 0.0000 time: 1.7513s\n",
      "Epoch: 438 loss_train: 214.8152 acc_train: 0.0000 loss_val: 178.3907 acc_val: 0.0000 time: 1.7958s\n",
      "Epoch: 439 loss_train: 216.7881 acc_train: 0.0000 loss_val: 222.6526 acc_val: 0.0000 time: 1.8763s\n",
      "Epoch: 440 loss_train: 221.6842 acc_train: 0.0000 loss_val: 231.0993 acc_val: 0.0000 time: 1.9872s\n",
      "Epoch: 441 loss_train: 229.7791 acc_train: 0.0000 loss_val: 234.1712 acc_val: 0.0000 time: 1.9450s\n",
      "Epoch: 442 loss_train: 232.3834 acc_train: 0.0000 loss_val: 235.1539 acc_val: 0.0000 time: 2.0223s\n",
      "Epoch: 443 loss_train: 233.1359 acc_train: 0.0000 loss_val: 235.1841 acc_val: 0.0000 time: 1.8388s\n",
      "Epoch: 444 loss_train: 232.9903 acc_train: 0.0000 loss_val: 234.6988 acc_val: 0.0000 time: 1.8017s\n",
      "Epoch: 445 loss_train: 232.9952 acc_train: 0.0000 loss_val: 233.8790 acc_val: 0.0000 time: 1.7805s\n",
      "Epoch: 446 loss_train: 232.0232 acc_train: 0.0000 loss_val: 232.8169 acc_val: 0.0000 time: 1.8884s\n",
      "Epoch: 447 loss_train: 231.5928 acc_train: 0.0000 loss_val: 231.5333 acc_val: 0.0000 time: 1.9164s\n",
      "Epoch: 448 loss_train: 229.7024 acc_train: 0.0000 loss_val: 229.9540 acc_val: 0.0000 time: 1.8944s\n",
      "Epoch: 449 loss_train: 228.8620 acc_train: 0.0000 loss_val: 228.0187 acc_val: 0.0000 time: 1.9244s\n",
      "Epoch: 450 loss_train: 227.7072 acc_train: 0.0000 loss_val: 225.5581 acc_val: 0.0000 time: 1.8947s\n",
      "Epoch: 451 loss_train: 225.4171 acc_train: 0.0000 loss_val: 222.5204 acc_val: 0.0000 time: 1.7341s\n",
      "Epoch: 452 loss_train: 223.2317 acc_train: 0.0000 loss_val: 219.0242 acc_val: 0.0000 time: 1.8713s\n",
      "Epoch: 453 loss_train: 220.6717 acc_train: 0.0000 loss_val: 215.2664 acc_val: 0.0000 time: 1.8801s\n",
      "Epoch: 454 loss_train: 218.4172 acc_train: 0.0000 loss_val: 212.2212 acc_val: 0.0000 time: 1.9317s\n",
      "Epoch: 455 loss_train: 217.9941 acc_train: 0.0000 loss_val: 211.3963 acc_val: 0.0000 time: 1.8189s\n",
      "Epoch: 456 loss_train: 221.8056 acc_train: 0.0000 loss_val: 211.4362 acc_val: 0.0000 time: 1.7626s\n",
      "Epoch: 457 loss_train: 222.3562 acc_train: 0.0000 loss_val: 212.7387 acc_val: 0.0000 time: 1.9800s\n",
      "Epoch: 458 loss_train: 217.6399 acc_train: 0.0000 loss_val: 214.9114 acc_val: 0.0000 time: 1.8825s\n",
      "Epoch: 459 loss_train: 217.9411 acc_train: 0.0000 loss_val: 216.6999 acc_val: 0.0000 time: 1.8228s\n",
      "Epoch: 460 loss_train: 219.4260 acc_train: 0.0000 loss_val: 217.8118 acc_val: 0.0000 time: 1.7751s\n",
      "Epoch: 461 loss_train: 220.0749 acc_train: 0.0000 loss_val: 218.3127 acc_val: 0.0000 time: 2.0860s\n",
      "Epoch: 462 loss_train: 220.2331 acc_train: 0.0000 loss_val: 218.3507 acc_val: 0.0000 time: 1.8440s\n",
      "Epoch: 463 loss_train: 220.4714 acc_train: 0.0000 loss_val: 218.0857 acc_val: 0.0000 time: 1.9609s\n",
      "Epoch: 464 loss_train: 219.4721 acc_train: 0.0000 loss_val: 217.3802 acc_val: 0.0000 time: 1.8860s\n",
      "Epoch: 465 loss_train: 220.1713 acc_train: 0.0000 loss_val: 216.2968 acc_val: 0.0000 time: 1.8198s\n",
      "Epoch: 466 loss_train: 219.3921 acc_train: 0.0000 loss_val: 214.8231 acc_val: 0.0000 time: 1.7951s\n",
      "Epoch: 467 loss_train: 218.1179 acc_train: 0.0000 loss_val: 213.1964 acc_val: 0.0000 time: 1.8960s\n",
      "Epoch: 468 loss_train: 216.2394 acc_train: 0.0000 loss_val: 211.7363 acc_val: 0.0000 time: 1.8290s\n",
      "Epoch: 469 loss_train: 214.4325 acc_train: 0.0000 loss_val: 210.9397 acc_val: 0.0000 time: 1.7406s\n",
      "Epoch: 470 loss_train: 217.0357 acc_train: 0.0000 loss_val: 210.7265 acc_val: 0.0000 time: 1.8903s\n",
      "Epoch: 471 loss_train: 217.4796 acc_train: 0.0000 loss_val: 210.7525 acc_val: 0.0000 time: 1.8349s\n",
      "Epoch: 472 loss_train: 217.6175 acc_train: 0.0000 loss_val: 212.2605 acc_val: 0.0000 time: 1.9231s\n",
      "Epoch: 473 loss_train: 215.5963 acc_train: 0.0000 loss_val: 214.8059 acc_val: 0.0000 time: 1.9889s\n",
      "Epoch: 474 loss_train: 216.4927 acc_train: 0.0000 loss_val: 217.0016 acc_val: 0.0000 time: 1.9754s\n",
      "Epoch: 475 loss_train: 217.5086 acc_train: 0.0000 loss_val: 218.3242 acc_val: 0.0000 time: 1.7602s\n",
      "Epoch: 476 loss_train: 218.8882 acc_train: 0.0000 loss_val: 218.8776 acc_val: 0.0000 time: 1.7383s\n",
      "Epoch: 477 loss_train: 220.3654 acc_train: 0.0000 loss_val: 218.7707 acc_val: 0.0000 time: 1.8089s\n",
      "Epoch: 478 loss_train: 219.3555 acc_train: 0.0000 loss_val: 218.0780 acc_val: 0.0000 time: 1.7359s\n",
      "Epoch: 479 loss_train: 219.2197 acc_train: 0.0000 loss_val: 216.7726 acc_val: 0.0000 time: 1.7582s\n",
      "Epoch: 480 loss_train: 217.9909 acc_train: 0.0000 loss_val: 215.0965 acc_val: 0.0000 time: 1.7060s\n",
      "Epoch: 481 loss_train: 216.4427 acc_train: 0.0000 loss_val: 213.2190 acc_val: 0.0000 time: 1.9303s\n",
      "Epoch: 482 loss_train: 215.3134 acc_train: 0.0000 loss_val: 211.5168 acc_val: 0.0000 time: 1.9978s\n",
      "Epoch: 483 loss_train: 215.1268 acc_train: 0.0000 loss_val: 210.4982 acc_val: 0.0000 time: 1.9760s\n",
      "Epoch: 484 loss_train: 214.7528 acc_train: 0.0000 loss_val: 209.9612 acc_val: 0.0000 time: 1.8984s\n",
      "Epoch: 485 loss_train: 214.0260 acc_train: 0.0000 loss_val: 208.3672 acc_val: 0.0000 time: 1.9295s\n",
      "Epoch: 486 loss_train: 216.6570 acc_train: 0.0000 loss_val: 207.3960 acc_val: 0.0000 time: 1.8374s\n",
      "Epoch: 487 loss_train: 215.4382 acc_train: 0.0000 loss_val: 208.3535 acc_val: 0.0000 time: 1.7281s\n",
      "Epoch: 488 loss_train: 220.5058 acc_train: 0.0000 loss_val: 210.0485 acc_val: 0.0000 time: 1.8355s\n",
      "Epoch: 489 loss_train: 214.7754 acc_train: 0.0000 loss_val: 211.6673 acc_val: 0.0000 time: 1.9265s\n",
      "Epoch: 490 loss_train: 213.8375 acc_train: 0.0000 loss_val: 213.4490 acc_val: 0.0000 time: 1.8191s\n",
      "Epoch: 491 loss_train: 215.7604 acc_train: 0.0000 loss_val: 214.7017 acc_val: 0.0000 time: 1.7338s\n",
      "Epoch: 492 loss_train: 213.8361 acc_train: 0.0000 loss_val: 215.1875 acc_val: 0.0000 time: 1.8192s\n",
      "Epoch: 493 loss_train: 215.9724 acc_train: 0.0000 loss_val: 214.9870 acc_val: 0.0000 time: 1.9486s\n",
      "Epoch: 494 loss_train: 216.1499 acc_train: 0.0000 loss_val: 214.1687 acc_val: 0.0000 time: 1.8533s\n",
      "Epoch: 495 loss_train: 214.9552 acc_train: 0.0000 loss_val: 212.8174 acc_val: 0.0000 time: 1.8614s\n",
      "Epoch: 496 loss_train: 214.4816 acc_train: 0.0000 loss_val: 211.0404 acc_val: 0.0000 time: 1.8578s\n",
      "Epoch: 497 loss_train: 211.6386 acc_train: 0.0000 loss_val: 209.1610 acc_val: 0.0000 time: 1.7214s\n",
      "Epoch: 498 loss_train: 212.6433 acc_train: 0.0000 loss_val: 206.8508 acc_val: 0.0000 time: 1.8503s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499 loss_train: 220.3937 acc_train: 0.0000 loss_val: 213.1201 acc_val: 0.0000 time: 1.8664s\n",
      "Epoch: 500 loss_train: 213.4197 acc_train: 0.0000 loss_val: 217.9125 acc_val: 0.0000 time: 1.9271s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 906.4544s\n",
      "\n",
      "Test set results: loss= 217.9125 accuracy= 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 500\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 32\n",
    "learning_rate = 1e-2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GCN(features.shape[1], n_hidden_1, n_hidden_2, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output,_ = model(features, adj)\n",
    "    loss_train = loss(output[idx_train].reshape(len(idx_train)), y_train)\n",
    "    acc_train = accuracy_score(torch.argmax(output[idx_train], dim=1).detach().cpu().numpy(), y_train.cpu().numpy())\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    output,_ = model(features, adj)\n",
    "\n",
    "    loss_val = loss(output[idx_test].reshape(len(idx_test)), y_test)\n",
    "    acc_val = accuracy_score(torch.argmax(output[idx_test], dim=1).detach().cpu().numpy(), y_test.cpu().numpy())\n",
    "    print('Epoch: {:03d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output, embeddings = model(features, adj)\n",
    "    loss_test = loss(output[idx_test].reshape(len(idx_test)), y_test)\n",
    "    acc_test = accuracy_score(torch.argmax(output[idx_test], dim=1).detach().cpu().numpy(), y_test.cpu().numpy())\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Train model\n",
    "print('Begin training...')\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "print()\n",
    "\n",
    "# Testing\n",
    "GCN_embeddings = test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20760d8",
   "metadata": {},
   "source": [
    "\n",
    "We will now visualise the hidden states, which are fed into the final layer of the GCN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_embeddings_local = GCN_embeddings.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "my_pca = PCA(n_components=10)\n",
    "my_tsne = TSNE(n_components=2)\n",
    "\n",
    "vecs_pca = my_pca.fit_transform(GCN_embeddings_local)\n",
    "vecs_tsne = my_tsne.fit_transform(vecs_pca)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.title(\"TSNE visualisation of the GCN embeddings\")\n",
    "colours = sns.color_palette(\"hls\", len(np.unique(class_labels)))\n",
    "sns.scatterplot(x=vecs_tsne[:,0], y=vecs_tsne[:,1], hue=class_labels, legend='full', palette=colours)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba521d53",
   "metadata": {},
   "source": [
    "This concludes the taught part of the INF554 course. In the past weeks we had the pleasure to meet a wide range of machine and deep learning models ranging from linear models all the way to graph neural networks. For further context on what we have learned, we want to leave you with a famous quote of George Box:\n",
    "\n",
    "> _\"All models are wrong, but some are useful.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b75df87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'trained_models/model_2.pt')\n",
    "\n",
    "the_model = GCN(features.shape[1], n_hidden_1, n_hidden_2, dropout_rate).to(device)\n",
    "the_model.load_state_dict(torch.load('trained_models/model_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49a44e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000\n",
      "110000\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape[0] + y_test.shape[0])\n",
    "print(features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31d3299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217801,)\n",
      "(174241, 2)\n",
      "(43560, 3)\n",
      "217801\n"
     ]
    }
   ],
   "source": [
    "G2 = nx.read_edgelist('../data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "nodes = np.array(list(G2.nodes))\n",
    "print(nodes.shape)\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_train.shape[0] + df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c09d5cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(135.)\n",
      "tensor([[1.5785],\n",
      "        [4.3889],\n",
      "        [7.9216],\n",
      "        [3.9266],\n",
      "        [7.5771],\n",
      "        [6.0223],\n",
      "        [4.7441],\n",
      "        [3.2470],\n",
      "        [0.8580],\n",
      "        [8.4155]], grad_fn=<SliceBackward0>)\n",
      "[2176915274 1966933645  223937459 1983936918 2107460205 1964615562\n",
      " 2063529815 1232375858 1988909771 1996728517]\n",
      "tensor([8., 8., 2., 3., 2., 7., 4., 7., 7., 3.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output,_ = model(features, adj)\n",
    "t = output[idx_test][-10:]\n",
    "a = nodes[idx_test][-10:]\n",
    "v = y_test[-10:]\n",
    "print(max(y_test))\n",
    "print(t)\n",
    "print(a)\n",
    "print(v)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8070d9ecc5710d547fe2c90daaf36b96eea76154b0ee874b6fff7ce52e7334fe"
  },
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
