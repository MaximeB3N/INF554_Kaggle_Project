{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maximebonnin/Notebooks/3A_Notebook/INF554/word2vec-pytorch/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(Path(os.getcwd()).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import (\n",
    "    get_descritpion,\n",
    "    get_line\n",
    ")\n",
    "\n",
    "from train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/maximebonnin/Notebooks/3A_Notebook/INF554/INF554_Kaggle_Project/data/abstracts.txt\",'r') as f:\n",
    "    abstracts = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [get_line(abstracts[i]) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this paper, we describe a new bitmap indexing technique to cluster XML documents. XML is a new standard for exchanging and representing information on the Internet. Documents can be hierarchically represented by XMLelements. XML documents are represented and indexed using a bitmap indexing technique. We define the similarity and popularity operations available in bitmap indexes and propose a method for partitioning a XML document set. Furthermore, a 2dimensional bitmap index is extended to a 3dimensional bitmap index, called BitCube. We define statistical measurements in the BitCube: mean, mode, standard derivation, and correlation coefficient. Based on these measurements, we also define the slice, project, and dice operations on a BitCube. BitCube can be manipulated efficiently and improves the performance of document retrieval.',\n",
       " 'The paper starts from the observation that in the inclusionbased approach to pointfree geometry there are serious difficulties in defining points. These difficulties disappear once we reformulate this approach in the framework of continuous multivalued logic. So, a theory of ‘graded inclusion’ is proposed as a counterpart of the usual ‘crisp inclusion’ of mereology. Again, a second theory is considered in which the graded predicates ‘to be close’ and ‘to be small’ are assumed as primitive. In both cases a suitable notion of abstractive sequence and of equivalence between abstractive sequences enables us to define the points. In the resulting set of points a distance is defined in a natural way and this enables a metrical approach to pointfree geometry and therefore to go beyond mereotopology.The general idea is that it is possible to search for mathematical formalizations of the naive theory of the space an ordinary man needs to have in its everyday life. To do this we have to direct our attention not only to regions and the related relation of inclusion as it is usual in pointfree geometry, but also to those (vague) properties which are geometrical in nature.',\n",
       " 'This contribution describes an approach to integrate a speech understanding and dialog system into a homogeneous architecture based on semantic networks. The definition of the network as well as its use in speech understanding is described briefly. A scoring function for word hypotheses meeting the requirements of a graph search algorithm is presented. The main steps of the linguistic analysis, i.e. syntax, semantics, and pragmatics, are described and their realization in the semantic network is shown. The processing steps alternating between data and modeldriven phases are outlined using an example sentence which demonstrates a tight interaction between word recognition and linguistic processing.',\n",
       " 'Cleaneval is a shared task and competitive evaluation on the topic of cleaning arbitrary web pages, with the goal of preparing web data for use as a corpus for linguistic and language technology research and development. The first exercise took place in 2007. We describe how it was set up, results, and lessons learnt.',\n",
       " 'Xax is a browser plugin model that enables developers to leverage existing tools, libraries, and entire programs to deliver featurerich applications on the web. Xax employs a novel combination of mechanisms that collectively provide security, OSindependence, performance, and support for legacy code. These mechanisms include memoryisolated native code execution behind a narrow syscall interface, an abstraction layer that provides a consistent binary interface across operating systems, system services via hooks to existing browser mechanisms, and lightweight modifications to existing tool chains and code bases. We demonstrate a variety of applications and libraries from existing code bases, in several languages, produced with various tool chains, running in multiple browsers on multiple operating systems. With roughly two personweeks of effort, we ported 3.3 million lines of code to Xax, including a PDF viewer, a Python interpreter, a speech synthesizer, and an OpenGL pipeline.',\n",
       " 'We believe that there are persuasive arguments for investigating the application of qualitative modelling techniques within medicine1. Although the work reported here is concerned with the specific domain of cardiac electrophysiology, we expect that some of the techniques will be more widely applicable both within medicine and elsewhere.',\n",
       " 'Background:   Glaucoma is an optic neuropathy in which changes in the appearance of both the optic nerve head and the surrounding tissues are important in both diagnosing its presence and progression. Accurate methods to objectively document the appearance of the optic nerve are necessary. The confocal laser scanning ophthalmoscope (Zeiss) is a new prototype instrument that may have the capability to accurately perform this function.    Methods:   The authors performed a prospective pilot study evaluating the ability of the confocal laser scanning ophthalmoscope to reproduce threedimensional optic nerve images. Each retinal image contained 600,000 bytes of information. Thirty discrete images of the right optic nerves of 19 visually normal volunteers were obtained. Depth measurements were compared from the same 100 × 100 μm areas (neighborhoods).    Results:   Image comparisons found the variability of depth measurements for the entire image were within 102 Am (95% confidence interval). Sixty percent of the depth measurements were reproducible within 100,um. Variability of the depth measurements was greatest where the neuroretinal rim sloped at the edge of the optic cup and lowest in the peripapillary area.    Conclusion:   The confocal laser scanning ophthalmoscope has the potential to be a safe, rapid, and reproducible method of imaging ocular structures.',\n",
       " 'We are developing a novel servicebased paradigm for dynamic component integration that facilitates the creation of Intelligent Services from COTS (Commercial Off The Shelf) components, legacy components, and application frameworks by using techniques such as mediation and adaption or “wrapping”. This framework supports the construction of applications by dynamically integrating these services at runtime based on available resources and allows for a federation of services that can evolve over time. As a part of the ongoing research effort we are utilizing an architecture based specification language that enables us to automate the process of creating these intelligent services.',\n",
       " 'XPath is a language for addressing parts of an XML document. It is used in many XML query languages and it can be used by itself for querying XML documents. While XPath is, in general, efficient for querying individual XML documents, it lacks the features for querying over collections of documents or joining parts of the same document. As the amount of complex documentcentric XML data is continually increasing, querying such documents has drawn surprisingly little attention. We propose an XPath axes extension to deal with querying collections of documentcentric XML documents sharing the same content (called concurrent XML). The algorithms we propose to evaluate the extended axes work in linear time combined complexity (number of documents and total size of documents).',\n",
       " 'Business Models play a pivotal role in organizations, especially in building bridges and enabling the dialogue between business and technological worlds. Complementarily, while Use Cases are one of the most popular techniques for eliciting requirements in the design of Information Systems, Business Goals and Business Rules associate with Business Process Use Cases to compose a Business Model base structure. However, methods for relating Business Processes, Goals and Rules (PGR) are scarce, dissonant or highly analystdependent. In this sense, we propose a twostep method to help in guiding the elicitation of Business Goals and Rules from Processlevel Use Cases, and their mapping to a Business Model representation. As a result, a solution Business Model generated by aligning the resulting trios (PGR) with a Business Model Canvas is presented to the organization stakeholders for review, validation and further negotiation.',\n",
       " 'The IEEE 802.16 Worldwide Interoperability for Microwave Access (WiMAX) standard is widely used for fixed and mobile Internet access. WiMAX provides maximum data rate of 75 Mbps and highspeed Internet access to a wide range of devices used by clients over the last mile. Asymmetric Digital Subscriber Line (ADSL) is widely used to provide guaranteed service. In this paper, we compare performance of WiMAX and ADSL by streaming audio and video contents. File Transfer Protocol (FTP), Hyper Text Transfer Protocol (HTTP), and electronic mail have also been used for the comparison. We used OPNET Modeler versions 15.0 and 16.0 to evaluate packet loss, delay, delay jitter, and throughput with various design parameters to determine whether WiMAX exhibits performance comparable to ADSL.',\n",
       " 'Due to demographic changes, European healthcare systems face two serious challenges: healthcare delivery may become inadequate to perceived needs of the citizens or the cost may spiral out of control. With the decrease in the labour force, there is an urgent need to make more health services mobile allowing citizens with chronic diseases stay longer in the labour markets, reduce the number of lost working days and generally support nomadic working. Mobile technologies have the potential to provide better healthcare while at the same time increasing the working population. However, it calls for dramatic changes of healthcare provisioning and of the care models. The REACTION project develops a mobile, cloudbased platform that provides healthcare services to diabetes patients and caregivers. As part of the project, new chronic care models that support separation of care spaces are proposed.',\n",
       " 'Data mining (DM) is the extraction of regularities from raw data, which are further transformed within the wider process of knowledge discovery in databases (KDD) into nontrivial facts intended to support decision making. Formal concept analysis (FCA) offers an appropriate framework for KDD, whereby our focus here is on its potential for DM support. A variety of mining methods powered by FCA have been published and the figures grow steadily, especially in the association rule mining (ARM) field. However, an analysis of current ARM practices suggests the impact of FCA has not reached its limits, i.e., appropriate FCAbased techniques could successfully apply in a larger set of situations. As a first step in the projected FCA expansion, we discuss the existing ARM methods, provide a set of guidelines for the design of novel ones, and list some open algorithmic issues on the FCA side. As an illustration, we propose two online methods computing the minimal generators of a closure system.',\n",
       " 'This paper describes the SmartKomHome system realized within the SmartKom project. It assists the user by means of a multimodal dialogue system in the home environment. This involves the control of various devices and the access to services. SmartKomHome is supposed to serve as a uniform interface to all these devices and services so the user is freed from the necessity to understand which of the devices to consult how and when to fulfill complex wishes. We describe the setting of this scenario together with the hardware used. We furthermore discuss the specific requirements that evolve in a home environment, and how they are handled in the project.',\n",
       " 'We present the application of software process modeling and simulation using an agentbased approach to a real case study of software maintenance. The original process used PSP/TSP; it spent a large amount of time estimating in advance maintenance requests, and needed to be greatly improved. To this purpose, a Kanban system was successfully implemented, that demonstrated to be able to substantially improve the process without giving up PSP/TSP. We customized the simulator and, using input data with the same characteristics of the real ones, we were able to obtain results very similar to that of the processes of the case study, in particular of the original process. We also simulated, using the same input data, the possible application of the Scrum process to the same data, showing results comparable to the Kanban process.',\n",
       " 'Energy dispersive xray diffraction spectra are obtained from numerous volume elements within an object. A feature set such as a set of cepstrum coefficients is extracted from each spectrum and classified by a trained classifier such as a neural network to provide an indication of whether or not contraband such as explosives is present in the volume element. Indications for adjacent volume elements are evaluated in conjunction with one another, as by an erosion process, to suppress isolated indications and thereby suppress false alarms.',\n",
       " 'The Next Steps In Signaling (NSIS) working group has been recently created to design a generic IP signaling protocol supporting various signaling applications. This paper presents the Generic In Signaling Service Protocol (GISP) we designed considering the current outputs of the NSIS working group. In particular, we focus on the state management and message fragmentation using a simple mechanism to detect the path MTU.',\n",
       " 'This paper analyses the merits of the IFM and its software project scheduling algorithms; challenges the claim of efficacy and efficiency of those algorithms made by their authors; indicates alternative solutions to the IFM’s pitfalls; and points towards research directions that could further improve the method.',\n",
       " 'Processbased contextaware applications are increasingly becoming more complex and dynamic. Besides the large sets of process variants to be managed in such dynamic systems, process variants need to be context sensitive in order to accommodate new user requirements and intrinsic complexity. This paradigm shift forces us to defer decisions to runtime where process variants must be customized and executed based on a recognized context. However, there exists a lack of deferral of the entire process variant configuration and execution to perform an automated decision of subsequent variation points at runtime. In this paper, we present a holistic methodology to automatically resolve process variability at runtime. The proposed solution performs a staged configuration considering static and dynamic context data to accomplish effective decision making. We demonstrate our approach by exemplifying a storage operation process in a smart logistics scenario. Our evaluation demonstrates the performance and scalability results of our methodology.',\n",
       " 'The Shared Pathology Informatics Network (SPIN), a research initiative of the National Cancer Institute, will allow for the retrieval of more than 4 million pathology reports and specimens. In this paper, we describe the special query tool as developed for the Indianapolis/Regenstrief SPIN node, integrated into the everexpanding Indiana Network for Patient care (INPC). This query tool allows for the retrieval of deidentified data sets using complex logic, autocoded final diagnoses, and intrinsically supports multiple types of statistical analyses. The new SPIN/INPC database represents a new generation of the Regenstrief Medical Record system – a centralized, but federated system of repositories.',\n",
       " 'This paper presents the systems that we participated with in the Semantic Text Similarity task at SEMEVAL 2012. Based on prior research in semantic similarity and relatedness, we combine various methods in a machine learning framework. The three variations submitted during the task evaluation period ranked number 5, 9 and 14 among the 89 participating systems. Our evaluations show that corpusbased methods display a more robust behavior on the training data, yet combining a variety of methods allows a learning algorithm to achieve a superior decision than that achievable by any of the individual parts.',\n",
       " 'PiQASso is a Question Answering system based on a combination of modern IR techniques and a series of semantic filters for selecting paragraphs containing a justifiable answer. Semantic filtering is based on several NLP tools, including a dependencybased parser, a POS tagger, a NE tagger and a lexical database. Semantic analysis of questions is performed in order to extract key word used in retrieval queries and to detect the expected answer type. Semantic analysis of retrieved paragraphs includes checking the presence of entities of the expected answer type and extracting logical relations between words. A paragraph is considered to justify an answer if similar relations are present in the question. When no answer passes the filters, the process is repeated applying further levels of query expansions in order to increase recall. We discuss results and limitations of the current implementation.',\n",
       " 'The objective of the IMS Learning Design specification is to provide a containment framework of elements that can describe any design of a teachinglearning process in a formal way. The proposed specification is pedagogically neutral and provides a means for defining diverse learning designs (e. g.: collaborative learning, problembased learning, blended learning). Regardless of the pedagogy involved, in practice every learning design comes to: a Method prescribing various Activities for learner and academic staff Actors in a certain order. We agree with this proposal, but we have found some deficiencies and limitations in the way these elements are described. Considering ideas and solutions of the Activity Theory (AT) and the Workflow Management Systems (WFMS) domain we propose alternative structures to describe the Activity and Method parts of this specification.',\n",
       " \"The Web is evolving from a repository for text and images to a provider of services  both informationproviding services, and services that have some effect on the world. Today's Web was designed primarily for human use. To enable reliable, largescale automated interoperation of services by computer programs or agents, the properties, capabilities, interfaces and effects of Web services must be understandable to computers. In this paper we propose a vision and a partial realization of precisely this. We propose markup of Web services in the DAML family of semantic Web markup languages. Our markup of Web services enables a wide variety of agent technologies for automated Web service discovery, execution, composition and interoperation. We present one logicbased agent technology for service composition, predicated on the use of reusable, taskspecific, highlevel generic procedures and userspecific customizing constraints.\",\n",
       " 'This paper describes how to provide an inexpensive capability that automatically captures, timestamps, and georectifies single and 360 degreepanoramic eyelevel imagery while the imagetaker freely moves around the environment. These same images readily overlay all virtual maps as icons to create phototrails that can be queried, sorted, and expanded to help support planning for a variety of commercial, industrial, and military purposes.',\n",
       " 'One of the main challenges in the field of Electronic Health Records (EHRs) is semantic interoperability. To utilise the full potential of interoperable EHR systems they have to be accepted by their users, the health care providers. Good Graphical User Interfaces (GUIs) that support customisation and data validation play a decisive role for user acceptance and data quality. This study investigates the use of openEHR archetypes to automatically generate coherent, customizable, datavalidating GUIs. Using the Mozilla XML User Interface Language (XUL) a series of prototypes has been developed. The results show that the automatic generation of GUIs from openEHR archetypes is feasible in principle. Although XUL revealed some problems, the advantages of XMLbased GUI languages are evident.',\n",
       " 'This article presents the main features of the APIS project that addresses the rapid development of information systems from formal specifications. Information systems are specified using EB, a tracebased formal language. The sequences of input events accepted by the system are described with a process algebra; they represent the valid traces of the information system. Entity types, associations and attributes are described using a class diagram and computed by means of recursive functions defined on the valid traces of the system. In the APIS framework, three tools have been developed. A first tool, called DCIWEB, allows the generation of Web interfaces from GUI specifications. To query and/or to update the system, an enduser can trigger an event through the Web interface. This event is then analyzed by EBPAI, an interpreter for EB process expressions. Finally, the tool EBTG generates, for each EB action, a Java program that executes a relational database transaction. The synthesized transactions implement the specification of the information system’s data structure and are used by the interpreter to update or query the database. The article also brings out the main future developments of the project.',\n",
       " \"Many annotation tasks in computational linguistics are tackled with manually constructed pipelines of algorithms. In realtime tasks where information needs are stated and addressed adhoc, however, manual construction is infeasible. This paper presents an artificial intelligence approach to automatically construct annotation pipelines for given information needs and quality prioritizations. Based on an abstract ontological model, we use partial order planning to select a pipeline's algorithms and informed search to obtain an efficient pipeline schedule. We realized the approach as an expert system on top of Apache UIMA, which offers evidence that pipelines can be constructed adhoc in nearzero time.\",\n",
       " \"We present an architecture and software framework for semantic allies: Semantic systems that complement existing software applications with semantic services and interactions based on a background ontology. On the one hand, our Semantic Alliance framework follows an invasive approach: Users can profit from semantic technology without having to leave their accustomed workflows and tools. On the other hand, Semantic Alliance offers a largely applicationindependent way of extending existing (open API) applications with MKM technologies. Semantic Alliance framework presented in this paper consists of three components: i.) a universal semantic interaction manager for given abstract document types, ii.) a set of thin APIs realized as invasive extensions to particular applications, and iii.) a set of renderer components for existing semantic services. We validate the Semantic Alliance approach by instantiating it with a spreadsheetspecific interaction manager, thin APIs for LibreOffice Calc 3.4 and MS Excel'10, and a browserbased renderer.\",\n",
       " 'It has been widely observed that different NLP applications require different sense granularities in order to best exploit word sense distinctions, and that for many applications WordNet senses are too finegrained. In contrast to previously proposed automatic methods for sense clustering, we formulate sense merging as a supervised learning problem, exploiting humanlabeled sense clusterings as training data. We train a discriminative classifier over a wide variety of features derived from WordNet structure, corpusbased evidence, and evidence from other lexical resources. Our learned similarity measure outperforms previously proposed automatic methods for sense clustering on the task of predicting human sense merging judgments, yielding an absolute Fscore improvement of 4.1% on nouns, 13.6% on verbs, and 4.0% on adjectives. Finally, we propose a model for clustering sense taxonomies using the outputs of our classifier, and we make available several automatically senseclustered WordNets of various sense granularities.',\n",
       " 'We present a framework for penbased, multiuser, online collaboration in mathematical domains. This environment provides participants, who may be in the same room or across the planet, with a shared whiteboard and voice channel. The digital ink stream is transmitted as InkML, allowing special recognizers for different content types, such as mathematics and diagrams. Sessions may be recorded and stored for later playback, analysis or annotation. The framework is currently structured to use the popular Skype and Google Talk services for the communications channel, but other transport mechanisms could be used. The goal of the work is to support computerenhanced distance collaboration, where domainspecific recognizers handle different kinds of digital ink input and editing. The first of these recognizers is for mathematics, which allows converting math input into machineunderstandable format. This supports multiparty collaboration, with sessions recorded in rich formats that allow semantic analysis and manipulation of the content.',\n",
       " 'This paper reports on a research effort involving design of a class of significantly complex products  nuclear submarines. It focuses on the use of features as a means of design abstraction, and it is found that a principal motivation for the use of features in this design environment is the convenience of the early stage submarine designer. To support this argument, a review of feature research is presented. Experiments in the development of feature catalogs are described, and implementation through two generations of feature based submarine CAD systems are discussed. The architecture of the feature based submarine CAD systems includes the use of Microsoft Foundation Classes (MFC), the ACIS geometric modeler, and user interfaces which store/recall hierarchical submarine feature information easily. Strong connections to objectorient ed programming and objectoriented databases are recognized. Conclusions are drawn regarding the use of features for designer convenience and regarding support provided by hierarchical, parameterized features for other means of design automation.',\n",
       " 'Even with intelligent query support data retrieval is not always optimal. This is increasingly true as database systems become more complex and powerful. The Intuitive toolset has been developed for use with multimedia, heterogeneous distributed databases, to support query reformulation before or after submission to the database(s). The toolset is reconfigurable to permit optimisation for novice or expert users and to suit specific domain knowledge. The theoretical background to user search difficulties, possible solutions to these difficulties, mixed initiative dialogue and user adaptive systems is discussed.',\n",
       " 'Environments with frequent changes in application requirements demand an evolutionary approach for database modeling. The challenge is greater when the database must support multiple applications simultaneously. An existing solution for database evolution is refactoring with a transition period. During this period, both the old and the new database schemas coexist and data is replicated in a synchronous process. This solution brings several difficulties, such as interference with the operation of applications. To minimize these difficulties, in this paper we present an asynchronous approach to keep these schemas updated. This paper presents the design for an experimental assessment of this novel approach for evolutionary database development.',\n",
       " 'A system and method that facilitate detection of direct memory access (DMA) corruption is provided. The system can mitigate DMA memory corruption in computer system(s) employing transactionbased DMA bus system(s) (e.g., PCI Express). DMA transaction(s) cannot normally be traced; however, in accordance with an aspect of the present invention, the system is extended to include an interface to specify “allowed” and/or “disallowed” memory range(s) for a DMA transaction. If a DMA transaction occurs in a disallowed range, then it is rejected and, optionally, an error is raised. Thus, the system of the present invention can facilitate detection of direct memory access transaction(s) that can, if permitted, cause memory corruption. The system includes an access information data store (e.g., access table) and a memory controller. The access information can include, for example, a source identifier, a memory range (e.g., one or more contiguous memory address(es)) and access attribute(s) (e.g., read access, read and write access, write access, no access permitted, etc.).',\n",
       " 'Computer implemented method, system and computer program product for creating a resume that facilitates generating and updating multiple versions of the resume. A computer implemented method for creating a resume includes storing resume information. Rules are specified by which a resume for a particular job application is to be created, and the stored resume information is parsed according to the specified rules to provide specified resume information. A resume is then created for the particular job application from the specified resume information.',\n",
       " 'BACKGROUND    \\r Fieldbased activities are regarded as essential to the development of a range of professional and personal skills for undergraduate students within geography, earth and environmental sciences. Students are taught investigative skills to enable them to interpret features within the landscape, establish technical skills such as sketching and the use of field equipment, and learn to collaborate with peers. Students enjoy field activities, and these improve deeper learning and understanding. However, due to issues such as cost and access some have little opportunity to participate in fieldbased studies. The ERA (Enabling Remote Activity) project is investigating how mobile and communication technologies might enhance field learning experiences for all participants. We identify two ways in which supporting technologies can enable greater participation and add value to existing fieldwork: remote access and collaborative groupwork.      METHODS    \\r In 2006 we enabled a single mobility impaired student to direct a remote geologist in the field, supporting remote access. A rapidly deployable, lightweight, battery powered wireless network was built (which we refer to as an ‘onthe fly network’) to enable the transmission of video, audio, and high resolution still images from the field to the student. In 2007 we supported three groups of volunteers undertaking remote collaboration, with half the participants in a university laboratory and the others in the field location. Each group was carrying out a separate specific geological investigation; graphic logging, paleontology, or mineralogy and paleocurrents. A network infrastructure supported communication and data transmission between the groups. Field and laboratory participants had their own distinct, significant roles and the trials explored how technology enhanced collaboration may be used to improve student learning.      CONTRIBUTION    \\r ERA has tested highly mobile, easily configurable low cost network tools to explore how onthefly networking can support geology field studies at undergraduate level in remote locations. We have explored two differing configurations, developed through a collaborative design process undertaken between technology developers and course managers.      EVALUATION AND REFLECTION    \\r A range of evaluation tools were used to enable analysis of the trials. Field journals were kept by all participants, which found ready acceptance with the geologists as an extension of their standard practice of keeping field notes. A wiki was used by the technical team to capture lessons learnt during the development and trial periods. Participants were gathered together for posttrial debrief sessions. In the second trial, participants’ responses were collected through written questionnaires and focus group discussions (audio recorded). Participants’ activities were also captured on video camera and this was analysed to capture critical incidents. Key findings underline the importance of codesigning technology and pedagogy, orchestration of multiple groups, onsite testing, and planning for graceful degradation of technologies and learning activities.     \\r In 2008 we will be looking to move the system from a development prototype to a production model that could be could be replicated by geology departments across the UK without intensive technical support, and the proving of specific technical enhancements including VOIP (Voice Over Internet Protocol) communication and the use of wireless digital cameras.',\n",
       " 'In this paper, we present the results from an experiment designed to compare two selection gestures (hand grab and screen tap) for the Leap Motion controller in 2D pointing tasks. We used the ISO 92419 multidirectional tapping test for comparing the devices, and we analyze the results using standard throughput and error rate measures as well as additional accuracy measures. We also present the results from the ISO 92419 assessment of comfort questionnaire. To complement this analysis, the computer mouse was also evaluated in order to serve as a comparison. Results indicate that the hand grab gesture performs better than the screen tap. The Leap Motion (LM) controller is a 3D sensing device for hand gesture interaction. The LM is a small device that plugs to the computer via USB (it is also sold embedded in the HP ENVY Leap Motion Notebook PC and in the HP Leap Motion keyboard) and is operated by positioning the hands over the device. Through stereoscopic computer vision techniques, it is able to determine the position and orientation of the fingers of the hands, as well as the palm orientation and curvature. The controller can be used to point to a computer screen with a finger or with a tool (a pen or pencil, for example), or perform various hand gestures. Although not meant to be a replacement of the mouse, many of the interactions with the LM involve pointing at a computer screen. There are situations where users would want or need to perform typical Windows, Icons, Menus, Pointer (WIMP) tasks with the LM, such as selecting buttons, navigating through menus and options or dragging graphical objects. Many applications in the Leap App Store are meant to give users various degrees of control over the computer, from selection and launching predefined applications and settings to scrolling content on webpages.',\n",
       " 'Multiple datasets that add high value to biomedical research have been exposed on the web as a part of the Life Sciences Linked Open Data (LSLOD) Cloud. The ability to easily navigate through these datasets is crucial for personalized medicine and the improvement of drug discovery process. However, navigating these multiple datasets is not trivial as most of these are only available as isolated SPARQL endpoints with very little vocabulary reuse. The content that is indexed through these endpoints is scarce, making the indexed dataset opaque for users. In this paper, we propose an approach for the creation of an active Linked Life Sciences Data Roadmap, a set of congurable rules which can be used to discover links (roads) between biological entities (cities) in the LSLOD cloud. We have catalogued and linked concepts and properties from 137 public SPARQL endpoints. Our Roadmap is primarily used to dynamically assemble queries retrieving data from multiple SPARQL endpoints simultaneously. We also demonstrate its use in conjunction with other tools for selective SPARQL querying, semantic annotation of experimental datasets and the visualization of the LSLOD cloud. We have evaluated the performance of our approach in terms of the time taken and entity capture. Our approach, if generalized to encompass other domains, can be used for roadmapping the entire LOD cloud.',\n",
       " 'A time period can be detected for which a user associated with calendar system entries is to be absent. A set of the calendar system entries associated with the user and occurring during the time period can be detected. At least a portion of the identified calendar entries can be reassigned to at least one delegate. For each reassigned calendar entry, a corresponding calendar entry can be added to a calendar of the associated delegate.',\n",
       " 'A physical synthesis flow for data path circuits is proposed that integrates bitslice tiling and gate size selection in traditional P&R tool, which outperforms traditional P&R tool in power and timing. This approach also reduces effort and multiple iterations of manual flow.',\n",
       " 'Nowadays, not only CPU but also GPU goes along the trend of  processors. Parallel processing presents not only an opportunity but also a challenge at the same time. To explicitly parallelize the software by programmers or compilers is the key for enhancing the performance on  chip. In this paper, we first introduce some of the automatic parallel tools based OpenMP, which could save the time to rewrite codes for parallel processing on multicore system. Then we focus on ROSE and explore it in depth. And we also implement an interface to reduce its complexity of use and use some automatic parallelization for CUDA.',\n",
       " 'The work presented in this dissertation was based on two main premises. First, the final documents of the facility consisting of drawings, specifications and other paper reports contain very little or no knowledge of planning, design and other decisions (and their rationale) that went into making the facility the way it is. The lack of whys and hows along with what of the solution results in a number of problems during the life cycle of the facility. Second, computer technology can be beneficially utilized for managing the project data and knowledge along with the intent and rationale of the related agents during the various phases of the facility life cycle. The study refers to the currently uncaptured knowledge as \"projectspecific knowledge (PSK)\" and formally defines it as the rationale behind the project data and specifications, including design decisions that link elements of basic data, design data, project specifications, domain knowledge, and general knowledge to explain, derive and justify the design.\\r The research method for this investigation was exploratory and inductive in nature and consisted of five case studies. The major research activities were the identification and analysis of documented project design decisions (with associated rationale or justifications). The research was based on a priori theory of PSK (Howard 91). The predominant mode of data analysis was similar to constant comparison method suggested by Glaser and Strauss (Glaser 67) for developing a grounded theory. The categories discovered from the analysis of casestudy data formed the elements of PSK framework: design issue, design object, design influences, design concerns, design options, design operation, decision logic, and decision outcome.\\r The overall goal of this investigation was attained by the development and validation of the PSK framework that can be transformed into a computational model. Specifically, the investigation resulted in: (1) characterization of PSK; (2) formalization of its potential uses in the design process, in downstream project activities, across the facility lifecycle, and beyond the current project; and (3) identification of the potential and existing barriers to its effective acquisition.',\n",
       " 'Efficient applications of expert systems to project risk management problems are seldom, if not unusual. In this paper we overcome this lack by using the probabilistic expert system shell SPIRIT. The rulebased shell ’s power in conditioning, inference and reasoning under incomplete information will work well on risk estimation and classification. A key characteristic of SPIRIT is the possibility to integrate project objectives into the risk management model. So known dependencies between risk variables can be modelled by the user if known beforehand, whereas hidden dependencies might be detected by the proper system. Because of the novelty of projects they suffer from incomplete information and it is this incompleteness which SPIRIT handles at high information fidelity. Furthermore undirected inference is possible, due to the undirected graphical structure in which knowledge is acquired and processed. So, in an earlystate risk management situation  where the final model in terms of certain variables and/or their respective dependencies is not yet available  preliminary risk analyses and even recommendations for adequate risk treatment measures are possible, too. A middle size product developement example, including 12 binary variables and 34 rules, shows the inferential power of SPIRIT.',\n",
       " 'In this paper we investigate how timeless ontologies such as DFault, an ontology for fault diagnosis in power transmission networks can be reengineered to include temporal entities. We propose a methodology, FONTE (Factorising ONTology Engineering complexity), that allows this complex process to be factored by dividing the problem into parts: modelling the domain concepts ontology (atemporal and aspatial), modelling or acquiring the temporal and /or spatial ontology, and finally producing the target ontology by assembling these modular ontologies via a semiautomated process.',\n",
       " 'This study has tried to suggest a new model that can effectively redistribute the tickets in the online ticket resale market, while suggesting a new allocation mechanism based on an agent negotiation. To this end, this study has analyzed and simulated the secondary ticket market through System dynamics. As a result of these simulations, it has been proved that the price stability of ticket resale market leads to an increase in revenue. An agent negotiation helps to stabilize the ticket prices that are usually inclined to rise at auction, benefiting all the participants in the negotiations, consequently showing a Pareto solution.',\n",
       " \"An important aim in bilateral negotiations is to achieve a winwin solution for both parties; therefore, a critical aspect of a negotiating agent's success is its ability to take the opponent's preferences into account. Every year, new negotiation agents are introduced with better learning techniques to model the opponent. Our main goal in this work is to evaluate and compare the performance of a selection of stateoftheart online opponent modeling techniques in negotiation, and to determine under which circumstances they are beneficial in a realtime, online negotiation setting. Towards this end, we provide an overview of the factors influencing the quality of a model and we analyze how the performance of opponent models depends on the negotiation setting. This results in better insight into the performance of opponent models, and allows us to pinpoint wellperforming opponent modeling techniques that did not receive much previous attention in literature.\",\n",
       " 'This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: “learn quickly while losing, slowly while winning.” The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.',\n",
       " 'Service Oriented Architecture (SOA) has become a promising paradigm for software development due to its benefits in costefficiency, agility, and adaptability. One of the most important research topics in SOA is to identify appropriate web services for a service composition so as to fulfill business requirements. Services selection focuses on selecting best services among a bunch of services with same or similar functions but having different QoS (Quality of Service). Many previous approaches have been proposed to optimize service selection. However, in current research, quality values normally come from service providers, who have high possibility to exaggerate these data for advertisement. In addition, we also argue that reputation based on an average user rating is not enough to indicate the trust degree of web services and service provider. In this paper, we focus on solving these problems. First we describe a QoS brokerbased service selection scenario. Based on the scenario, we propose a Trust model, which includes QoS model, decision model and trust correction. We apply a statistical method of Paired ttest to model trust correction. By experiments, we verify that web service capacity and trust corrections have significant effects on web service selection. We also verify that the proposed approach is better than traditional approaches in terms of consumer satisfaction degree on selected web services.',\n",
       " 'The objective of this chapter is to describe building an eScience infrastructure suitable for use with environmental sciences and especially with hydrological science applications. The infrastructure allows a wide range of hydrological problems to be investigated and is particularly suitable for either computationally intensive or multiple scenario applications. To accomplish this objective, this research discovered the shortcomings of current grid infrastructures for hydrological science and developed missing components to fill this gap. In particular, there were three primary areas which needed work: first, integrating data and computing grids; second, visualization of geographic information from grid outputs; and third, implementing hydrological simulations based on this infrastructure. This chapter focuses on the first area, which is focusing on grid infrastructure system integration and development. A grid infrastructure, which consists of a computing and a data grid, has been built. In addition, the computing grid has been extended to utilize the Amazon EC2 cloud computing resources. Users can conduct a complete simulation job life cycle from job submission, and data management to metadata management based on the tools available in the infrastructure.',\n",
       " '1 This research work was funded, in part, by an MRI grant from the National Science Foundation (Grant #: CNS – 0619069). ABSTRACT Code injections can aid successful intrusion attempts, thereby allowing viruses and worms to spread. Current research into intrusion detection is notably focused on application behavior profiling through system call trace analysis. Studying the system call layer has been identified as a potential approach to render revealing details about an application’s behavior. System call sequences available from the execution trace of an application can be subjected to different modeling techniques to approximate the application’s normal execution. This research views application programs as dynamical systems, and applies dynamical system analysis tools operating on time series data, merely the system calls made by an application, to identify the degree of determinism in a dynamical system. There is some prior work in the literature analyzing programs as dynamical systems, but they lack proper utilization of dynamical system formalisms and associated analysis tools. In our research we utilize a set of dynamical system analysis tools composed of Approximate Entropy, Central Tendency Measure, and Recurrence Plot derived measures. Our initial results are promising in detecting code injections.',\n",
       " \"Often libraries try to assess digital library service for their user populations in comprehensive terms that judge its overall success or failure. This article's key assumption is that the people involved must be understood before services can be assessed, especially if evaluators and developers intend to improve a digital library product. Its argument is simply that anthropology can provide the initial understanding, the intellectual basis, on which informed choices about sample population, survey design, or focus group selection can reasonably be made. As an example, this article analyzes the National Gallery of the Spoken Word (NGSW). It includes brief descriptions of nine NGSW microcultures and three pairs of dichotomies within these microcultures.\",\n",
       " 'UML models are widely used by software developers to model complex software systems. Testing the models can reveal aws in the early stages of software development. Criteria based on UML class diagrams and interaction diagrams were proposed earlier to specify UML model elements that need to be covered during testing. This paper describes a case study that evaluated the fault detection effectiveness of these criteria.',\n",
       " 'Techniques for performing peer discovery to enable peertopeer (P2P) communication are disclosed. In an aspect, a proximity detection signal used for peer discovery may be generated based on one or more physical channels and/or signals used in a wireless network. In one design, a user equipment (UE) may generate a proximity detection signal occupying at least one resource block based on a SCFDMA modulation technique. In another design, the UE may generate a proximity detection signal occupying at least one resource block based on an OFDMA modulation technique. The UE may generate SCFDMA symbols or OFDMA symbols in different manners for different physical channels. In yet another design, the UE may generate a proximity detection signal including a primary synchronization signal and a secondary synchronization signal. For all designs, the UE may transmit the proximity detection signal to indicate its presence and to enable other UEs to detect the UE.',\n",
       " 'In this paper we propose a novel approach for contentbased music recommendation. The main innovation of the proposed technique consists of a similarity function that, instead of considering entire songs or their thumbnail representations, analyzes audio similarities between semantic segments from different audio tracks. The rationale of our idea is that a song similarity and recommendation technique, to be more meaningful to the user from a semantic point of view, may evaluate and exploit similarities on semantic units between audio tracks. Our similarity algorithm consists of two main stages: the first step performs segmentation of the song in semantic parts. The latter assigns a similarity and recommendation score to a pair of songs, by computing the distance between the representations of their segments. To assign the global similarity and recommendation score, we consider a consistent subset of all the intersegment distances. By adopting a graphbases framework, we propose a graphreduction algorithm on weighted edges that connect segments of different songs to optimize the similarity score with respect to our recommendation goal. Experiments conducted on a database of 200 audio tracks of various authors and genres show promising results.',\n",
       " 'Biomedical domain and proteomics in particular are faced with an increasing volume of data. The heterogeneity of data sources implies heterogeneity in the representation and in the content of data. Data may also be incorrect, implicate errors and can compromise the analysis of experiments results. Our approach aims to ensure the initial quality of data during import into an information system dedicated to proteomics. It is based on the joint use of models, which represent the system sources, and ontologies, which are use as mediators between them. The controls, we propose, ensure the validity of values, semantics and data consistency during import process.',\n",
       " 'This paper describes an approach to developing a universal tool for eliciting, from a nonexpert human user, knowledge about any language L. The purpose of this elicitation is rapid development of NLP systems. The approach is described on the example of the syntax module of the Boas knowledge elicitation system for a quick ramp up of a standard transferbased machine translation system from L into English. The preparation of knowledge for the MT system is carried out into two stages; the acquisition of descriptive knowledge about L and using the descriptive knowledge to derive operational knowledge for the system. Boas guides the acquisition process using datadriven, expectationdriven and goaldriven methodologies.',\n",
       " 'This article describes an integrated approach for the handling and modeling of knowledge for assembly processes in an automated production environment. Highlevel information on the process provided by the user needs to be transferred into executable code. This is informal information and might be even available in natural language. With the help of several information sources like CAD or sensor data, device and skill descriptions, planning algorithms, process knowledge and finally domain knowledge the Knowledge Integration Framework derives a formal application description. Therefore, the user input is parsed and analyzed with all the relevant knowledge made available in a formal representation beforehand. With the help of computerbased reasoning and inference algorithms the input is evaluated and enhancements for missing information are requested from the user. The core part of the implementation is to capture and to enable access to the available knowledge. Without a proper representation of all process and operation details the Knowledge Integration Framework cannot perform this challenging work. The formal application description is fed into the code generator where devicespecific code together with the execution sequence is created. On runtime these executable programs  after deployment to the physical devices  are iteratively readapted taking strategies for safety and error recovery into account. This work promises a strong potential for realizing future robot installations in a robust and efficient way, taking into account a dynamic environment and human robot cooperation with respect to safe task execution and human injury prevention.',\n",
       " 'Agentoriented software engineering has become an extended practice. The autonomy and flexibility provided by agents makes it appropriate either for the development of agentbased systems and for the development of complex and distributed software systems. Due to this wide applicability there are many modelling languages and methodologies for representing and developing systems using the  paradigm. Among them, the i* language is very appropriate, not only for the agent concepts that it models, but also for its capabilities in the disciplines of requirements engineering and organizational process modelling. Nowadays, i* is one of the most widespread notations used for these purposes, and agentoriented methodologies may take advantage of its existence in the requirements phase. However, due to the degree of freedom inherent to the i* language, the construction of the models may not be an easy task. To give light to this subject, we present several i*  modelling techniques and we compare them according to a certain set of criteria.',\n",
       " 'Problem statement: Computer networks are expanding at very fast rate and the number of network users is increasing day by day, for full utilization of networks it need to be secured against many threats including malware, which is harmful software with the capability to damage data and systems. Fuzzy rule based classification systems considered as an  active research area  in recent years, due to their unique capability of classifying. Approach: This study presents a neural fuzzy classifier based on Adaptive NeuroFuzzy Inference System (ANFIS) for malware detection. Firstly, the malware exe files was analyzed and the most important API calls were selected and used as training and testing datasets, using the training data set the ANFIS classifier learned how to detect the malware in the test dataset. Results and Conclusion: The performances of the Neuro fuzzy classifier were evaluated based on the performance of training and accuracy of classification, the results show that the proposed Neuro fuzzy classifier can detect the malware exe files effectively.',\n",
       " 'In this paper we present a lightweight interprocedural sideeffect analysis on assembly code. We represent the modifying potential of a procedure f by classifying all write accesses, occurring within f, relative to the parameter registers. In particular our approach is the first to accurately handle reference parameters. We demonstrate the usefulness of this approach by integrating this analysis into our assembly analyser and provide an evaluation of the precision of our approach. Approximately 50 per cent of all procedures can be statically shown to have sideeffects.',\n",
       " 'Although research on wearable computing is in fashion, few researches pay attentions on interface of wearable computers. This paper presents \"HandMenu\" System\", an innovative input interface suitable for wearable computers, utilizing augmented realty techniques. Users can handle wearable computers by clicking a menu activated simply activated with exposed hand with another hand. As HandMenu is a \"Deviceless\" hand I/O interface, HandMenu is instant, placeindependent and intuitive interface. Additionally, solidity on virtual menu given by tactile feedback from real finger used as a clicking target promotes users\\' reliability of HandMenu.',\n",
       " 'In this paper, we propose three algorithms LCMfreq, LCM, and LCMmax for mining all frequent sets, frequent closed item sets, and maximal frequent sets, respectively, from transaction databases. The main theoretical contribution is that we construct treeshaped transversal routes composed of only frequent closed item sets, which is induced by a parentchild relationship defined on frequent closed item sets. By traversing the route in a depthfirst manner, LCM finds all frequent closed item sets in polynomial time per item set, without storing previously obtained closed item sets in memory. Moreover, we introduce several algorithmic techniques using the sparse and dense structures of input data. Algorithms for enumerating all frequent item sets and maximal frequent item sets are obtained from LCM as its variants. By computational experiments on real world and synthetic databases to compare their performance to the previous algorithms, we found that our algorithms are fast on large real world datasets with natural distributions such as KDDcup2000 datasets, and many other synthetic databases.',\n",
       " 'In the vision of an IoT, trillions of tiny devices extend the Internet to the physical world and enable novel applications that have not been possible before. Such applications emerge out of the interaction of these devices with each other and with more powerful serverclass computers on the Internet. Programming such applications is challenging due to the massively distributed nature of these networks combined with the challenges of embedded programming. In addition, resource constraints, device heterogeneity, and the integration with the Internet further complicate this situation. In this paper, we present a programminginthelarge approach for resourceconstraint devices such as wireless sensor nodes. Our approach is to model such applications using the Business Process Execution Language (BPEL), which is successfully and widely used in the Internet to model complete applications and business processes. However, BPEL and its associated technologies are too resourcedemanding to be directly applied in resourceconstraint environments. We therefore use the BPEL model as input to a code generation process that generates customtailored, lean code for different target platforms. The resulting code is fully standardcompliant and allows a seamless integration of IoT devices in enterprise IT environments. We present an exhaustive evaluation on real hardware showing the firstrate performance of the approach.',\n",
       " \"Keywords: Rendering model, image quality, subjective assessment, spatial frequencies, sCIECAM. Abstract: With the development and the multiplicity of imaging devices, the color quality and portability have become a very challenging problem. Moreover, a color is perceived with regards to its environment. In order to take into account the variation of perceptual vision in function of environment, the CIE (Commission Internationale de l'eclairage) has standardized a tool named color appearance model (CIECAM97*, CIECAM02). These models are able to take into account many phenomena related to human vision of color and can predict the color of a stimulus, function of its observations conditions. However, these models do not deal with the influence of spatial frequencies which can have a big impact on our perception. In this paper, an extended version of the CIECAM02 was presented. This new version integrates a spatial model correcting the color in relation to its spatial frequency and its environment. Moreover, a study on the influence of the background’s chromaticity has been also performed. The obtained results are sound and demonstrate the efficiency of the proposed extension.\",\n",
       " 'This paper is a case study of the effectiveness of componentoriented development for enhancing both productivity and performance for parallel programs. A process for converting monolithic applications into semantically composable components is described. The supporting software, the PCOM2 compositional compiler, is briefly described. The componentized version of Sweep3D is described. Productivity is illustrated by composing different instances of the Sweep3D code through automated composition of components using PCOM2. These instances, each of which targets improving performance for some execution environment or problem case, are examples of a family of instances which are composable from a modest set of components. It is found that customization of componentized codes by componentlevel adaptation may yield substantial performance improvement for specific execution environments. We identify and explain some of the benefits of componentoriented development for highperformance parallel systems. Copyright © 2006 John Wiley & Sons, Ltd.',\n",
       " 'Modern architectures are becoming more heterogeneous. OpenMP currently has no mechanism for assigning work to specific parts of these heterogeneous architectures.We propose a combination of thread mapping and subteams as a means to give programmers control over how work is allocated on these architectures. Experiments with a prototype implementation on the Cell Broadband Engine show the benefit of allowing OpenMP teams to be created across the different elements of a heterogeneous architecture.',\n",
       " 'Most association rule mining techniques concentrate on finding frequent rules. However, rare association rules are in some cases more interesting than frequent association rules since rare rules represent unexpected or unknown associations. All current algorithms for rare association rule mining use an Apriori levelwise approach which has computationally expensive candidate generation and pruning steps. We propose RPTree, a method for mining a subset of rare association rules using a tree structure, and an information gain component that helps to identify the more interesting association rules. Empirical evaluation using a range of real world datasets shows that RPTree itemset and rule generation is more time efficient than modified versions of FPGrowth and ARIMA, and discovers 92100% of all the interesting rare association rules.',\n",
       " 'Methods for managing large scale sequencing projects are available through the use of our GAP4 package and the applications to which it can link are described. This main assembly and editing program, also provides a graphical user interface to the assembly engines: CAP3, FAKII, and PHRAP. Because of the diversity of working practices in the large number of laboratories where the package is used, these methods are very flexible and are readily tailored to suit local needs. For example, the Sanger Centre in the UK and the Whitehead Institute in the United States have both made major contributions to the human genome project using the package in different ways. The manual for the current (2001.0) version of the package is over 500 pages when printed, so this chapter is a brief overview of some of its most important components. We have tried to show a logical route through the methods in the package: preprocessing, assembly, contig1 ordering using readpairs, contig joining using sequence comparison, assembly checking, automated experiment suggestions for extending contigs and solving problems, and ending with editing and consensus file generation. Before this overview, two important aspects of the package are outlined: the file formats used, the displays and the powerful user interface of GAP4. The package runs on UNIX and Microsoft Windows platforms and is entirely free to academic users, and can be downloaded from Website: http://www.mrclmb.cam.ac.uk/pubseq.',\n",
       " 'This paper describes an algorithm whereby an initial, naive user query to a search engine can be subsequently refined to improve both its recall and precision. This is achieved by manually classifying the documents retrieved by the original query into relevant and irrelevant categories, and then finding additional Boolean terms which successfully discriminate between these categories. Latent semantic analysis is used to weight the choice of these extra search terms to make the resulting queries more intuitive to users.',\n",
       " 'Since the introduction of the ubiquitous computing vision by Mark Weiser, we observed quite some research from various R&D projects as well as groups that focused on different aspects in the area of contextawareness. However, there is yet no result which we can easily apply in a real environment. The contextaware service development and deployment process seldom focused on reusability and realistic implementation issues. We believe these issues can be tackled by proposing a complete contextaware system package. In this package both developers and users will be supported. This idea is realised at our department as a contextaware playground. This paper explains the observation on current work and the shortcomings, and proposes how the contextaware playground approach can be a solution.',\n",
       " 'In this paper we introduce the general architecture of an image search engine based on preattentive similarities. The components of this system are presented and some of them are discussed. Our approach is based on the extraction of interest points using a multiresolution technique. The spatial distribution of these points is embeded in bidimensional histograms of classic features (the first five differential invariants). A userbased relevance feedback is introduced in order to overcome the limitations of any preattentive similarities based scheme.',\n",
       " 'One of the objectives of computational Grids is to offer applications the collective computational power of distributed but typically shared heterogeneous resources. Unfortunately, efficiently harnessing the performance potential of such systems (i.e. how and where applications should execute on the Grid) is a challenging endeavor due principally to the very distributed, shared and heterogeneous nature of the resources involved. A crucial step towards solving this problem is the need to identify both an appropriate scheduling model and scheduling algorithm(s). This paper presents a tool to aid the design and evaluation of scheduling policies suitable for efficient execution of systemaware parallel applications on computational Grids. Copyright © 2005 John Wiley & Sons, Ltd.',\n",
       " 'The application of formal methods especially, model checking and static analysis techniques for the verification of safety critical embedded systems has produced very good results and raised the interest of system designers up to the application of these technologies in real size projects. However, these methods usually rely on specific verification oriented formal languages that most designers do not master. It is thus mandatory to embed the associated tools in automated verification toolchains that allow designers to rely on their usual domainspecific modeling languages DSMLs while enjoying the benefits of these powerful methods. More precisely, we propose a language to formally express system requirements and interpret verification results so that system designers DSML endusers avoid the burden of learning some formal verification technologies. Formal verification is achieved through translational semantics. This work is based on a metamodeling pattern for executable DSML that favors the definition of generative tools and thus eases the integration of tools for new DSMLs.',\n",
       " 'Drahtlose Netzwerke nach dem Wireless LAN Standard IEEE 802.11 sind heute in immer groseren Bereichen im Einsatz. Trotz der vorhandenen HandoverFahigkeit zeigt sich, dass in derartigen Netzwerken eine zuverlassige Lastverteilung auf erreichbare BasisStationen gerade fur echtzeitbasierte Anwendungen entscheidende Vorteile hat. Im Rahmen dieses Artikels wird ein Verfahren zum aktiven  in Wireless LAN vorgestellt. Neben der Erlauterung des zu Grunde liegenden Protokolls zeigen erste Messungen die dadurch erreichbare Effizienz. Den Abschluss bildet ein Ausblick uber weitere Untersuchungen auf diesem Gebiet. 1 Wireless LAN und LoadBalancing Wireless LAN nach dem IEEE 802.11 Standard ist nicht zuletzt auf Grund der heute verfugbaren Erweiterungen IEEE 802.11b, g und a in unterschiedlichsten Auspragungen verfugbar. So werden haufig gerade Firmengebaude, Hochschulen und offentliche, stark frequentierte Bereiche, wie Flughafen und Bahnhofe mit einem drahtlosen Netzwerk ausgestattet. Auf Grund geringer Reichweiten einzelner WLANZellen (Access Point AP), findet oftmals eine so genannte multizellulare Architektur Anwendung, in der ein Handover zwischen den einzelnen Zellen durch entsprechende Protokolle im Standard geregelt wurde. Der Algorithmus des Roamings im WLAN, nach dem Standard IEEE 802.11, beginnt mit der Suche benachbarter APs in Reichweite des Clients sobald das SignalRauschVerhaltnis zum assoziierten AP einen bestimmten Grenzwert unterschreitet. Existieren mehrere APs, wahlt der Client einen dieser aus und sendet diesem ein REASSOCIATIONRequest. Ist der ausgewahlte AP in der Lage den neuen Client zu verwalten, sendet er eine REASSOCIATIONResponse zuruck um den Vorgang abzuschliesen. Ab diesem Zeitpunkt ist der Client mit dem neuen AP assoziiert[IE98]. Die Zeit dieses Prozesses wird als Handoveroder RoamingZeit bezeichnet. Diese Zeit ist grundsatzlich abhangig von Herstellern sowohl der Clients, als auch der APs[Ju04]. Da fur den den Einsatz von VoiceoverIP eine Latenz von nicht mehr als 50 ms im WorstCase empfohlen ist, kann dies jedoch zu Problemen fuhren[Ju04]. Der Begriff LoadBalancing wird als Algorithmus zur ausgewogenen Verteilung von Last auf Knoten (APs) des Systems innerhalb einer multizellularen Organisationsstruktur, mit sich uberlappenden Bereichen, verwendet. Die Last eines Wireless LANs nach IEEE 802.11 ist jedoch nicht nur von der Anzahl aktiver Clients pro Zelle, wie in Netzwerken mit fester Kanalzuweisung [FZ02], abhangig, son',\n",
       " 'One of the elementary tasks of an autonomous mobile robot is the execution of different behavior patterns in order to fulfill a given task. The complexity of this problem is especially high if the robot operates in a dynamic, unpredictable environment and requires the parallel control of multiple actuators. In this paper we present a novel architecture for robust and fast mobile robot control. The architecture is based on TeleoReactive Programs. We discuss the benefits and drawbacks of such programs, extend the basic definition for the parallel control of multiple actuators, and propose a new language and a compiler for extended TeleoReactive Programs. These tools simplify the creation of new behavior patterns and increase the runtime performance. Finally, we discuss implementation issues of the architecture when applying it to RoboCup MiddleSize soccer robots.',\n",
       " 'Semantic web technologies show great promise in usage scenarios that involve information logistics. This paper is an experience report on improving the semantic web ontology underlying an application used in expert finding. We use ontology design patterns to find and correct poor design choices, and align the application ontology to commonly used semantic web ontologies in order to increase the interoperability of the ontology and application. Lessons learned and problems faced are discussed, and possible future developments of the project mapped out.',\n",
       " 'The literature suggests that businessIT alignment is an important antecedent of IS success, business process performance, and competitive advantage. Additionally, IT governance practices are highlighted as being instrumental to fostering businessIT alignment. In this paper, we derive various IT governance practices (in terms of structures, processes, relational mechanisms, and enterprise architecture characteristics) from literature and expert interviews. While prior investigations only considered the effect of such practices on strategic businessIT alignment, we also incorporate alignment at operational level. Using results from a case study in the IT services division of a large multinational, multidivisional company acting in diverse industries we highlight the effect of various IT governance practices and offer new insights by showing which mechanisms are effective in facilitating strategic or operational businessIT alignment. Our results indicate the most important practices for both strategic and operational alignment.',\n",
       " 'This paper describes the evaluation of the system for selective dissemination of Broadcast News that we developed in the context of the European project ALERT. Each component of the main processing block of our system was evaluated separately, using the ALERT corpus. Likewise, the user interface was also evaluated separately. Besides this modular evaluation which will be briefly mentioned here, as a reference, the system can also be evaluated as a whole, in a field trial from the point of view of a potential user. This is the main topic of this paper. The analysis of the main sources of problems hinted at a large number of issues that must be dealt with in order to improve the performance. In spite of these pending problems, we believe that having a fully operational system is a must for being able to address user needs in the future in this type of service.',\n",
       " 'Enriching answer set programming with function symbols makes modeling easier, increases the expressive power, and allows us to deal with infinite domains. However, this comes at a cost: common inference tasks become undecidable. To cope with this issue, recent research has focused on finding tradeoffs between expressivity and decidability by identifying classes of logic programs that impose limitations on the use of function symbols but guarantee decidability of common inference tasks. Despite the significant body of work in this area, current approaches do not include many simple practical programs whose evaluation terminates. In this paper, we present the novel class of rulebounded programs. While current techniques perform a limited analysis of how terms are propagated from an individual argument to another, our technique is able to perform a more global analysis, thereby overcoming several limitations of current approaches. We also present a further class of cyclebounded programs where groups of rules are analyzed together. We show different results on the correctness and the expressivity of the proposed techniques.',\n",
       " 'PURPOSE. A series of 205 retinoblastoma (RB) patients referred to the Department of Ophthalmology at the University of Siena (Italy) was evaluated in order to assess the proportion of unilateral cases later developing tumors in the companion eye (metachronous bilateral retinobastoma) (MBRB). METHODS. The total number of unilaterally affected patients developing tumors in the fellow eye was recorded and the risk factors assessed for the development of asynchronous bilateral retinoblastoma, i.e., family history, tumor multifocality and early age at diagnosis. RESULTS. Only two out of 133 (1.5%) unilateral retinoblastoma patients in our series could be considered affected by MBRB. CONCLUSIONS. The incidence of MBRB in our series was negligible (1.5% of all unilateral cases) compared to other reports. None of the reported risk factors for the development of tumors in the fellow eye was relevant in the present series. Although close followup of some unilateral cases is still recommended, thorough examination of the fellow eye, to search for lesions in the peripheral retina, is essential in all cases of unilateral RB. MBRB may be a distinctive clinical entity with specific clinical, genetic and prognostic features. However, all these aspects need to be better investigated in larger series.',\n",
       " 'We exploit the Justification Logic capabilities of reasoning about justifications, comparing pieces of evidence, and measuring the complexity of justifications in the context of argumentative agents. The research can be integrated into the larger context of integrating logic and argumentation. The paper introduces distributed justification logic $\\\\mathcal{DJL}$ as an extension of justification logic for multiagent systems, and it also investigates the expressivity of $\\\\mathcal{DJL}$ for argumentative agents. Not knowing all of the implications of their knowledge base, agents use justified arguments for reflection and guidance.',\n",
       " 'The Lack of written representation for Italian Sign Language (LIS) makes it difficult to do perform tasks like looking up a new word in a dictionary. Most of the paper dictionaries show LIS signs in drawings or pictures. It’s not a simple proposition to understand the meaning of sign from paper dictionaries unless one already knows the meanings. This paper presents the LIS dictionary which provides the facility to translate Italian text into sign language. LIS signs are shown as video animations performed by a virtual character. The LIS dictionary provides the integration with MultiWordNet database. The integration with MultiWordNet allows a rich extension with the meanings and senses of the words existing in MultiWordNet. The dictionary allows users to acquire information about lemmas, synonyms and synsets in the Sign Language (SL). The application is platform independent and can be used on any operating system. The results of input lemmas are displayed in groups of grammatical categories.',\n",
       " 'An active set is a unifying space being able to act as a \"bridge\" for transferring information, ideas and results between distinct types of uncertainties and different types of applications. An active set is a set of agents who independently deliver true or false values for a given proposition. An active set is not a simple vector of logic values for different propositions, the results are a vector but the set is not. The difference between an ordinary set and active set is that the ordinary set has passive elements with values of the attributes defined by an external agent, in the active set any element is an agent that internally defines the value of a given attribute for a passive element. In this paper we show the connection between classical, fuzzy, evidence theory and active sets. In conclusion at one external agent we substitute a set of experts or agents that evaluate in a conflicting way the logic value of a given proposition or attribute. Under the same meta level of active sets we can discover analogy and similarity among distinct theories of uncertainty and types of many valued logics.',\n",
       " 'Dieser Artikel beschreibt, wie mit Hilfe von Standardprozessen ein Softwarepraktikum im Hauptstudium an der Universitat Ulm gestaltet wurde. Dieses Praktikum sollte nicht nur Entwicklungstatigkeiten schulen, sondern insbesondere auch auf begleitende Tatigkeiten wie Konfigurationsund Qualitatsmanagement eingehen. Der Leser erhalt einen kurzen Uberblick uber den Aufbau des Praktikums, die Durchfuhrung im SS2000 und einen Einblick in die dabei gewonnenen Erfahrungen.',\n",
       " 'Even though the Dublin Core Metadata Element Set is well accepted as a general solution, it fails to describe more complex information assets and their crosscorrelation. These include data from political history, history of arts and sciences, archaeology or observational data from natural history or geosciences. Therefore IFLA and ICOM are merging their core ontologies, an important step towards semantic interoperability of metadata schemata across all archives, libraries and museums. It opens new prospects for advanced global information integration services. The first draft of the combined model was published in June 2006.',\n",
       " 'There is a common situation among current distance bounding protocols in the literature: they set the fast bit exchange phase after a slow phase in which the nonces for both the reader and a tag are exchanged. The output computed in the slow phase is acting as the responses in the subsequent fast phase. Due to the calculation constrained RFID environment of being lightweight and efficient, it is the important objective of building the protocol which can have fewer number of message flows and less number of cryptographic operations in real time performed by the tag. In this paper, we propose a new highly efficient mutuallyauthenticated RFID distance bounding protocol that enables precomputation which is carried out offline by the tag. There is no evaluation on any PRF during the real time protocol running which makes the tag significantly more efficient at a lowcost. The protocol requires only O(1) complexity for achieving tag privacy. In addition, we give a detailed security analysis to prove that our protocol is secure against all common attacks in distance bounding.',\n",
       " 'Most algorithms for learning and pattern discovery in data assume that all the needed data is available on one computer at a single site. This assumption does not hold in situations where a number of independent databases reside on geographically distributed nodes of a computer network. These databases cannot be moved to a single site due to size, security, privacy and dataownership concerns but all of them together constitute the dataset in which patterns must be discovered. Some pattern discovery algorithms can be adapted to such situations and some others become inefficient or inapplicable. In this paper we show how a decisiontree induction algorithm may be adapted for distributed data situations. We also discuss some general issues relating to the adaptability of other pattern discovery algorithms to distributed data situations',\n",
       " 'In the current way for construction cost estimating by using application software in China, the generation of bill of quantity usually needs plenty of manual work, because the design results cannot be used directly. BIM (Building Information Modeling) technology has provided a potential approach to generating bill of quantity more efficiently by utilizing the computable and reusable information which is available from the 3D design results. However, while several commercial application software have already existed in foreign countries, no BIMbased software in this aspect has been developed in China. In addition, the existing BIMbased application software cannot be directly used in China because they do not support Chinese standards. In this paper, through analyzing the related Chinese standard, a discrimination model for bill of quantity (short for BQ) items was established and the corresponding rule database and semantic database were created. Then the IFCbased discrimination model for BQ items was established and the mechanism of intelligent generation of bill of quantity from IFC data was formulated. Finally the mechanism was programmed and verified through an actual project. As a conclusion, the established mechanism can not only be applied to the development of BIMbased construction cost estimating application software subject to Chinese standard, but also can be referred to develop IFCbased construction cost estimating application software for different countries.',\n",
       " 'An image forming apparatus comprising an exposure light source, a printer head which comprises a group of microshutters each controlling the transmission or interruption of light from the exposure light source, and an image bearing so disposed to be irradiated with light signals transmitted through the printer head. The group of microshutters are arranged in a matrix comprising a plurality of rows and a plurality of columns. The group of microshutters are formed by a substrate having thereon a plurality of segment electrodes each forming a shutter and connected to a drain of a thin film transistor, another substrate having thereon a common electrode, and a liquid crystal interposed between the substrates. The image forming apparatus further comprises means for applying a scanning signal to the gate of the thin film transistor and applying an electric signal corresponding to image information in synchronism with the scanning signal.',\n",
       " 'An article in Science magazine (Bhattacharjee, 2007) discussed how the U.S. military is interested in enlisting the help of multidisciplinary scientific experts to better understand “how local populations behave in a war zone.” The article mentioned the “Human Social Culture Behavior Modeling” (HSBC) program at U.S. Department of Defense and indicated, through a few anecdotal examples, the types of prior research emanating from multidisciplinary fields that may be considered the state of the art.',\n",
       " 'Jstacs is an objectoriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classifiers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples.',\n",
       " 'A secure remote assistance session between computers that are behind firewalls and/or NAT devices is provided by an arrangement that uses a terminal services (“TS”) gateway to enable utilization of a remote desktop protocol (“RDP”) connection by a terminal services client in a reverse direction to that used in a conventional terminal services session. The connection is made via a regular TS gateway protocol mechanism by which the TS client behind a firewall establishes a connection to the remote server that is typically behind a firewall that protects a corporate network. The server then functions as the terminal services client to tunnel RDP data through the established TS gateway connection through the NAT firewall to a client. Thus, the server and client reverse roles after the TS gateway connection is made to thereby enable remote viewing of the graphical user interface that is displayed by the client in support of the remote assistance session.',\n",
       " 'Nondestructive structural testing aims to characterize material and detect failures without damaging the structure in any way. Detection historically meant some form of a visual assessment combined with auditory tests carried out by trained personnel. Inspectors well prepared to identify various types of deterioration would conduct periodic evaluations comparing the current state with previous reports. The problem with this was not only that most likely damage locations had to be known a priori and had to be readily accessible but also that the whole approach had a strong subjective aspect to it. This latter issue was somewhat alleviated with the introduction of more advanced inspection methods and instruments, such as Xray and ultrasonic techniques, but the main problem of inspections being cumbersome, slow, and superficial still remained untouched.',\n",
       " 'Process Discovery techniques help a business analyst to understand the actual processes deployed in an organization, i.e. based on a log of events, the actual activity workflow is discovered. In most cases their results conform to general purpose representations like Petri nets or Causal nets which are preferred by academic scholars but difficult to comprehend for business analysts. In this paper we propose an algorithm that follows a topdown approach to directly mine a process model which consists of common BPdomain constructs and represents the main behaviour of the process. The algorithm is designed so it can deal with noise and notsupported behaviour. This is achieved by letting the different supported constructs compete with each other for the most suitable solution from top to bottom using ”soft” constraints and behaviour approximations. The key parts of the algorithm are formally described and evaluation results are presented and discussed.',\n",
       " 'We propose an anthropomorphization framework that determines an object’s body image. This framework directly intervenes and anthropomorphizes objects in ubiquitouscomputing environments through robotic body parts shaped like those of human beings, which provide information through spoken directions and body language. Our purpose is to demonstrate that an object acquires subjective representations through anthropomorphization. Using this framework, people can more fully understand instructions given by an object. We designed an anthropomorphization framework that changes the body image by attaching body parts. We also conducted experiments to evaluate this framework. Results indicate that the site at which an anthropomorphization device is attached influences human perception of the object’s virtual body image, and participants in experiments understood several instructions given by the object more clearly. Results also indicate that participants better intuited their devices’ instructions and movement in ubiquitouscomputing environments.',\n",
       " 'There is a growing need for parametric design software that communicates building performance feedback in early architectural exploration to support decisionmaking. This paper examines how the circuit of design and analysis process can be closed to provide active and concurrent feedback between architecture and services engineering domains. It presents the structure for an openly customisable design system that couples parametric modelling and energy analysis software to allow designers to assess the performance of early design iterations quickly. Finally, it discusses how user interactions with the system foster information exchanges that facilitate the sharing of design intelligence across disciplines.',\n",
       " 'In recent years, the demand of multimedia data in wireless sensor networks has been significantly increased for the environment monitoring applications that utilize sensor nodes to collect multimedia data such as sound and video. However, the amount of multimedia data is very large. Therefore, if the data transmission schemes in traditional wireless sensor networks are applied in wireless multimedia sensor networks, the network lifetime is significantly reduced due to excessive energy consumption on particular nodes. In this paper, we propose a novel energyefficient data compression scheme for multimedia data transmission in wireless sensor networks. The proposed scheme compresses and splits the multimedia data using the Chinese Remainder Theorem (CRT) and transmits the bitpattern packets of the remainder to the base station. As a result, it can reduce the amount of the transmitted multimedia data. To show the superiority of our proposed scheme, we compare it with the existing scheme. Our experimental results show that our proposed scheme reduces about 71 % in the amount of transmitted data and increases about 188 % in the ratio of surviving nodes over the existing scheme on average.',\n",
       " 'In this paper, we address the lexicon design problem in Turkish large vocabulary speech recognition. Although we focus only on Turkish, the methods described here are general enough that they can be considered for other agglutinative languages like Finnish, Korean etc. In an agglutinative language, several words can be created from a single root word using a rich collection of morphological rules. So, a virtually infinite size lexicon is required to cover the language if words are used as the basic units. The standard approach to this problem is to discover a number of primitive units so that a large set of words can be created by compounding those units. Two broad classes of methods are available for splitting words into their subunits; morphologybased and datadriven methods. Although the word splitting significantly reduces the out of vocabulary rate, it shrinks the context and increases acoustic confusibility. We have used two methods to address the latter. In one method, we use word counts to avoid splitting of high frequency lexical units, and in the other method, we recompound splits according to a probabilistic measure. We present experimental results that show the methods are very effective to lower the word error rate at the expense of lexicon size.',\n",
       " 'One of the essential factors that affect the performance of Artificial Neural Networks is the learning algorithm. The performance of Multilayer Feed Forward Artificial Neural Network performance in image compression using different learning algorithms is examined in this paper. Based on Gradient Descent, Conjugate Gradient, QuasiNewton techniques three different error back propagation algorithms have been developed for use in training two types of neural networks, a single hidden layer network and three hidden layers network. The essence of this study is to investigate the most efficient and effective training methods for use in image compression and its subsequent applications. The obtained results show that the QuasiNewton based algorithm\\r has better performance as compared to the other two algorithms.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.48M/4.48M [00:01<00:00, 2.92MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "t = WikiText2(root='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = t[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for batch in data_iter:\n",
    "    print(type(batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets(abstracts[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import (\n",
    "    get_english_tokenizer,\n",
    "    get_data_iterator,\n",
    "    build_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = get_data_iterator(\"Own\",\"train\",\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_english_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab()\n"
     ]
    }
   ],
   "source": [
    "voc = build_vocab(data_iter, tokenizer)\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 29\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Epoch: 1/30, Train Loss=3.22225, Val Loss=2.67733\n",
      "Adjusting learning rate of group 0 to 2.4167e-02.\n",
      "Epoch: 2/30, Train Loss=2.52584, Val Loss=2.07005\n",
      "Adjusting learning rate of group 0 to 2.3333e-02.\n",
      "Epoch: 3/30, Train Loss=1.97733, Val Loss=1.88527\n",
      "Adjusting learning rate of group 0 to 2.2500e-02.\n",
      "Epoch: 4/30, Train Loss=1.81215, Val Loss=1.87471\n",
      "Adjusting learning rate of group 0 to 2.1667e-02.\n",
      "Epoch: 5/30, Train Loss=1.92039, Val Loss=1.97338\n",
      "Adjusting learning rate of group 0 to 2.0833e-02.\n",
      "Epoch: 6/30, Train Loss=1.93198, Val Loss=1.87125\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Epoch: 7/30, Train Loss=1.78257, Val Loss=1.85872\n",
      "Adjusting learning rate of group 0 to 1.9167e-02.\n",
      "Epoch: 8/30, Train Loss=1.76553, Val Loss=1.79827\n",
      "Adjusting learning rate of group 0 to 1.8333e-02.\n",
      "Epoch: 9/30, Train Loss=1.80628, Val Loss=1.77007\n",
      "Adjusting learning rate of group 0 to 1.7500e-02.\n",
      "Epoch: 10/30, Train Loss=1.78799, Val Loss=1.79490\n",
      "Adjusting learning rate of group 0 to 1.6667e-02.\n",
      "Epoch: 11/30, Train Loss=1.80623, Val Loss=1.78359\n",
      "Adjusting learning rate of group 0 to 1.5833e-02.\n",
      "Epoch: 12/30, Train Loss=1.70923, Val Loss=1.76921\n",
      "Adjusting learning rate of group 0 to 1.5000e-02.\n",
      "Epoch: 13/30, Train Loss=1.81033, Val Loss=1.84198\n",
      "Adjusting learning rate of group 0 to 1.4167e-02.\n",
      "Epoch: 14/30, Train Loss=1.83489, Val Loss=1.80952\n",
      "Adjusting learning rate of group 0 to 1.3333e-02.\n",
      "Epoch: 15/30, Train Loss=1.82464, Val Loss=1.71822\n",
      "Adjusting learning rate of group 0 to 1.2500e-02.\n",
      "Epoch: 16/30, Train Loss=1.84594, Val Loss=1.79059\n",
      "Adjusting learning rate of group 0 to 1.1667e-02.\n",
      "Epoch: 17/30, Train Loss=1.76239, Val Loss=1.79114\n",
      "Adjusting learning rate of group 0 to 1.0833e-02.\n",
      "Epoch: 18/30, Train Loss=1.77541, Val Loss=1.81883\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch: 19/30, Train Loss=1.78540, Val Loss=1.77122\n",
      "Adjusting learning rate of group 0 to 9.1667e-03.\n",
      "Epoch: 20/30, Train Loss=1.66355, Val Loss=1.81158\n",
      "Adjusting learning rate of group 0 to 8.3333e-03.\n",
      "Epoch: 21/30, Train Loss=1.81843, Val Loss=1.75532\n",
      "Adjusting learning rate of group 0 to 7.5000e-03.\n",
      "Epoch: 22/30, Train Loss=1.80252, Val Loss=1.79780\n",
      "Adjusting learning rate of group 0 to 6.6667e-03.\n",
      "Epoch: 23/30, Train Loss=1.81045, Val Loss=1.79107\n",
      "Adjusting learning rate of group 0 to 5.8333e-03.\n",
      "Epoch: 24/30, Train Loss=1.81080, Val Loss=1.78915\n",
      "Adjusting learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: 25/30, Train Loss=1.82840, Val Loss=1.83739\n",
      "Adjusting learning rate of group 0 to 4.1667e-03.\n",
      "Epoch: 26/30, Train Loss=1.81558, Val Loss=1.75811\n",
      "Adjusting learning rate of group 0 to 3.3333e-03.\n",
      "Epoch: 27/30, Train Loss=1.74370, Val Loss=1.79997\n",
      "Adjusting learning rate of group 0 to 2.5000e-03.\n",
      "Epoch: 28/30, Train Loss=1.73449, Val Loss=1.80618\n",
      "Adjusting learning rate of group 0 to 1.6667e-03.\n",
      "Epoch: 29/30, Train Loss=1.79453, Val Loss=1.80590\n",
      "Adjusting learning rate of group 0 to 8.3333e-04.\n",
      "Epoch: 30/30, Train Loss=1.75599, Val Loss=1.80357\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n",
      "Training finished.\n",
      "Model artifacts saved to folder: weights/cbow_own\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\", 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
