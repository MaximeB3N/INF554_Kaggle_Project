{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Global embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory : /users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project/Global, Project directory : /users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model as LinearModels\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "from scipy import stats\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "NathanPath=\"d:\\Documents\\Info\\INF554\\INF554_Kaggle_Project\"\n",
    "NathanPath=\"/users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project\"\n",
    "\n",
    "project_path = str(Path(os.getcwd()).parent.absolute())\n",
    "print(\"Current directory : \" + os.getcwd() + \", Project directory : \" + project_path)\n",
    "\n",
    "os.chdir(project_path)\n",
    "os.chdir(NathanPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=np.load(\"Global/paper_vectors.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_abstracts=vectors[:,0].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624181\n",
      "2908511069\n",
      "3603\n"
     ]
    }
   ],
   "source": [
    "print(len(id_abstracts))\n",
    "print(np.max(id_abstracts))\n",
    "print(np.min(id_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_id_abstracts=dict([(a,b) for a,b in enumerate(id_abstracts)])\n",
    "id_abstracts_num=dict([(b,a) for a,b in enumerate(id_abstracts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624181"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_abstracts_num.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/author_papers.txt\") as f:\n",
    "    authors_papers=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=vectors.shape[1]-1\n",
    "authors_vectors=np.zeros((len(authors_papers), n_dim+1), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1510273386', '1827736641', '1588673897', '2252711322', '2123653597']\n",
      "1510273386\n",
      "58046\n"
     ]
    }
   ],
   "source": [
    "papers=authors_papers[0].split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "print(papers)\n",
    "p=papers[0]\n",
    "print(int(p))\n",
    "print(id_abstracts_num.get(int(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880\n"
     ]
    }
   ],
   "source": [
    "s=0\n",
    "for i,author in enumerate(authors_papers):\n",
    "    papers=author.split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "    vector=np.zeros(n_dim)\n",
    "    no_fail=False\n",
    "    for p in papers:\n",
    "        try:\n",
    "            vector+=vectors[id_abstracts_num[int(p)], 1:]\n",
    "            no_fail=True\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if (not no_fail):\n",
    "        s+=1\n",
    "    authors_vectors[i][0]=int(author.split(\":\")[0])\n",
    "    if (np.linalg.norm(vector)>0):\n",
    "        vector=vector/np.linalg.norm(vector)\n",
    "    authors_vectors[i][1:]=vector.copy()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Global/authors_vectors.npy\", authors_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_improved=np.load(\"DeepWalk/embeddings_improved.npy\")\n",
    "authors_vectors=np.load(\"Global/authors_vectors.npy\")\n",
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217801, 135)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_improved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217801, 135)\n",
      "(217801, 151)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_improved.shape)\n",
    "print(authors_vectors.shape)\n",
    "\n",
    "auth_vec_num_id_author=dict([(a,b) for a,b in enumerate(authors_vectors[:,0])])\n",
    "id_author_auth_vec_num=dict([(b,a) for a,b in enumerate(authors_vectors[:,0])])\n",
    "\n",
    "graph_num_id_author=dict([(a,b) for a,b in enumerate(G.nodes)])\n",
    "id_author_graph_num=dict([(b,a) for a,b in enumerate(G.nodes)])\n",
    "\n",
    "emb_num_id_author=dict([(a,int(b)) for a,b in enumerate(embeddings_improved[:,0])])\n",
    "id_author_emb_num=dict([(int(b),a) for a,b in enumerate(embeddings_improved[:,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=G.number_of_nodes()\n",
    "n_emb=embeddings_improved.shape[1]-1\n",
    "n_abs=authors_vectors.shape[1]-1\n",
    "n_dim_tot=1+n_emb+n_abs\n",
    "full_matrix=np.zeros((n_nodes, n_dim_tot), dtype=np.float64)\n",
    "for i in range(n_nodes):\n",
    "    node=graph_num_id_author[i]\n",
    "    full_matrix[i,0]=node\n",
    "    full_matrix[i,1:1+n_emb]=embeddings_improved[id_author_emb_num[node],1:].copy()\n",
    "    full_matrix[i,1+n_emb:]=authors_vectors[id_author_auth_vec_num[node],1:].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Global/full_embedding_matrix.npy\", full_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217801, 285)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP model\"\"\"\n",
    "    def __init__(self, n_feat, n_hidden_1, n_hidden_2, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z0 = self.relu(self.fc1(x))\n",
    "        z0 = self.dropout(z0)\n",
    "        z1 = self.relu(self.fc2(z0))\n",
    "        out = self.fc3(z1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X,Y):\n",
    "    if (len(X)!=len(Y)):\n",
    "        print(\"Sizes not identical\")\n",
    "        return -1\n",
    "    return (X-Y)@(X-Y) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'author': np.int64})\n",
    "full_embedding=np.load(\"Global/full_embedding_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_nodeID_Train=dict(df_train[\"author\"])\n",
    "nodeID_abs_Train=dict([(b,a) for a,b in abs_nodeID_Train.items()])\n",
    "\n",
    "abs_nodeID_Test=dict(df_test[\"author\"])\n",
    "nodeID_abs_Test=dict([(b,a) for a,b in abs_nodeID_Test.items()])\n",
    "\n",
    "abs_hindex_Train=dict(df_train[\"hindex\"])\n",
    "\n",
    "abs_nodeID_Graph=dict(enumerate(G.nodes))\n",
    "nodeID_abs_Graph=dict([(b,a) for a,b in enumerate(G.nodes)])\n",
    "\n",
    "n=G.number_of_nodes()\n",
    "n_train=abs_nodeID_Train.__len__()\n",
    "n_test=abs_nodeID_Test.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Careful, those indexes are related to the TRAIN set, not to the global graph indexing\n",
    "idx=np.random.permutation(n_train)\n",
    "idx_train=idx[:int(0.8*n_train)]\n",
    "idx_val=idx[int(0.8*n_train):]\n",
    "\n",
    "nodes_train=[abs_nodeID_Train[i] for i in idx_train]\n",
    "nodes_val=[abs_nodeID_Train[i] for i in idx_val]\n",
    "\n",
    "X_train_x = torch.tensor([full_embedding[nodeID_abs_Graph[node]][1:] for node in nodes_train], dtype=torch.float32)\n",
    "X_val_x = torch.tensor([full_embedding[nodeID_abs_Graph[node]][1:] for node in nodes_val], dtype=torch.float32)\n",
    "\n",
    "hindex_train_x=torch.tensor([abs_hindex_Train[i] for i in idx_train], dtype=torch.float32)\n",
    "hindex_val_x=torch.tensor([abs_hindex_Train[i] for i in idx_val], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Training on split set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=X_train_x.shape[1]\n",
    "model=MLP(n_dim,256,64,0.3)\n",
    "\n",
    "loss_vals=[]\n",
    "loss_trains=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 137.6450 loss_val: 905.9951\n",
      "Epoch: 002 loss_train: 1071.9847 loss_val: 165.9547\n",
      "Epoch: 003 loss_train: 170.5118 loss_val: 315.3433\n",
      "Epoch: 004 loss_train: 334.6509 loss_val: 438.5247\n",
      "Epoch: 005 loss_train: 460.5245 loss_val: 329.2988\n",
      "Epoch: 006 loss_train: 345.9984 loss_val: 206.5145\n",
      "Epoch: 007 loss_train: 216.5356 loss_val: 160.1002\n",
      "Epoch: 008 loss_train: 166.6315 loss_val: 180.7333\n",
      "Epoch: 009 loss_train: 185.9707 loss_val: 222.8044\n",
      "Epoch: 010 loss_train: 227.9804 loss_val: 250.3248\n",
      "Epoch: 011 loss_train: 256.1060 loss_val: 251.6022\n",
      "Epoch: 012 loss_train: 257.0326 loss_val: 233.2712\n",
      "Epoch: 013 loss_train: 238.6973 loss_val: 208.2743\n",
      "Epoch: 014 loss_train: 213.6866 loss_val: 187.3928\n",
      "Epoch: 015 loss_train: 192.7582 loss_val: 175.9686\n",
      "Epoch: 016 loss_train: 181.7449 loss_val: 174.2970\n",
      "Epoch: 017 loss_train: 180.5564 loss_val: 179.3531\n",
      "Epoch: 018 loss_train: 186.1823 loss_val: 186.8495\n",
      "Epoch: 019 loss_train: 193.9069 loss_val: 192.9894\n",
      "Epoch: 020 loss_train: 200.7965 loss_val: 195.5065\n",
      "Epoch: 021 loss_train: 203.4594 loss_val: 193.9918\n",
      "Epoch: 022 loss_train: 202.1641 loss_val: 189.4787\n",
      "Epoch: 023 loss_train: 197.2197 loss_val: 183.6753\n",
      "Epoch: 024 loss_train: 191.1110 loss_val: 178.2514\n",
      "Epoch: 025 loss_train: 185.2958 loss_val: 174.3300\n",
      "Epoch: 026 loss_train: 181.1550 loss_val: 172.3250\n",
      "Epoch: 027 loss_train: 178.6781 loss_val: 172.0095\n",
      "Epoch: 028 loss_train: 178.1138 loss_val: 172.6892\n",
      "Epoch: 029 loss_train: 178.8335 loss_val: 173.4687\n",
      "Epoch: 030 loss_train: 179.8029 loss_val: 173.9949\n",
      "Epoch: 031 loss_train: 179.9940 loss_val: 173.3551\n",
      "Epoch: 032 loss_train: 179.6798 loss_val: 171.4687\n",
      "Epoch: 033 loss_train: 178.2716 loss_val: 168.5832\n",
      "Epoch: 034 loss_train: 175.1914 loss_val: 165.3149\n",
      "Epoch: 035 loss_train: 171.8543 loss_val: 161.5182\n",
      "Epoch: 036 loss_train: 167.6163 loss_val: 157.4553\n",
      "Epoch: 037 loss_train: 165.0554 loss_val: 152.9194\n",
      "Epoch: 038 loss_train: 160.1600 loss_val: 148.3464\n",
      "Epoch: 039 loss_train: 154.0471 loss_val: 151.1039\n",
      "Epoch: 040 loss_train: 279.7176 loss_val: 146.0700\n",
      "Epoch: 041 loss_train: 152.4315 loss_val: 147.7362\n",
      "Epoch: 042 loss_train: 154.4572 loss_val: 147.6299\n",
      "Epoch: 043 loss_train: 157.8715 loss_val: 147.9154\n",
      "Epoch: 044 loss_train: 221.2046 loss_val: 149.8093\n",
      "Epoch: 045 loss_train: 162.0562 loss_val: 152.3117\n",
      "Epoch: 046 loss_train: 158.2999 loss_val: 154.3187\n",
      "Epoch: 047 loss_train: 160.2138 loss_val: 155.5066\n",
      "Epoch: 048 loss_train: 161.0694 loss_val: 155.6672\n",
      "Epoch: 049 loss_train: 161.3947 loss_val: 154.9031\n",
      "Epoch: 050 loss_train: 160.6000 loss_val: 153.6581\n",
      "Epoch: 051 loss_train: 159.4855 loss_val: 152.3334\n",
      "Epoch: 052 loss_train: 159.4851 loss_val: 151.9199\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "lr=2e-2\n",
    "for i in range(5):\n",
    "    lr/=2\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_x)\n",
    "        loss_train = loss(output.reshape(-1), hindex_train_x)\n",
    "        loss_trains.append(loss_train.item())\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        output= model(X_val_x)\n",
    "\n",
    "        loss_val = loss(output.reshape(-1), hindex_val_x)\n",
    "        loss_vals.append(loss_val.item())\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "                'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                'loss_val: {:.4f}'.format(loss_val.item()))\n",
    "        if (epoch>100 and loss_val.item()>loss_train.item()*1.1):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f142cb8ae80>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_vals[20:])\n",
    "plt.plot(loss_trains[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Global/256_64_1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_embedding_glob=np.load(\"Global/full_embedding_matrix.npy\")\n",
    "idx=range(n_train)\n",
    "nodes_train=[abs_nodeID_Train[i] for i in idx]\n",
    "X_train_glob = torch.tensor([full_embedding_glob[nodeID_abs_Graph[node]][1:] for node in nodes_train], dtype=torch.float32)\n",
    "hindex_train_glob=torch.tensor([abs_hindex_Train[i] for i in idx], dtype=torch.float32)\n",
    "\n",
    "X_test_glob = torch.tensor([full_embedding_glob[nodeID_abs_Graph[node]][1:] for node in nodeID_abs_Test.keys()], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 23380.0215\n",
      "Epoch: 002 loss_train: 3457031.2500\n",
      "Epoch: 003 loss_train: 257840.1719\n",
      "Epoch: 004 loss_train: 211251.9375\n",
      "Epoch: 005 loss_train: 347498.1562\n",
      "Epoch: 006 loss_train: 223528.3438\n",
      "Epoch: 007 loss_train: 104738.3750\n",
      "Epoch: 008 loss_train: 35493.9219\n",
      "Epoch: 009 loss_train: 5250.1182\n",
      "Epoch: 010 loss_train: 6597.1719\n",
      "Epoch: 011 loss_train: 17167.1465\n",
      "Epoch: 012 loss_train: 18752.0449\n",
      "Epoch: 013 loss_train: 17678.8047\n",
      "Epoch: 014 loss_train: 13926.9795\n",
      "Epoch: 015 loss_train: 10780.8926\n",
      "Epoch: 016 loss_train: 7624.3662\n",
      "Epoch: 017 loss_train: 5409.1680\n",
      "Epoch: 018 loss_train: 3903.7239\n",
      "Epoch: 019 loss_train: 2462.0950\n",
      "Epoch: 020 loss_train: 1715.0781\n",
      "Epoch: 021 loss_train: 1064.9261\n",
      "Epoch: 022 loss_train: 721.5031\n",
      "Epoch: 023 loss_train: 553.3916\n",
      "Epoch: 024 loss_train: 519.2625\n",
      "Epoch: 025 loss_train: 489.1161\n",
      "Epoch: 026 loss_train: 494.2748\n",
      "Epoch: 027 loss_train: 513.5694\n",
      "Epoch: 028 loss_train: 482.0765\n",
      "Epoch: 029 loss_train: 450.9106\n",
      "Epoch: 030 loss_train: 406.2280\n",
      "Epoch: 031 loss_train: 385.1508\n",
      "Epoch: 032 loss_train: 397.2316\n",
      "Epoch: 033 loss_train: 394.9321\n",
      "Epoch: 034 loss_train: 442.5471\n",
      "Epoch: 035 loss_train: 367.4452\n",
      "Epoch: 036 loss_train: 369.2917\n",
      "Epoch: 037 loss_train: 315.2085\n",
      "Epoch: 038 loss_train: 324.2715\n",
      "Epoch: 039 loss_train: 312.3531\n",
      "Epoch: 040 loss_train: 294.3727\n",
      "Epoch: 041 loss_train: 276.7083\n",
      "Epoch: 042 loss_train: 268.1567\n",
      "Epoch: 043 loss_train: 255.6629\n",
      "Epoch: 044 loss_train: 236.8975\n",
      "Epoch: 045 loss_train: 229.3862\n",
      "Epoch: 046 loss_train: 219.5960\n",
      "Epoch: 047 loss_train: 219.5909\n",
      "Epoch: 048 loss_train: 212.6798\n",
      "Epoch: 049 loss_train: 205.1161\n",
      "Epoch: 050 loss_train: 204.9488\n",
      "Epoch: 051 loss_train: 201.2724\n",
      "Epoch: 052 loss_train: 198.7368\n",
      "Epoch: 053 loss_train: 195.2917\n",
      "Epoch: 054 loss_train: 195.5580\n",
      "Epoch: 055 loss_train: 192.3639\n",
      "Epoch: 056 loss_train: 190.5216\n",
      "Epoch: 057 loss_train: 188.5880\n",
      "Epoch: 058 loss_train: 189.1097\n",
      "Epoch: 059 loss_train: 184.7276\n",
      "Epoch: 060 loss_train: 184.1974\n",
      "Epoch: 061 loss_train: 180.4115\n",
      "Epoch: 062 loss_train: 182.0009\n",
      "Epoch: 063 loss_train: 180.5664\n",
      "Epoch: 064 loss_train: 178.7625\n",
      "Epoch: 065 loss_train: 177.2484\n",
      "Epoch: 066 loss_train: 176.3800\n",
      "Epoch: 067 loss_train: 176.7969\n",
      "Epoch: 068 loss_train: 176.4041\n",
      "Epoch: 069 loss_train: 173.5914\n",
      "Epoch: 070 loss_train: 177.7057\n",
      "Epoch: 071 loss_train: 173.5565\n",
      "Epoch: 072 loss_train: 170.4158\n",
      "Epoch: 073 loss_train: 170.9007\n",
      "Epoch: 074 loss_train: 167.6165\n",
      "Epoch: 075 loss_train: 164.9581\n",
      "Epoch: 076 loss_train: 167.5023\n",
      "Epoch: 077 loss_train: 165.8414\n",
      "Epoch: 078 loss_train: 164.8342\n",
      "Epoch: 079 loss_train: 166.4208\n",
      "Epoch: 080 loss_train: 162.5057\n",
      "Epoch: 081 loss_train: 159.7021\n",
      "Epoch: 082 loss_train: 159.3123\n",
      "Epoch: 083 loss_train: 157.9953\n",
      "Epoch: 084 loss_train: 155.6940\n",
      "Epoch: 085 loss_train: 152.4837\n",
      "Epoch: 086 loss_train: 152.8459\n",
      "Epoch: 087 loss_train: 150.7457\n",
      "Epoch: 088 loss_train: 148.2245\n",
      "Epoch: 089 loss_train: 147.8920\n",
      "Epoch: 090 loss_train: 146.2970\n",
      "Epoch: 091 loss_train: 140.8715\n",
      "Epoch: 092 loss_train: 144.4770\n",
      "Epoch: 093 loss_train: 144.2468\n",
      "Epoch: 094 loss_train: 139.7316\n",
      "Epoch: 095 loss_train: 139.6821\n",
      "Epoch: 096 loss_train: 135.4742\n",
      "Epoch: 097 loss_train: 135.8127\n",
      "Epoch: 098 loss_train: 134.3104\n",
      "Epoch: 099 loss_train: 148.6405\n",
      "Epoch: 100 loss_train: 132.2744\n",
      "Epoch: 101 loss_train: 135.7437\n",
      "Epoch: 102 loss_train: 140.1507\n",
      "Epoch: 103 loss_train: 140.4569\n",
      "Epoch: 104 loss_train: 143.0324\n",
      "Epoch: 105 loss_train: 137.7431\n",
      "Epoch: 106 loss_train: 140.1228\n",
      "Epoch: 107 loss_train: 136.2785\n",
      "Epoch: 108 loss_train: 133.6037\n",
      "Epoch: 109 loss_train: 135.4780\n",
      "Epoch: 110 loss_train: 130.0580\n",
      "Epoch: 111 loss_train: 130.6035\n",
      "Epoch: 112 loss_train: 127.7597\n",
      "Epoch: 113 loss_train: 126.4844\n",
      "Epoch: 114 loss_train: 124.8899\n",
      "Epoch: 115 loss_train: 124.7755\n",
      "Epoch: 116 loss_train: 125.7614\n",
      "Epoch: 117 loss_train: 123.7073\n",
      "Epoch: 118 loss_train: 120.3959\n",
      "Epoch: 119 loss_train: 120.3202\n",
      "Epoch: 120 loss_train: 117.3320\n",
      "Epoch: 121 loss_train: 117.1926\n",
      "Epoch: 122 loss_train: 115.9355\n",
      "Epoch: 123 loss_train: 116.4177\n",
      "Epoch: 124 loss_train: 117.1748\n",
      "Epoch: 125 loss_train: 115.1871\n",
      "Epoch: 126 loss_train: 116.0301\n",
      "Epoch: 127 loss_train: 115.6080\n",
      "Epoch: 128 loss_train: 115.2824\n",
      "Epoch: 129 loss_train: 114.1595\n",
      "Epoch: 130 loss_train: 114.9165\n",
      "Epoch: 131 loss_train: 114.8184\n",
      "Epoch: 132 loss_train: 112.4771\n",
      "Epoch: 133 loss_train: 113.5008\n",
      "Epoch: 134 loss_train: 112.9183\n",
      "Epoch: 135 loss_train: 112.0150\n",
      "Epoch: 136 loss_train: 112.0601\n",
      "Epoch: 137 loss_train: 108.1830\n",
      "Epoch: 138 loss_train: 108.8929\n",
      "Epoch: 139 loss_train: 110.1531\n",
      "Epoch: 140 loss_train: 108.6956\n",
      "Epoch: 141 loss_train: 106.9290\n",
      "Epoch: 142 loss_train: 109.6552\n",
      "Epoch: 143 loss_train: 109.2485\n",
      "Epoch: 144 loss_train: 108.4077\n",
      "Epoch: 145 loss_train: 108.6064\n",
      "Epoch: 146 loss_train: 107.0789\n",
      "Epoch: 147 loss_train: 107.9569\n",
      "Epoch: 148 loss_train: 106.4946\n",
      "Epoch: 149 loss_train: 105.9221\n",
      "Epoch: 150 loss_train: 105.4673\n",
      "Epoch: 151 loss_train: 105.6169\n",
      "Epoch: 152 loss_train: 104.4105\n",
      "Epoch: 153 loss_train: 104.2913\n",
      "Epoch: 154 loss_train: 104.6613\n",
      "Epoch: 155 loss_train: 104.9348\n",
      "Epoch: 156 loss_train: 103.6290\n",
      "Epoch: 157 loss_train: 103.9102\n",
      "Epoch: 158 loss_train: 103.2304\n",
      "Epoch: 159 loss_train: 102.0761\n",
      "Epoch: 160 loss_train: 103.2494\n",
      "Epoch: 161 loss_train: 103.0057\n",
      "Epoch: 162 loss_train: 102.4782\n",
      "Epoch: 163 loss_train: 101.5868\n",
      "Epoch: 164 loss_train: 100.6878\n",
      "Epoch: 165 loss_train: 101.8039\n",
      "Epoch: 166 loss_train: 101.7448\n",
      "Epoch: 167 loss_train: 101.3271\n",
      "Epoch: 168 loss_train: 99.9111\n",
      "Epoch: 169 loss_train: 100.9000\n",
      "Epoch: 170 loss_train: 100.4168\n",
      "Epoch: 171 loss_train: 100.7604\n",
      "Epoch: 172 loss_train: 100.9953\n",
      "Epoch: 173 loss_train: 99.2965\n",
      "Epoch: 174 loss_train: 98.9932\n",
      "Epoch: 175 loss_train: 101.2242\n",
      "Epoch: 176 loss_train: 100.9896\n",
      "Epoch: 177 loss_train: 99.5398\n",
      "Epoch: 178 loss_train: 100.5863\n",
      "Epoch: 179 loss_train: 99.0885\n",
      "Epoch: 180 loss_train: 99.6148\n",
      "Epoch: 181 loss_train: 99.5169\n",
      "Epoch: 182 loss_train: 98.4737\n",
      "Epoch: 183 loss_train: 97.9485\n",
      "Epoch: 184 loss_train: 98.2994\n",
      "Epoch: 185 loss_train: 97.0515\n",
      "Epoch: 186 loss_train: 97.6690\n",
      "Epoch: 187 loss_train: 97.3375\n",
      "Epoch: 188 loss_train: 96.7325\n",
      "Epoch: 189 loss_train: 97.5129\n",
      "Epoch: 190 loss_train: 97.2365\n",
      "Epoch: 191 loss_train: 97.3309\n",
      "Epoch: 192 loss_train: 96.7381\n",
      "Epoch: 193 loss_train: 96.2654\n",
      "Epoch: 194 loss_train: 96.3939\n",
      "Epoch: 195 loss_train: 98.1627\n",
      "Epoch: 196 loss_train: 101.5981\n",
      "Epoch: 197 loss_train: 100.5338\n",
      "Epoch: 198 loss_train: 100.1884\n",
      "Epoch: 199 loss_train: 99.9601\n",
      "Epoch: 200 loss_train: 99.5149\n",
      "Epoch: 001 loss_train: 99.0302\n",
      "Epoch: 002 loss_train: 946.0245\n",
      "Epoch: 003 loss_train: 98.4029\n",
      "Epoch: 004 loss_train: 122.5448\n",
      "Epoch: 005 loss_train: 146.5809\n",
      "Epoch: 006 loss_train: 161.2324\n",
      "Epoch: 007 loss_train: 165.7770\n",
      "Epoch: 008 loss_train: 165.0894\n",
      "Epoch: 009 loss_train: 161.2491\n",
      "Epoch: 010 loss_train: 155.5166\n",
      "Epoch: 011 loss_train: 148.6118\n",
      "Epoch: 012 loss_train: 144.7093\n",
      "Epoch: 013 loss_train: 143.3587\n",
      "Epoch: 014 loss_train: 134.7001\n",
      "Epoch: 015 loss_train: 130.0331\n",
      "Epoch: 016 loss_train: 129.6857\n",
      "Epoch: 017 loss_train: 126.9085\n",
      "Epoch: 018 loss_train: 127.2519\n",
      "Epoch: 019 loss_train: 127.4792\n",
      "Epoch: 020 loss_train: 126.4385\n",
      "Epoch: 021 loss_train: 122.5569\n",
      "Epoch: 022 loss_train: 122.9942\n",
      "Epoch: 023 loss_train: 117.6425\n",
      "Epoch: 024 loss_train: 117.6454\n",
      "Epoch: 025 loss_train: 117.1459\n",
      "Epoch: 026 loss_train: 115.7057\n",
      "Epoch: 027 loss_train: 116.1053\n",
      "Epoch: 028 loss_train: 114.7766\n",
      "Epoch: 029 loss_train: 115.2937\n",
      "Epoch: 030 loss_train: 114.2490\n",
      "Epoch: 031 loss_train: 114.4541\n",
      "Epoch: 032 loss_train: 114.1514\n",
      "Epoch: 033 loss_train: 112.9257\n",
      "Epoch: 034 loss_train: 111.6554\n",
      "Epoch: 035 loss_train: 111.9790\n",
      "Epoch: 036 loss_train: 113.4665\n",
      "Epoch: 037 loss_train: 111.6219\n",
      "Epoch: 038 loss_train: 112.3347\n",
      "Epoch: 039 loss_train: 110.6451\n",
      "Epoch: 040 loss_train: 110.5700\n",
      "Epoch: 041 loss_train: 112.1760\n",
      "Epoch: 042 loss_train: 109.8011\n",
      "Epoch: 043 loss_train: 110.6588\n",
      "Epoch: 044 loss_train: 109.8654\n",
      "Epoch: 045 loss_train: 109.4757\n",
      "Epoch: 046 loss_train: 107.8343\n",
      "Epoch: 047 loss_train: 106.7627\n",
      "Epoch: 048 loss_train: 106.7176\n",
      "Epoch: 049 loss_train: 104.9249\n",
      "Epoch: 050 loss_train: 103.6680\n",
      "Epoch: 051 loss_train: 106.0341\n",
      "Epoch: 052 loss_train: 105.4991\n",
      "Epoch: 053 loss_train: 106.2796\n",
      "Epoch: 054 loss_train: 106.9400\n",
      "Epoch: 055 loss_train: 105.0007\n",
      "Epoch: 056 loss_train: 103.6364\n",
      "Epoch: 057 loss_train: 105.6830\n",
      "Epoch: 058 loss_train: 102.2085\n",
      "Epoch: 059 loss_train: 103.5394\n",
      "Epoch: 060 loss_train: 104.5019\n",
      "Epoch: 061 loss_train: 103.7888\n",
      "Epoch: 062 loss_train: 102.8528\n",
      "Epoch: 063 loss_train: 102.7154\n",
      "Epoch: 064 loss_train: 103.2329\n",
      "Epoch: 065 loss_train: 103.2117\n",
      "Epoch: 066 loss_train: 101.8228\n",
      "Epoch: 067 loss_train: 99.9763\n",
      "Epoch: 068 loss_train: 111.3250\n",
      "Epoch: 069 loss_train: 103.8982\n",
      "Epoch: 070 loss_train: 108.4862\n",
      "Epoch: 071 loss_train: 107.4995\n",
      "Epoch: 072 loss_train: 109.4730\n",
      "Epoch: 073 loss_train: 109.7410\n",
      "Epoch: 074 loss_train: 113.0643\n",
      "Epoch: 075 loss_train: 112.2496\n",
      "Epoch: 076 loss_train: 114.5359\n",
      "Epoch: 077 loss_train: 111.4319\n",
      "Epoch: 078 loss_train: 112.6842\n",
      "Epoch: 079 loss_train: 113.4733\n",
      "Epoch: 080 loss_train: 110.1252\n",
      "Epoch: 081 loss_train: 108.9455\n",
      "Epoch: 082 loss_train: 111.4653\n",
      "Epoch: 083 loss_train: 111.8791\n",
      "Epoch: 084 loss_train: 110.0951\n",
      "Epoch: 085 loss_train: 108.3946\n",
      "Epoch: 086 loss_train: 109.1566\n",
      "Epoch: 087 loss_train: 110.4584\n",
      "Epoch: 088 loss_train: 107.2275\n",
      "Epoch: 089 loss_train: 107.4319\n",
      "Epoch: 090 loss_train: 107.2701\n",
      "Epoch: 091 loss_train: 107.5133\n",
      "Epoch: 092 loss_train: 105.8788\n",
      "Epoch: 093 loss_train: 103.7634\n",
      "Epoch: 094 loss_train: 102.2654\n",
      "Epoch: 095 loss_train: 102.9359\n",
      "Epoch: 096 loss_train: 102.8297\n",
      "Epoch: 097 loss_train: 101.1007\n",
      "Epoch: 098 loss_train: 100.1667\n",
      "Epoch: 099 loss_train: 100.2236\n",
      "Epoch: 100 loss_train: 99.4763\n",
      "Epoch: 101 loss_train: 98.1389\n",
      "Epoch: 102 loss_train: 97.7757\n",
      "Epoch: 103 loss_train: 96.9775\n",
      "Epoch: 104 loss_train: 96.5141\n",
      "Epoch: 105 loss_train: 95.9989\n",
      "Epoch: 106 loss_train: 95.7484\n",
      "Epoch: 107 loss_train: 94.8285\n",
      "Epoch: 108 loss_train: 97.2196\n",
      "Epoch: 109 loss_train: 98.2187\n",
      "Epoch: 110 loss_train: 98.4398\n",
      "Epoch: 111 loss_train: 95.0693\n",
      "Epoch: 112 loss_train: 93.7397\n",
      "Epoch: 113 loss_train: 95.2587\n",
      "Epoch: 114 loss_train: 94.7249\n",
      "Epoch: 115 loss_train: 97.5112\n",
      "Epoch: 116 loss_train: 97.7072\n",
      "Epoch: 117 loss_train: 95.3224\n",
      "Epoch: 118 loss_train: 92.6391\n",
      "Epoch: 119 loss_train: 98.2589\n",
      "Epoch: 120 loss_train: 97.2860\n",
      "Epoch: 121 loss_train: 100.3736\n",
      "Epoch: 122 loss_train: 101.5144\n",
      "Epoch: 123 loss_train: 101.3910\n",
      "Epoch: 124 loss_train: 101.4489\n",
      "Epoch: 125 loss_train: 101.7380\n",
      "Epoch: 126 loss_train: 101.0218\n",
      "Epoch: 127 loss_train: 100.2819\n",
      "Epoch: 128 loss_train: 98.8639\n",
      "Epoch: 129 loss_train: 97.8339\n",
      "Epoch: 130 loss_train: 97.0556\n",
      "Epoch: 131 loss_train: 98.9381\n",
      "Epoch: 132 loss_train: 96.2572\n",
      "Epoch: 133 loss_train: 96.7809\n",
      "Epoch: 134 loss_train: 96.2135\n",
      "Epoch: 135 loss_train: 96.7925\n",
      "Epoch: 136 loss_train: 96.2328\n",
      "Epoch: 137 loss_train: 95.0542\n",
      "Epoch: 138 loss_train: 92.9491\n",
      "Epoch: 139 loss_train: 94.4016\n",
      "Epoch: 140 loss_train: 93.5145\n",
      "Epoch: 141 loss_train: 95.0516\n",
      "Epoch: 142 loss_train: 94.1690\n",
      "Epoch: 143 loss_train: 92.9373\n",
      "Epoch: 144 loss_train: 92.6194\n",
      "Epoch: 145 loss_train: 92.8067\n",
      "Epoch: 146 loss_train: 91.5911\n",
      "Epoch: 147 loss_train: 91.8886\n",
      "Epoch: 148 loss_train: 91.7202\n",
      "Epoch: 149 loss_train: 90.4433\n",
      "Epoch: 150 loss_train: 89.9299\n",
      "Epoch: 151 loss_train: 88.9059\n",
      "Epoch: 152 loss_train: 89.2501\n",
      "Epoch: 153 loss_train: 89.0511\n",
      "Epoch: 154 loss_train: 87.9816\n",
      "Epoch: 155 loss_train: 90.5173\n",
      "Epoch: 156 loss_train: 88.5926\n",
      "Epoch: 157 loss_train: 90.4951\n",
      "Epoch: 158 loss_train: 90.9700\n",
      "Epoch: 159 loss_train: 89.3293\n",
      "Epoch: 160 loss_train: 88.3779\n",
      "Epoch: 161 loss_train: 90.5680\n",
      "Epoch: 162 loss_train: 87.8728\n",
      "Epoch: 163 loss_train: 88.8160\n",
      "Epoch: 164 loss_train: 87.7557\n",
      "Epoch: 165 loss_train: 86.1591\n",
      "Epoch: 166 loss_train: 87.1724\n",
      "Epoch: 167 loss_train: 87.4261\n",
      "Epoch: 168 loss_train: 88.0154\n",
      "Epoch: 169 loss_train: 85.9123\n",
      "Epoch: 170 loss_train: 117.3145\n",
      "Epoch: 171 loss_train: 96.5061\n",
      "Epoch: 172 loss_train: 103.8161\n",
      "Epoch: 173 loss_train: 103.8983\n",
      "Epoch: 174 loss_train: 101.6411\n",
      "Epoch: 175 loss_train: 100.4886\n",
      "Epoch: 176 loss_train: 103.9292\n",
      "Epoch: 177 loss_train: 102.0388\n",
      "Epoch: 178 loss_train: 100.1395\n",
      "Epoch: 179 loss_train: 100.4517\n",
      "Epoch: 180 loss_train: 102.0570\n",
      "Epoch: 181 loss_train: 102.0884\n",
      "Epoch: 182 loss_train: 100.9567\n",
      "Epoch: 183 loss_train: 99.9704\n",
      "Epoch: 184 loss_train: 99.5686\n",
      "Epoch: 185 loss_train: 100.6115\n",
      "Epoch: 186 loss_train: 99.4873\n",
      "Epoch: 187 loss_train: 98.1987\n",
      "Epoch: 188 loss_train: 98.2684\n",
      "Epoch: 189 loss_train: 98.0397\n",
      "Epoch: 190 loss_train: 97.5253\n",
      "Epoch: 191 loss_train: 97.3011\n",
      "Epoch: 192 loss_train: 96.3029\n",
      "Epoch: 193 loss_train: 95.7749\n",
      "Epoch: 194 loss_train: 96.1308\n",
      "Epoch: 195 loss_train: 95.0727\n",
      "Epoch: 196 loss_train: 94.1323\n",
      "Epoch: 197 loss_train: 93.7104\n",
      "Epoch: 198 loss_train: 95.1670\n",
      "Epoch: 199 loss_train: 93.6698\n",
      "Epoch: 200 loss_train: 93.7567\n",
      "Epoch: 001 loss_train: 93.4778\n",
      "Epoch: 002 loss_train: 157.6676\n",
      "Epoch: 003 loss_train: 92.5919\n",
      "Epoch: 004 loss_train: 93.7497\n",
      "Epoch: 005 loss_train: 97.7141\n",
      "Epoch: 006 loss_train: 97.9258\n",
      "Epoch: 007 loss_train: 96.5818\n",
      "Epoch: 008 loss_train: 95.0243\n",
      "Epoch: 009 loss_train: 92.8543\n",
      "Epoch: 010 loss_train: 92.7251\n",
      "Epoch: 011 loss_train: 92.3153\n",
      "Epoch: 012 loss_train: 91.4396\n",
      "Epoch: 013 loss_train: 90.9181\n",
      "Epoch: 014 loss_train: 90.8314\n",
      "Epoch: 015 loss_train: 89.6313\n",
      "Epoch: 016 loss_train: 89.5471\n",
      "Epoch: 017 loss_train: 89.6004\n",
      "Epoch: 018 loss_train: 89.3972\n",
      "Epoch: 019 loss_train: 90.3211\n",
      "Epoch: 020 loss_train: 89.2755\n",
      "Epoch: 021 loss_train: 88.8902\n",
      "Epoch: 022 loss_train: 88.4176\n",
      "Epoch: 023 loss_train: 87.8680\n",
      "Epoch: 024 loss_train: 87.9702\n",
      "Epoch: 025 loss_train: 87.8412\n",
      "Epoch: 026 loss_train: 88.0789\n",
      "Epoch: 027 loss_train: 88.1432\n",
      "Epoch: 028 loss_train: 87.6520\n",
      "Epoch: 029 loss_train: 87.2846\n",
      "Epoch: 030 loss_train: 87.4402\n",
      "Epoch: 031 loss_train: 86.6640\n",
      "Epoch: 032 loss_train: 86.8922\n",
      "Epoch: 033 loss_train: 86.5038\n",
      "Epoch: 034 loss_train: 86.3085\n",
      "Epoch: 035 loss_train: 86.6130\n",
      "Epoch: 036 loss_train: 85.9463\n",
      "Epoch: 037 loss_train: 85.4243\n",
      "Epoch: 038 loss_train: 85.2009\n",
      "Epoch: 039 loss_train: 85.6453\n",
      "Epoch: 040 loss_train: 85.5936\n",
      "Epoch: 041 loss_train: 85.1434\n",
      "Epoch: 042 loss_train: 85.6465\n",
      "Epoch: 043 loss_train: 85.3232\n",
      "Epoch: 044 loss_train: 85.2915\n",
      "Epoch: 045 loss_train: 85.3085\n",
      "Epoch: 046 loss_train: 86.2671\n",
      "Epoch: 047 loss_train: 84.8589\n",
      "Epoch: 048 loss_train: 86.3373\n",
      "Epoch: 049 loss_train: 85.0289\n",
      "Epoch: 050 loss_train: 84.6430\n",
      "Epoch: 051 loss_train: 85.1792\n",
      "Epoch: 052 loss_train: 84.6325\n",
      "Epoch: 053 loss_train: 84.3683\n",
      "Epoch: 054 loss_train: 84.5238\n",
      "Epoch: 055 loss_train: 84.0233\n",
      "Epoch: 056 loss_train: 83.8612\n",
      "Epoch: 057 loss_train: 84.2783\n",
      "Epoch: 058 loss_train: 83.7504\n",
      "Epoch: 059 loss_train: 83.5171\n",
      "Epoch: 060 loss_train: 83.5568\n",
      "Epoch: 061 loss_train: 83.3693\n",
      "Epoch: 062 loss_train: 83.3318\n",
      "Epoch: 063 loss_train: 83.1456\n",
      "Epoch: 064 loss_train: 82.0491\n",
      "Epoch: 065 loss_train: 82.5308\n",
      "Epoch: 066 loss_train: 82.2124\n",
      "Epoch: 067 loss_train: 82.1706\n",
      "Epoch: 068 loss_train: 81.9807\n",
      "Epoch: 069 loss_train: 81.7535\n",
      "Epoch: 070 loss_train: 81.5948\n",
      "Epoch: 071 loss_train: 81.7445\n",
      "Epoch: 072 loss_train: 81.5422\n",
      "Epoch: 073 loss_train: 80.1608\n",
      "Epoch: 074 loss_train: 81.0004\n",
      "Epoch: 075 loss_train: 81.6753\n",
      "Epoch: 076 loss_train: 81.9457\n",
      "Epoch: 077 loss_train: 80.7271\n",
      "Epoch: 078 loss_train: 84.6417\n",
      "Epoch: 079 loss_train: 82.8298\n",
      "Epoch: 080 loss_train: 84.1560\n",
      "Epoch: 081 loss_train: 83.7135\n",
      "Epoch: 082 loss_train: 82.2202\n",
      "Epoch: 083 loss_train: 81.6384\n",
      "Epoch: 084 loss_train: 84.6885\n",
      "Epoch: 085 loss_train: 81.3336\n",
      "Epoch: 086 loss_train: 82.8340\n",
      "Epoch: 087 loss_train: 83.5309\n",
      "Epoch: 088 loss_train: 82.7199\n",
      "Epoch: 089 loss_train: 82.3815\n",
      "Epoch: 090 loss_train: 81.8496\n",
      "Epoch: 091 loss_train: 81.6189\n",
      "Epoch: 092 loss_train: 81.5671\n",
      "Epoch: 093 loss_train: 81.5163\n",
      "Epoch: 094 loss_train: 80.6822\n",
      "Epoch: 095 loss_train: 80.7793\n",
      "Epoch: 096 loss_train: 80.9097\n",
      "Epoch: 097 loss_train: 80.5157\n",
      "Epoch: 098 loss_train: 80.1784\n",
      "Epoch: 099 loss_train: 79.9254\n",
      "Epoch: 100 loss_train: 79.6733\n",
      "Epoch: 101 loss_train: 79.7195\n",
      "Epoch: 102 loss_train: 79.5503\n",
      "Epoch: 103 loss_train: 79.2542\n",
      "Epoch: 104 loss_train: 79.9216\n",
      "Epoch: 105 loss_train: 78.9014\n",
      "Epoch: 106 loss_train: 78.6234\n",
      "Epoch: 107 loss_train: 78.9805\n",
      "Epoch: 108 loss_train: 78.8375\n",
      "Epoch: 109 loss_train: 78.3919\n",
      "Epoch: 110 loss_train: 78.3984\n",
      "Epoch: 111 loss_train: 78.4679\n",
      "Epoch: 112 loss_train: 78.4520\n",
      "Epoch: 113 loss_train: 78.1650\n",
      "Epoch: 114 loss_train: 77.9457\n",
      "Epoch: 115 loss_train: 77.9438\n",
      "Epoch: 116 loss_train: 78.0792\n",
      "Epoch: 117 loss_train: 77.3767\n",
      "Epoch: 118 loss_train: 77.2704\n",
      "Epoch: 119 loss_train: 77.0717\n",
      "Epoch: 120 loss_train: 77.1226\n",
      "Epoch: 121 loss_train: 76.7131\n",
      "Epoch: 122 loss_train: 76.9831\n",
      "Epoch: 123 loss_train: 76.7327\n",
      "Epoch: 124 loss_train: 76.6249\n",
      "Epoch: 125 loss_train: 76.3820\n",
      "Epoch: 126 loss_train: 76.6860\n",
      "Epoch: 127 loss_train: 76.3571\n",
      "Epoch: 128 loss_train: 76.6759\n",
      "Epoch: 129 loss_train: 76.4591\n",
      "Epoch: 130 loss_train: 76.2468\n",
      "Epoch: 131 loss_train: 75.9979\n",
      "Epoch: 132 loss_train: 76.3174\n",
      "Epoch: 133 loss_train: 75.6193\n",
      "Epoch: 134 loss_train: 75.6717\n",
      "Epoch: 135 loss_train: 75.5842\n",
      "Epoch: 136 loss_train: 75.7956\n",
      "Epoch: 137 loss_train: 75.8420\n",
      "Epoch: 138 loss_train: 75.6108\n",
      "Epoch: 139 loss_train: 75.3616\n",
      "Epoch: 140 loss_train: 75.2709\n",
      "Epoch: 141 loss_train: 74.9984\n",
      "Epoch: 142 loss_train: 75.2497\n",
      "Epoch: 143 loss_train: 75.7490\n",
      "Epoch: 144 loss_train: 75.3359\n",
      "Epoch: 145 loss_train: 75.4234\n",
      "Epoch: 146 loss_train: 75.1222\n",
      "Epoch: 147 loss_train: 75.9438\n",
      "Epoch: 148 loss_train: 75.2474\n",
      "Epoch: 149 loss_train: 75.5991\n",
      "Epoch: 150 loss_train: 75.1504\n",
      "Epoch: 151 loss_train: 75.2092\n",
      "Epoch: 152 loss_train: 74.9932\n",
      "Epoch: 153 loss_train: 74.8364\n",
      "Epoch: 154 loss_train: 74.6994\n",
      "Epoch: 155 loss_train: 75.3979\n",
      "Epoch: 156 loss_train: 75.2708\n",
      "Epoch: 157 loss_train: 74.1575\n",
      "Epoch: 158 loss_train: 74.6262\n",
      "Epoch: 159 loss_train: 73.8951\n",
      "Epoch: 160 loss_train: 74.4623\n",
      "Epoch: 161 loss_train: 74.2601\n",
      "Epoch: 162 loss_train: 74.7585\n",
      "Epoch: 163 loss_train: 74.3972\n",
      "Epoch: 164 loss_train: 74.5044\n",
      "Epoch: 165 loss_train: 73.7134\n",
      "Epoch: 166 loss_train: 74.3964\n",
      "Epoch: 167 loss_train: 74.0015\n",
      "Epoch: 168 loss_train: 74.3512\n",
      "Epoch: 169 loss_train: 73.9389\n",
      "Epoch: 170 loss_train: 73.9286\n",
      "Epoch: 171 loss_train: 74.2363\n",
      "Epoch: 172 loss_train: 74.0227\n",
      "Epoch: 173 loss_train: 74.0655\n",
      "Epoch: 174 loss_train: 73.5645\n",
      "Epoch: 175 loss_train: 73.3051\n",
      "Epoch: 176 loss_train: 72.8102\n",
      "Epoch: 177 loss_train: 73.1022\n",
      "Epoch: 178 loss_train: 73.5286\n",
      "Epoch: 179 loss_train: 72.6699\n",
      "Epoch: 180 loss_train: 73.1502\n",
      "Epoch: 181 loss_train: 73.6574\n",
      "Epoch: 182 loss_train: 73.5901\n",
      "Epoch: 183 loss_train: 72.7827\n",
      "Epoch: 184 loss_train: 72.8687\n",
      "Epoch: 185 loss_train: 73.1704\n",
      "Epoch: 186 loss_train: 72.9479\n",
      "Epoch: 187 loss_train: 72.8287\n",
      "Epoch: 188 loss_train: 73.4269\n",
      "Epoch: 189 loss_train: 72.8481\n",
      "Epoch: 190 loss_train: 72.2428\n",
      "Epoch: 191 loss_train: 72.4292\n",
      "Epoch: 192 loss_train: 72.2078\n",
      "Epoch: 193 loss_train: 72.4735\n",
      "Epoch: 194 loss_train: 72.1772\n",
      "Epoch: 195 loss_train: 71.8299\n",
      "Epoch: 196 loss_train: 71.8528\n",
      "Epoch: 197 loss_train: 71.7802\n",
      "Epoch: 198 loss_train: 71.4090\n",
      "Epoch: 199 loss_train: 71.5672\n",
      "Epoch: 200 loss_train: 71.9355\n",
      "Epoch: 001 loss_train: 71.7530\n",
      "Epoch: 002 loss_train: 76.3908\n",
      "Epoch: 003 loss_train: 72.7744\n",
      "Epoch: 004 loss_train: 78.1078\n",
      "Epoch: 005 loss_train: 72.6533\n",
      "Epoch: 006 loss_train: 73.8369\n",
      "Epoch: 007 loss_train: 73.9849\n",
      "Epoch: 008 loss_train: 73.9992\n",
      "Epoch: 009 loss_train: 74.0592\n",
      "Epoch: 010 loss_train: 74.4127\n",
      "Epoch: 011 loss_train: 73.1994\n",
      "Epoch: 012 loss_train: 72.8362\n",
      "Epoch: 013 loss_train: 72.8017\n",
      "Epoch: 014 loss_train: 73.2066\n",
      "Epoch: 015 loss_train: 72.4912\n",
      "Epoch: 016 loss_train: 71.8126\n",
      "Epoch: 017 loss_train: 71.7279\n",
      "Epoch: 018 loss_train: 71.6232\n",
      "Epoch: 019 loss_train: 72.1537\n",
      "Epoch: 020 loss_train: 72.1949\n",
      "Epoch: 021 loss_train: 72.0835\n",
      "Epoch: 022 loss_train: 72.1397\n",
      "Epoch: 023 loss_train: 71.4374\n",
      "Epoch: 024 loss_train: 71.9479\n",
      "Epoch: 025 loss_train: 71.2069\n",
      "Epoch: 026 loss_train: 71.2493\n",
      "Epoch: 027 loss_train: 71.2934\n",
      "Epoch: 028 loss_train: 70.8905\n",
      "Epoch: 029 loss_train: 71.3409\n",
      "Epoch: 030 loss_train: 70.6032\n",
      "Epoch: 031 loss_train: 70.8938\n",
      "Epoch: 032 loss_train: 70.6288\n",
      "Epoch: 033 loss_train: 70.6187\n",
      "Epoch: 034 loss_train: 70.6054\n",
      "Epoch: 035 loss_train: 70.1253\n",
      "Epoch: 036 loss_train: 70.6049\n",
      "Epoch: 037 loss_train: 70.3449\n",
      "Epoch: 038 loss_train: 70.7345\n",
      "Epoch: 039 loss_train: 70.3042\n",
      "Epoch: 040 loss_train: 70.4229\n",
      "Epoch: 041 loss_train: 70.3415\n",
      "Epoch: 042 loss_train: 70.3550\n",
      "Epoch: 043 loss_train: 70.3084\n",
      "Epoch: 044 loss_train: 70.0240\n",
      "Epoch: 045 loss_train: 70.1743\n",
      "Epoch: 046 loss_train: 70.5858\n",
      "Epoch: 047 loss_train: 69.9603\n",
      "Epoch: 048 loss_train: 70.2489\n",
      "Epoch: 049 loss_train: 69.9582\n",
      "Epoch: 050 loss_train: 69.9606\n",
      "Epoch: 051 loss_train: 70.0278\n",
      "Epoch: 052 loss_train: 70.2615\n",
      "Epoch: 053 loss_train: 70.0683\n",
      "Epoch: 054 loss_train: 69.6062\n",
      "Epoch: 055 loss_train: 70.2731\n",
      "Epoch: 056 loss_train: 70.1838\n",
      "Epoch: 057 loss_train: 69.8887\n",
      "Epoch: 058 loss_train: 69.6453\n",
      "Epoch: 059 loss_train: 69.6914\n",
      "Epoch: 060 loss_train: 69.3111\n",
      "Epoch: 061 loss_train: 69.4193\n",
      "Epoch: 062 loss_train: 69.3518\n",
      "Epoch: 063 loss_train: 69.4273\n",
      "Epoch: 064 loss_train: 69.4710\n",
      "Epoch: 065 loss_train: 69.3595\n",
      "Epoch: 066 loss_train: 69.5269\n",
      "Epoch: 067 loss_train: 69.4447\n",
      "Epoch: 068 loss_train: 69.4500\n",
      "Epoch: 069 loss_train: 69.4631\n",
      "Epoch: 070 loss_train: 69.2694\n",
      "Epoch: 071 loss_train: 69.1740\n",
      "Epoch: 072 loss_train: 69.3700\n",
      "Epoch: 073 loss_train: 69.0046\n",
      "Epoch: 074 loss_train: 69.3010\n",
      "Epoch: 075 loss_train: 69.3682\n",
      "Epoch: 076 loss_train: 69.1984\n",
      "Epoch: 077 loss_train: 69.1233\n",
      "Epoch: 078 loss_train: 69.3378\n",
      "Epoch: 079 loss_train: 69.2132\n",
      "Epoch: 080 loss_train: 68.7483\n",
      "Epoch: 081 loss_train: 69.3530\n",
      "Epoch: 082 loss_train: 69.0900\n",
      "Epoch: 083 loss_train: 68.9304\n",
      "Epoch: 084 loss_train: 69.1570\n",
      "Epoch: 085 loss_train: 69.2900\n",
      "Epoch: 086 loss_train: 68.6988\n",
      "Epoch: 087 loss_train: 69.0517\n",
      "Epoch: 088 loss_train: 69.0418\n",
      "Epoch: 089 loss_train: 68.4747\n",
      "Epoch: 090 loss_train: 68.5730\n",
      "Epoch: 091 loss_train: 68.8529\n",
      "Epoch: 092 loss_train: 68.7001\n",
      "Epoch: 093 loss_train: 68.9083\n",
      "Epoch: 094 loss_train: 68.5630\n",
      "Epoch: 095 loss_train: 69.0134\n",
      "Epoch: 096 loss_train: 68.2802\n",
      "Epoch: 097 loss_train: 68.6935\n",
      "Epoch: 098 loss_train: 68.4532\n",
      "Epoch: 099 loss_train: 68.2457\n",
      "Epoch: 100 loss_train: 68.4737\n",
      "Epoch: 101 loss_train: 68.3247\n",
      "Epoch: 102 loss_train: 68.2088\n",
      "Epoch: 103 loss_train: 68.2837\n",
      "Epoch: 104 loss_train: 68.4367\n",
      "Epoch: 105 loss_train: 68.6315\n",
      "Epoch: 106 loss_train: 68.3736\n",
      "Epoch: 107 loss_train: 68.2321\n",
      "Epoch: 108 loss_train: 68.0397\n",
      "Epoch: 109 loss_train: 68.2273\n",
      "Epoch: 110 loss_train: 68.1377\n",
      "Epoch: 111 loss_train: 68.0190\n",
      "Epoch: 112 loss_train: 68.0591\n",
      "Epoch: 113 loss_train: 68.1100\n",
      "Epoch: 114 loss_train: 67.9857\n",
      "Epoch: 115 loss_train: 67.9894\n",
      "Epoch: 116 loss_train: 68.0988\n",
      "Epoch: 117 loss_train: 68.0542\n",
      "Epoch: 118 loss_train: 68.2674\n",
      "Epoch: 119 loss_train: 68.1279\n",
      "Epoch: 120 loss_train: 67.9615\n",
      "Epoch: 121 loss_train: 67.9685\n",
      "Epoch: 122 loss_train: 68.0157\n",
      "Epoch: 123 loss_train: 68.3397\n",
      "Epoch: 124 loss_train: 67.8232\n",
      "Epoch: 125 loss_train: 67.5952\n",
      "Epoch: 126 loss_train: 67.7865\n",
      "Epoch: 127 loss_train: 67.8381\n",
      "Epoch: 128 loss_train: 67.9438\n",
      "Epoch: 129 loss_train: 67.6859\n",
      "Epoch: 130 loss_train: 67.4599\n",
      "Epoch: 131 loss_train: 67.5570\n",
      "Epoch: 132 loss_train: 67.7161\n",
      "Epoch: 133 loss_train: 67.4159\n",
      "Epoch: 134 loss_train: 67.3922\n",
      "Epoch: 135 loss_train: 67.3441\n",
      "Epoch: 136 loss_train: 67.2654\n",
      "Epoch: 137 loss_train: 67.3631\n",
      "Epoch: 138 loss_train: 66.9269\n",
      "Epoch: 139 loss_train: 67.3065\n",
      "Epoch: 140 loss_train: 66.9613\n",
      "Epoch: 141 loss_train: 67.3628\n",
      "Epoch: 142 loss_train: 66.9879\n",
      "Epoch: 143 loss_train: 67.1919\n",
      "Epoch: 144 loss_train: 67.4949\n",
      "Epoch: 145 loss_train: 67.4737\n",
      "Epoch: 146 loss_train: 67.1407\n",
      "Epoch: 147 loss_train: 67.3062\n",
      "Epoch: 148 loss_train: 67.3537\n",
      "Epoch: 149 loss_train: 66.9131\n",
      "Epoch: 150 loss_train: 67.0395\n",
      "Epoch: 151 loss_train: 66.9319\n",
      "Epoch: 152 loss_train: 67.0004\n",
      "Epoch: 153 loss_train: 67.1876\n",
      "Epoch: 154 loss_train: 67.4541\n",
      "Epoch: 155 loss_train: 66.9773\n",
      "Epoch: 156 loss_train: 67.0245\n",
      "Epoch: 157 loss_train: 66.8229\n",
      "Epoch: 158 loss_train: 66.8258\n",
      "Epoch: 159 loss_train: 66.6747\n",
      "Epoch: 160 loss_train: 66.8739\n",
      "Epoch: 161 loss_train: 66.5199\n",
      "Epoch: 162 loss_train: 66.7247\n",
      "Epoch: 163 loss_train: 66.4536\n",
      "Epoch: 164 loss_train: 66.5438\n",
      "Epoch: 165 loss_train: 66.6423\n",
      "Epoch: 166 loss_train: 66.8162\n",
      "Epoch: 167 loss_train: 66.2289\n",
      "Epoch: 168 loss_train: 66.6955\n",
      "Epoch: 169 loss_train: 66.4680\n",
      "Epoch: 170 loss_train: 66.4382\n",
      "Epoch: 171 loss_train: 66.4462\n",
      "Epoch: 172 loss_train: 66.5036\n",
      "Epoch: 173 loss_train: 67.0145\n",
      "Epoch: 174 loss_train: 66.2726\n",
      "Epoch: 175 loss_train: 66.1031\n",
      "Epoch: 176 loss_train: 66.2924\n",
      "Epoch: 177 loss_train: 66.4449\n",
      "Epoch: 178 loss_train: 66.3621\n",
      "Epoch: 179 loss_train: 66.3400\n",
      "Epoch: 180 loss_train: 66.7817\n",
      "Epoch: 181 loss_train: 66.1280\n",
      "Epoch: 182 loss_train: 66.6176\n",
      "Epoch: 183 loss_train: 66.6041\n",
      "Epoch: 184 loss_train: 66.3278\n",
      "Epoch: 185 loss_train: 66.0033\n",
      "Epoch: 186 loss_train: 66.0860\n",
      "Epoch: 187 loss_train: 66.4391\n",
      "Epoch: 188 loss_train: 66.0186\n",
      "Epoch: 189 loss_train: 66.0399\n",
      "Epoch: 190 loss_train: 66.1465\n",
      "Epoch: 191 loss_train: 65.8830\n",
      "Epoch: 192 loss_train: 65.8562\n",
      "Epoch: 193 loss_train: 65.9343\n",
      "Epoch: 194 loss_train: 65.8455\n",
      "Epoch: 195 loss_train: 65.9166\n",
      "Epoch: 196 loss_train: 65.7066\n",
      "Epoch: 197 loss_train: 65.9131\n",
      "Epoch: 198 loss_train: 65.7180\n",
      "Epoch: 199 loss_train: 65.8382\n",
      "Epoch: 200 loss_train: 65.8340\n",
      "Epoch: 001 loss_train: 65.7584\n",
      "Epoch: 002 loss_train: 87.1943\n",
      "Epoch: 003 loss_train: 66.0805\n",
      "Epoch: 004 loss_train: 71.0667\n",
      "Epoch: 005 loss_train: 73.8421\n",
      "Epoch: 006 loss_train: 73.7054\n",
      "Epoch: 007 loss_train: 72.6806\n",
      "Epoch: 008 loss_train: 71.3966\n",
      "Epoch: 009 loss_train: 69.3054\n",
      "Epoch: 010 loss_train: 67.6553\n",
      "Epoch: 011 loss_train: 67.3365\n",
      "Epoch: 012 loss_train: 67.6501\n",
      "Epoch: 013 loss_train: 68.2797\n",
      "Epoch: 014 loss_train: 68.8496\n",
      "Epoch: 015 loss_train: 68.5181\n",
      "Epoch: 016 loss_train: 68.2189\n",
      "Epoch: 017 loss_train: 67.4238\n",
      "Epoch: 018 loss_train: 66.7480\n",
      "Epoch: 019 loss_train: 67.1712\n",
      "Epoch: 020 loss_train: 66.9330\n",
      "Epoch: 021 loss_train: 66.8539\n",
      "Epoch: 022 loss_train: 66.9683\n",
      "Epoch: 023 loss_train: 66.7086\n",
      "Epoch: 024 loss_train: 66.2861\n",
      "Epoch: 025 loss_train: 66.5080\n",
      "Epoch: 026 loss_train: 66.7167\n",
      "Epoch: 027 loss_train: 66.6289\n",
      "Epoch: 028 loss_train: 66.7390\n",
      "Epoch: 029 loss_train: 66.2568\n",
      "Epoch: 030 loss_train: 66.5502\n",
      "Epoch: 031 loss_train: 66.1812\n",
      "Epoch: 032 loss_train: 66.5059\n",
      "Epoch: 033 loss_train: 65.7844\n",
      "Epoch: 034 loss_train: 65.6908\n",
      "Epoch: 035 loss_train: 66.0332\n",
      "Epoch: 036 loss_train: 66.0928\n",
      "Epoch: 037 loss_train: 66.0053\n",
      "Epoch: 038 loss_train: 66.0877\n",
      "Epoch: 039 loss_train: 65.8491\n",
      "Epoch: 040 loss_train: 66.0271\n",
      "Epoch: 041 loss_train: 65.6559\n",
      "Epoch: 042 loss_train: 65.6483\n",
      "Epoch: 043 loss_train: 65.8024\n",
      "Epoch: 044 loss_train: 65.8188\n",
      "Epoch: 045 loss_train: 65.8795\n",
      "Epoch: 046 loss_train: 66.1006\n",
      "Epoch: 047 loss_train: 65.7282\n",
      "Epoch: 048 loss_train: 65.7905\n",
      "Epoch: 049 loss_train: 65.6245\n",
      "Epoch: 050 loss_train: 65.3276\n",
      "Epoch: 051 loss_train: 65.4595\n",
      "Epoch: 052 loss_train: 65.5605\n",
      "Epoch: 053 loss_train: 65.3558\n",
      "Epoch: 054 loss_train: 65.5273\n",
      "Epoch: 055 loss_train: 65.6285\n",
      "Epoch: 056 loss_train: 65.5149\n",
      "Epoch: 057 loss_train: 65.6613\n",
      "Epoch: 058 loss_train: 65.6126\n",
      "Epoch: 059 loss_train: 65.4297\n",
      "Epoch: 060 loss_train: 65.3964\n",
      "Epoch: 061 loss_train: 65.5383\n",
      "Epoch: 062 loss_train: 65.2148\n",
      "Epoch: 063 loss_train: 65.1435\n",
      "Epoch: 064 loss_train: 65.6016\n",
      "Epoch: 065 loss_train: 65.2723\n",
      "Epoch: 066 loss_train: 65.2393\n",
      "Epoch: 067 loss_train: 65.1359\n",
      "Epoch: 068 loss_train: 65.1976\n",
      "Epoch: 069 loss_train: 65.2688\n",
      "Epoch: 070 loss_train: 65.1490\n",
      "Epoch: 071 loss_train: 65.1225\n",
      "Epoch: 072 loss_train: 65.3027\n",
      "Epoch: 073 loss_train: 64.9054\n",
      "Epoch: 074 loss_train: 65.5249\n",
      "Epoch: 075 loss_train: 65.1925\n",
      "Epoch: 076 loss_train: 64.9817\n",
      "Epoch: 077 loss_train: 65.1095\n",
      "Epoch: 078 loss_train: 65.2882\n",
      "Epoch: 079 loss_train: 64.9836\n",
      "Epoch: 080 loss_train: 65.1818\n",
      "Epoch: 081 loss_train: 65.1924\n",
      "Epoch: 082 loss_train: 65.0396\n",
      "Epoch: 083 loss_train: 65.2953\n",
      "Epoch: 084 loss_train: 64.8806\n",
      "Epoch: 085 loss_train: 65.1539\n",
      "Epoch: 086 loss_train: 64.9746\n",
      "Epoch: 087 loss_train: 65.0384\n",
      "Epoch: 088 loss_train: 65.1145\n",
      "Epoch: 089 loss_train: 65.1871\n",
      "Epoch: 090 loss_train: 65.3043\n",
      "Epoch: 091 loss_train: 64.6395\n",
      "Epoch: 092 loss_train: 64.7888\n",
      "Epoch: 093 loss_train: 65.2870\n",
      "Epoch: 094 loss_train: 65.0382\n",
      "Epoch: 095 loss_train: 65.1525\n",
      "Epoch: 096 loss_train: 64.9083\n",
      "Epoch: 097 loss_train: 64.9949\n",
      "Epoch: 098 loss_train: 64.9976\n",
      "Epoch: 099 loss_train: 64.7823\n",
      "Epoch: 100 loss_train: 64.9780\n",
      "Epoch: 101 loss_train: 64.7266\n",
      "Epoch: 102 loss_train: 64.8558\n",
      "Epoch: 103 loss_train: 64.8272\n",
      "Epoch: 104 loss_train: 64.9268\n",
      "Epoch: 105 loss_train: 64.6035\n",
      "Epoch: 106 loss_train: 64.9097\n",
      "Epoch: 107 loss_train: 64.7741\n",
      "Epoch: 108 loss_train: 64.9377\n",
      "Epoch: 109 loss_train: 64.7318\n",
      "Epoch: 110 loss_train: 64.9113\n",
      "Epoch: 111 loss_train: 65.0085\n",
      "Epoch: 112 loss_train: 64.6615\n",
      "Epoch: 113 loss_train: 64.6823\n",
      "Epoch: 114 loss_train: 64.6487\n",
      "Epoch: 115 loss_train: 64.5939\n",
      "Epoch: 116 loss_train: 64.7801\n",
      "Epoch: 117 loss_train: 64.7654\n",
      "Epoch: 118 loss_train: 64.5950\n",
      "Epoch: 119 loss_train: 64.5535\n",
      "Epoch: 120 loss_train: 64.3920\n",
      "Epoch: 121 loss_train: 64.5123\n",
      "Epoch: 122 loss_train: 64.1808\n",
      "Epoch: 123 loss_train: 64.4682\n",
      "Epoch: 124 loss_train: 64.3679\n",
      "Epoch: 125 loss_train: 64.7673\n",
      "Epoch: 126 loss_train: 64.3856\n",
      "Epoch: 127 loss_train: 64.5034\n",
      "Epoch: 128 loss_train: 64.6592\n",
      "Epoch: 129 loss_train: 64.4341\n",
      "Epoch: 130 loss_train: 64.2451\n",
      "Epoch: 131 loss_train: 64.9247\n",
      "Epoch: 132 loss_train: 64.5369\n",
      "Epoch: 133 loss_train: 64.3285\n",
      "Epoch: 134 loss_train: 64.4444\n",
      "Epoch: 135 loss_train: 64.3940\n",
      "Epoch: 136 loss_train: 64.1806\n",
      "Epoch: 137 loss_train: 64.3651\n",
      "Epoch: 138 loss_train: 64.5208\n",
      "Epoch: 139 loss_train: 64.1591\n",
      "Epoch: 140 loss_train: 64.2722\n",
      "Epoch: 141 loss_train: 64.6777\n",
      "Epoch: 142 loss_train: 64.3759\n",
      "Epoch: 143 loss_train: 64.2318\n",
      "Epoch: 144 loss_train: 64.1711\n",
      "Epoch: 145 loss_train: 64.4071\n",
      "Epoch: 146 loss_train: 64.2528\n",
      "Epoch: 147 loss_train: 64.1510\n",
      "Epoch: 148 loss_train: 64.0988\n",
      "Epoch: 149 loss_train: 64.2109\n",
      "Epoch: 150 loss_train: 64.0979\n",
      "Epoch: 151 loss_train: 64.3431\n",
      "Epoch: 152 loss_train: 63.9717\n",
      "Epoch: 153 loss_train: 64.3694\n",
      "Epoch: 154 loss_train: 64.0256\n",
      "Epoch: 155 loss_train: 64.2915\n",
      "Epoch: 156 loss_train: 64.2411\n",
      "Epoch: 157 loss_train: 64.2856\n",
      "Epoch: 158 loss_train: 64.1538\n",
      "Epoch: 159 loss_train: 64.0127\n",
      "Epoch: 160 loss_train: 64.3182\n",
      "Epoch: 161 loss_train: 64.1612\n",
      "Epoch: 162 loss_train: 63.8123\n",
      "Epoch: 163 loss_train: 64.0805\n",
      "Epoch: 164 loss_train: 64.4441\n",
      "Epoch: 165 loss_train: 64.0665\n",
      "Epoch: 166 loss_train: 63.8217\n",
      "Epoch: 167 loss_train: 63.9071\n",
      "Epoch: 168 loss_train: 63.8390\n",
      "Epoch: 169 loss_train: 64.1879\n",
      "Epoch: 170 loss_train: 64.2396\n",
      "Epoch: 171 loss_train: 64.0277\n",
      "Epoch: 172 loss_train: 63.7746\n",
      "Epoch: 173 loss_train: 64.0208\n",
      "Epoch: 174 loss_train: 64.0174\n",
      "Epoch: 175 loss_train: 63.9360\n",
      "Epoch: 176 loss_train: 63.7957\n",
      "Epoch: 177 loss_train: 64.0468\n",
      "Epoch: 178 loss_train: 64.1002\n",
      "Epoch: 179 loss_train: 63.8236\n",
      "Epoch: 180 loss_train: 64.0505\n",
      "Epoch: 181 loss_train: 63.6833\n",
      "Epoch: 182 loss_train: 64.0077\n",
      "Epoch: 183 loss_train: 63.8909\n",
      "Epoch: 184 loss_train: 63.9625\n",
      "Epoch: 185 loss_train: 63.6810\n",
      "Epoch: 186 loss_train: 63.7245\n",
      "Epoch: 187 loss_train: 63.9088\n",
      "Epoch: 188 loss_train: 64.0033\n",
      "Epoch: 189 loss_train: 63.7816\n",
      "Epoch: 190 loss_train: 63.6126\n",
      "Epoch: 191 loss_train: 63.5379\n",
      "Epoch: 192 loss_train: 63.5494\n",
      "Epoch: 193 loss_train: 64.0229\n",
      "Epoch: 194 loss_train: 63.5908\n",
      "Epoch: 195 loss_train: 63.5676\n",
      "Epoch: 196 loss_train: 63.6635\n",
      "Epoch: 197 loss_train: 63.6280\n",
      "Epoch: 198 loss_train: 63.7105\n",
      "Epoch: 199 loss_train: 63.7405\n",
      "Epoch: 200 loss_train: 63.6475\n"
     ]
    }
   ],
   "source": [
    "n_dim=X_train_x.shape[1]\n",
    "model_glob=MLP(n_dim,256,64,0.3)\n",
    "\n",
    "loss_trains_glob=[]\n",
    "loss = nn.MSELoss()\n",
    "lr=2e-2\n",
    "for i in range(5):\n",
    "    lr/=2\n",
    "    optimizer = optim.Adam(model_glob.parameters(), lr=lr)\n",
    "    for epoch in range(200):\n",
    "        model_glob.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glob(X_train_glob)\n",
    "        loss_train_glob = loss(output.reshape(-1), hindex_train_glob)\n",
    "        loss_trains_glob.append(loss_train_glob.item())\n",
    "        loss_train_glob.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "                'loss_train: {:.4f}'.format(loss_train_glob.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1404962b70>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_trains_glob[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_glob.state_dict(), \"Global/full_train_model_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_test=[abs_nodeID_Test[i] for i in range(n_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred=model_glob(X_test_glob)\n",
    "submission=dict([(nodes_test[i], _pred[i]) for i in range(len(X_test_glob))])\n",
    "with open(\"submissions/deepwalk_MLP_full_emb_submission.csv\", 'w') as f:\n",
    "    f.write(\"author,hindex\\n\")\n",
    "    for k,h in submission.items():\n",
    "        f.write(str(k)+\",\"+str(h.item())+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "571ac4587ae4eeb6b02353bd76aeaaf0ceca15cf49d684242a7eb1fb5d42efb7"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('myEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
