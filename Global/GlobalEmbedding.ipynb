{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Global embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory : /users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project, Project directory : /users/eleves-a/2019/nathan.peluso/INF554\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model as LinearModels\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "from scipy import stats\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "NathanPath=\"d:\\Documents\\Info\\INF554\\INF554_Kaggle_Project\"\n",
    "NathanPath=\"/users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project\"\n",
    "\n",
    "project_path = str(Path(os.getcwd()).parent.absolute())\n",
    "print(\"Current directory : \" + os.getcwd() + \", Project directory : \" + project_path)\n",
    "\n",
    "os.chdir(project_path)\n",
    "os.chdir(NathanPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=np.load(\"Global/paper_vectors.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_abstracts=vectors[:,0].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624181\n",
      "2908511069\n",
      "3603\n"
     ]
    }
   ],
   "source": [
    "print(len(id_abstracts))\n",
    "print(np.max(id_abstracts))\n",
    "print(np.min(id_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_id_abstracts=dict([(a,b) for a,b in enumerate(id_abstracts)])\n",
    "id_abstracts_num=dict([(b,a) for a,b in enumerate(id_abstracts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624181"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_abstracts_num.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/author_papers.txt\") as f:\n",
    "    authors_papers=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=vectors.shape[1]-1\n",
    "authors_vectors=np.zeros((len(authors_papers), n_dim+1), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1510273386', '1827736641', '1588673897', '2252711322', '2123653597']\n",
      "1510273386\n",
      "58046\n"
     ]
    }
   ],
   "source": [
    "papers=authors_papers[0].split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "print(papers)\n",
    "p=papers[0]\n",
    "print(int(p))\n",
    "print(id_abstracts_num.get(int(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880\n"
     ]
    }
   ],
   "source": [
    "s=0\n",
    "for i,author in enumerate(authors_papers):\n",
    "    papers=author.split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "    vector=np.zeros(n_dim)\n",
    "    no_fail=False\n",
    "    for p in papers:\n",
    "        try:\n",
    "            vector+=vectors[id_abstracts_num[int(p)], 1:]\n",
    "            no_fail=True\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if (not no_fail):\n",
    "        s+=1\n",
    "    authors_vectors[i][0]=int(author.split(\":\")[0])\n",
    "    if (np.linalg.norm(vector)>0):\n",
    "        vector=vector/np.linalg.norm(vector)\n",
    "    authors_vectors[i][1:]=vector.copy()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Global/authors_vectors.npy\", authors_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_improved=np.load(\"DeepWalk/embeddings_improved.npy\")\n",
    "authors_vectors=np.load(\"Global/authors_vectors.npy\")\n",
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217801, 135)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_improved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217801, 135)\n",
      "(217801, 151)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_improved.shape)\n",
    "print(authors_vectors.shape)\n",
    "\n",
    "auth_vec_num_id_author=dict([(a,b) for a,b in enumerate(authors_vectors[:,0])])\n",
    "id_author_auth_vec_num=dict([(b,a) for a,b in enumerate(authors_vectors[:,0])])\n",
    "\n",
    "graph_num_id_author=dict([(a,b) for a,b in enumerate(G.nodes)])\n",
    "id_author_graph_num=dict([(b,a) for a,b in enumerate(G.nodes)])\n",
    "\n",
    "emb_num_id_author=dict([(a,int(b)) for a,b in enumerate(embeddings_improved[:,0])])\n",
    "id_author_emb_num=dict([(int(b),a) for a,b in enumerate(embeddings_improved[:,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=G.number_of_nodes()\n",
    "n_emb=embeddings_improved.shape[1]-1\n",
    "n_abs=authors_vectors.shape[1]-1\n",
    "n_dim_tot=1+n_emb+n_abs\n",
    "full_matrix=np.zeros((n_nodes, n_dim_tot), dtype=np.float64)\n",
    "for i in range(n_nodes):\n",
    "    node=graph_num_id_author[i]\n",
    "    full_matrix[i,0]=node\n",
    "    full_matrix[i,1:1+n_emb]=embeddings_improved[id_author_emb_num[node],1:].copy()\n",
    "    full_matrix[i,1+n_emb:]=authors_vectors[id_author_auth_vec_num[node],1:].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Global/full_embedding_matrix.npy\", full_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217801, 285)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP model\"\"\"\n",
    "    def __init__(self, n_feat, n_hidden_1, n_hidden_2, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        z0 = self.relu(self.fc1(x))\n",
    "        z0 = self.dropout(z0)\n",
    "        z1 = self.relu(self.fc2(z0))\n",
    "        out = self.fc3(z1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    \"\"\"More complex MLP model\"\"\"\n",
    "    def __init__(self, n_feat, n_hidden_1, n_hidden_2,n_hidden_3, dropout):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.fc4=nn.Linear(n_hidden_3,1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z0 = self.relu(self.fc1(x))\n",
    "        z0 = self.dropout(z0)\n",
    "        z1 = self.relu(self.fc2(z0))\n",
    "        z1 = self.dropout(z1)\n",
    "        z2=self.relu(self.fc3(z1))\n",
    "        out = self.fc4(z2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X,Y):\n",
    "    if (len(X)!=len(Y)):\n",
    "        print(\"Sizes not identical\")\n",
    "        return -1\n",
    "    return (X-Y)@(X-Y) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'author': np.int64})\n",
    "full_embedding=np.load(\"Global/full_embedding_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_nodeID_Train=dict(df_train[\"author\"])\n",
    "nodeID_abs_Train=dict([(b,a) for a,b in abs_nodeID_Train.items()])\n",
    "\n",
    "abs_nodeID_Test=dict(df_test[\"author\"])\n",
    "nodeID_abs_Test=dict([(b,a) for a,b in abs_nodeID_Test.items()])\n",
    "\n",
    "abs_hindex_Train=dict(df_train[\"hindex\"])\n",
    "\n",
    "abs_nodeID_Graph=dict(enumerate(G.nodes))\n",
    "nodeID_abs_Graph=dict([(b,a) for a,b in enumerate(G.nodes)])\n",
    "\n",
    "n=G.number_of_nodes()\n",
    "n_train=abs_nodeID_Train.__len__()\n",
    "n_test=abs_nodeID_Test.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Careful, those indexes are related to the TRAIN set, not to the global graph indexing\n",
    "idx=np.random.permutation(n_train)\n",
    "idx_train=idx[:int(0.8*n_train)]\n",
    "idx_val=idx[int(0.8*n_train):]\n",
    "\n",
    "nodes_train=[abs_nodeID_Train[i] for i in idx_train]\n",
    "nodes_val=[abs_nodeID_Train[i] for i in idx_val]\n",
    "\n",
    "X_train_x = torch.tensor([full_embedding[nodeID_abs_Graph[node]][1:] for node in nodes_train], dtype=torch.float32)\n",
    "X_val_x = torch.tensor([full_embedding[nodeID_abs_Graph[node]][1:] for node in nodes_val], dtype=torch.float32)\n",
    "\n",
    "hindex_train_x=torch.tensor([abs_hindex_Train[i] for i in idx_train], dtype=torch.float32)\n",
    "hindex_val_x=torch.tensor([abs_hindex_Train[i] for i in idx_val], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Training on split set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=X_train_x.shape[1]\n",
    "model=MLP(n_dim,256,64,0.2)\n",
    "#model=MLP2(n_dim,256,128,64,0.6)\n",
    "\n",
    "loss_vals=[]\n",
    "loss_trains=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 53883.6641 loss_val: 29013512.0000\n",
      "Epoch: 002 loss_train: 27094364.0000 loss_val: 1078400.6250\n",
      "Epoch: 003 loss_train: 1117576.8750 loss_val: 30923.2969\n",
      "Epoch: 004 loss_train: 24864.4648 loss_val: 59048.1562\n",
      "Epoch: 005 loss_train: 54996.7812 loss_val: 79797.7188\n",
      "Epoch: 006 loss_train: 74113.1719 loss_val: 84416.3984\n",
      "Epoch: 007 loss_train: 78342.0078 loss_val: 80666.8672\n",
      "Epoch: 008 loss_train: 78557.6953 loss_val: 64537.2695\n",
      "Epoch: 009 loss_train: 60896.5664 loss_val: 44886.8281\n",
      "Epoch: 010 loss_train: 40921.5508 loss_val: 20530.0410\n",
      "Epoch: 011 loss_train: 19272.8457 loss_val: 1134.0271\n",
      "Epoch: 012 loss_train: 3252.9514 loss_val: 14799.6631\n",
      "Epoch: 013 loss_train: 17218.8320 loss_val: 18569.6348\n",
      "Epoch: 014 loss_train: 20717.2012 loss_val: 9025.2148\n",
      "Epoch: 015 loss_train: 10913.2617 loss_val: 2243.8005\n",
      "Epoch: 016 loss_train: 3605.7087 loss_val: 315.3615\n",
      "Epoch: 017 loss_train: 998.4036 loss_val: 294.2816\n",
      "Epoch: 018 loss_train: 767.4999 loss_val: 609.1317\n",
      "Epoch: 019 loss_train: 927.2397 loss_val: 680.5611\n",
      "Epoch: 020 loss_train: 997.4270 loss_val: 480.9253\n",
      "Epoch: 021 loss_train: 899.5054 loss_val: 296.7440\n",
      "Epoch: 022 loss_train: 684.4467 loss_val: 210.7310\n",
      "Epoch: 023 loss_train: 545.9230 loss_val: 199.3502\n",
      "Epoch: 024 loss_train: 394.5915 loss_val: 217.6554\n",
      "Epoch: 025 loss_train: 304.0900 loss_val: 217.4135\n",
      "Epoch: 026 loss_train: 251.4218 loss_val: 216.8588\n",
      "Epoch: 027 loss_train: 235.9264 loss_val: 216.1923\n",
      "Epoch: 028 loss_train: 228.3894 loss_val: 215.5983\n",
      "Epoch: 029 loss_train: 224.7469 loss_val: 214.9739\n",
      "Epoch: 030 loss_train: 222.3778 loss_val: 214.3423\n",
      "Epoch: 031 loss_train: 220.5926 loss_val: 213.6422\n",
      "Epoch: 032 loss_train: 219.9847 loss_val: 212.8167\n",
      "Epoch: 033 loss_train: 218.5233 loss_val: 211.8792\n",
      "Epoch: 034 loss_train: 218.9636 loss_val: 210.8242\n",
      "Epoch: 035 loss_train: 217.1629 loss_val: 209.7092\n",
      "Epoch: 036 loss_train: 215.3869 loss_val: 208.5015\n",
      "Epoch: 037 loss_train: 214.2923 loss_val: 207.2163\n",
      "Epoch: 038 loss_train: 212.8296 loss_val: 205.8731\n",
      "Epoch: 039 loss_train: 219.3632 loss_val: 205.0773\n",
      "Epoch: 040 loss_train: 212.8969 loss_val: 204.2595\n",
      "Epoch: 041 loss_train: 210.2358 loss_val: 203.3053\n",
      "Epoch: 042 loss_train: 209.1908 loss_val: 202.2181\n",
      "Epoch: 043 loss_train: 209.2702 loss_val: 201.2759\n",
      "Epoch: 044 loss_train: 207.5477 loss_val: 200.2687\n",
      "Epoch: 045 loss_train: 207.1672 loss_val: 199.2240\n",
      "Epoch: 046 loss_train: 205.2563 loss_val: 197.9764\n",
      "Epoch: 047 loss_train: 204.0426 loss_val: 196.5404\n",
      "Epoch: 048 loss_train: 203.3818 loss_val: 195.1074\n",
      "Epoch: 049 loss_train: 205.4250 loss_val: 194.2781\n",
      "Epoch: 050 loss_train: 202.8658 loss_val: 193.7812\n",
      "Epoch: 051 loss_train: 200.0378 loss_val: 193.0299\n",
      "Epoch: 052 loss_train: 199.2544 loss_val: 191.9930\n",
      "Epoch: 053 loss_train: 198.5810 loss_val: 190.6203\n",
      "Epoch: 054 loss_train: 200.1570 loss_val: 189.2832\n",
      "Epoch: 055 loss_train: 196.2923 loss_val: 187.1563\n",
      "Epoch: 056 loss_train: 194.4103 loss_val: 184.4540\n",
      "Epoch: 057 loss_train: 192.0723 loss_val: 180.8964\n",
      "Epoch: 058 loss_train: 196.8525 loss_val: 182.0028\n",
      "Epoch: 059 loss_train: 189.2431 loss_val: 182.1151\n",
      "Epoch: 060 loss_train: 189.6686 loss_val: 181.6565\n",
      "Epoch: 061 loss_train: 189.4484 loss_val: 180.8009\n",
      "Epoch: 062 loss_train: 189.3561 loss_val: 179.7624\n",
      "Epoch: 063 loss_train: 188.3414 loss_val: 178.6300\n",
      "Epoch: 064 loss_train: 187.3842 loss_val: 177.2785\n",
      "Epoch: 065 loss_train: 185.3173 loss_val: 175.7359\n",
      "Epoch: 066 loss_train: 183.6300 loss_val: 173.9160\n",
      "Epoch: 067 loss_train: 180.9608 loss_val: 171.6038\n",
      "Epoch: 068 loss_train: 176.9390 loss_val: 168.6709\n",
      "Epoch: 069 loss_train: 171.9176 loss_val: 165.0332\n",
      "Epoch: 070 loss_train: 167.4816 loss_val: 152.8384\n",
      "Epoch: 071 loss_train: 165.6681 loss_val: 146.5050\n",
      "Epoch: 072 loss_train: 159.3496 loss_val: 142.3554\n",
      "Epoch: 073 loss_train: 156.3845 loss_val: 152.8979\n",
      "Epoch: 074 loss_train: 157.7347 loss_val: 153.3778\n",
      "Epoch: 075 loss_train: 155.8059 loss_val: 147.5474\n",
      "Epoch: 076 loss_train: 154.7829 loss_val: 139.9895\n",
      "Epoch: 077 loss_train: 153.0530 loss_val: 137.2272\n",
      "Epoch: 078 loss_train: 161.9150 loss_val: 149.5577\n",
      "Epoch: 079 loss_train: 153.1910 loss_val: 153.9132\n",
      "Epoch: 080 loss_train: 156.7443 loss_val: 152.0408\n",
      "Epoch: 081 loss_train: 156.2972 loss_val: 147.7576\n",
      "Epoch: 082 loss_train: 153.8230 loss_val: 142.3441\n",
      "Epoch: 083 loss_train: 152.3566 loss_val: 136.3702\n",
      "Epoch: 084 loss_train: 149.5242 loss_val: 130.6759\n",
      "Epoch: 085 loss_train: 148.7025 loss_val: 127.7002\n",
      "Epoch: 086 loss_train: 147.9109 loss_val: 128.5183\n",
      "Epoch: 087 loss_train: 144.4433 loss_val: 129.0693\n",
      "Epoch: 088 loss_train: 142.4205 loss_val: 128.8139\n",
      "Epoch: 089 loss_train: 143.3330 loss_val: 129.2625\n",
      "Epoch: 090 loss_train: 141.1247 loss_val: 127.5061\n",
      "Epoch: 091 loss_train: 140.6532 loss_val: 124.7194\n",
      "Epoch: 092 loss_train: 138.7806 loss_val: 123.2605\n",
      "Epoch: 093 loss_train: 138.9809 loss_val: 122.9958\n",
      "Epoch: 094 loss_train: 136.5132 loss_val: 122.1866\n",
      "Epoch: 095 loss_train: 136.6304 loss_val: 120.9925\n",
      "Epoch: 096 loss_train: 134.8653 loss_val: 119.9469\n",
      "Epoch: 097 loss_train: 134.8449 loss_val: 119.9878\n",
      "Epoch: 098 loss_train: 134.6596 loss_val: 120.1722\n",
      "Epoch: 099 loss_train: 133.4723 loss_val: 118.7246\n",
      "Epoch: 100 loss_train: 132.0829 loss_val: 116.4464\n",
      "Epoch: 101 loss_train: 131.4732 loss_val: 115.6935\n",
      "Epoch: 102 loss_train: 131.3970 loss_val: 116.1675\n",
      "Epoch: 103 loss_train: 130.5228 loss_val: 117.3280\n",
      "Epoch: 104 loss_train: 130.9672 loss_val: 118.2000\n",
      "Epoch: 105 loss_train: 130.6928 loss_val: 114.9908\n",
      "Epoch: 106 loss_train: 128.4042 loss_val: 111.6034\n",
      "Epoch: 107 loss_train: 131.7594 loss_val: 117.2917\n",
      "Epoch: 108 loss_train: 129.6241 loss_val: 120.1179\n",
      "Epoch: 109 loss_train: 135.5431 loss_val: 119.1719\n",
      "Epoch: 110 loss_train: 131.0315 loss_val: 115.4350\n",
      "Epoch: 111 loss_train: 129.7983 loss_val: 111.7170\n",
      "Epoch: 112 loss_train: 127.9789 loss_val: 110.5774\n",
      "Epoch: 113 loss_train: 126.5771 loss_val: 112.5989\n",
      "Epoch: 114 loss_train: 127.6639 loss_val: 114.0734\n",
      "Epoch: 115 loss_train: 128.2195 loss_val: 111.4020\n",
      "Epoch: 116 loss_train: 124.9486 loss_val: 108.4786\n",
      "Epoch: 117 loss_train: 123.6468 loss_val: 108.4336\n",
      "Epoch: 118 loss_train: 124.0846 loss_val: 109.7564\n",
      "Epoch: 119 loss_train: 124.1827 loss_val: 110.9358\n",
      "Epoch: 120 loss_train: 124.1045 loss_val: 109.5335\n",
      "Epoch: 121 loss_train: 122.1008 loss_val: 106.0574\n",
      "Epoch: 122 loss_train: 121.5327 loss_val: 104.9495\n",
      "Epoch: 123 loss_train: 121.8588 loss_val: 105.0727\n",
      "Epoch: 124 loss_train: 122.0785 loss_val: 108.9278\n",
      "Epoch: 125 loss_train: 120.9840 loss_val: 109.1833\n",
      "Epoch: 126 loss_train: 121.5261 loss_val: 106.5858\n",
      "Epoch: 127 loss_train: 120.0154 loss_val: 104.6681\n",
      "Epoch: 128 loss_train: 120.6911 loss_val: 104.5797\n",
      "Epoch: 129 loss_train: 120.8344 loss_val: 105.8499\n",
      "Epoch: 130 loss_train: 118.9159 loss_val: 104.7054\n",
      "Epoch: 131 loss_train: 117.8125 loss_val: 101.9426\n",
      "Epoch: 132 loss_train: 118.7120 loss_val: 101.8346\n",
      "Epoch: 133 loss_train: 117.6548 loss_val: 102.8337\n",
      "Epoch: 134 loss_train: 116.2255 loss_val: 103.2672\n",
      "Epoch: 135 loss_train: 116.4059 loss_val: 102.0712\n",
      "Epoch: 136 loss_train: 115.1441 loss_val: 101.2073\n",
      "Epoch: 137 loss_train: 115.6069 loss_val: 101.6722\n",
      "Epoch: 138 loss_train: 116.2953 loss_val: 103.1147\n",
      "Epoch: 139 loss_train: 116.0057 loss_val: 102.8157\n",
      "Epoch: 140 loss_train: 114.8716 loss_val: 100.3665\n",
      "Epoch: 141 loss_train: 114.4708 loss_val: 98.6012\n",
      "Epoch: 142 loss_train: 114.4637 loss_val: 99.8987\n",
      "Epoch: 143 loss_train: 111.8482 loss_val: 100.8489\n",
      "Epoch: 144 loss_train: 115.1595 loss_val: 100.6300\n",
      "Epoch: 145 loss_train: 112.3483 loss_val: 98.8587\n",
      "Epoch: 146 loss_train: 112.8452 loss_val: 98.1358\n",
      "Epoch: 147 loss_train: 111.9368 loss_val: 98.1725\n",
      "Epoch: 148 loss_train: 112.5414 loss_val: 99.2119\n",
      "Epoch: 149 loss_train: 111.5608 loss_val: 99.6466\n",
      "Epoch: 150 loss_train: 112.4716 loss_val: 98.6353\n",
      "Epoch: 151 loss_train: 111.6318 loss_val: 97.3888\n",
      "Epoch: 152 loss_train: 110.8865 loss_val: 97.5360\n",
      "Epoch: 153 loss_train: 110.5436 loss_val: 97.3496\n",
      "Epoch: 154 loss_train: 110.8785 loss_val: 97.7601\n",
      "Epoch: 155 loss_train: 109.4678 loss_val: 96.9080\n",
      "Epoch: 156 loss_train: 109.2503 loss_val: 95.7757\n",
      "Epoch: 157 loss_train: 109.3380 loss_val: 95.4447\n",
      "Epoch: 158 loss_train: 109.4054 loss_val: 96.0511\n",
      "Epoch: 159 loss_train: 108.6418 loss_val: 96.3310\n",
      "Epoch: 160 loss_train: 108.7064 loss_val: 95.6321\n",
      "Epoch: 161 loss_train: 108.7801 loss_val: 94.2506\n",
      "Epoch: 162 loss_train: 108.4336 loss_val: 93.9014\n",
      "Epoch: 163 loss_train: 108.4191 loss_val: 95.5002\n",
      "Epoch: 164 loss_train: 107.7816 loss_val: 96.0940\n",
      "Epoch: 165 loss_train: 108.6798 loss_val: 94.9071\n",
      "Epoch: 166 loss_train: 107.9456 loss_val: 93.8182\n",
      "Epoch: 167 loss_train: 107.0431 loss_val: 93.1394\n",
      "Epoch: 168 loss_train: 107.4962 loss_val: 94.2696\n",
      "Epoch: 169 loss_train: 106.3160 loss_val: 94.8790\n",
      "Epoch: 170 loss_train: 105.7926 loss_val: 93.2810\n",
      "Epoch: 171 loss_train: 107.2336 loss_val: 92.0618\n",
      "Epoch: 172 loss_train: 106.2676 loss_val: 92.4268\n",
      "Epoch: 173 loss_train: 106.0155 loss_val: 94.8573\n",
      "Epoch: 174 loss_train: 106.2988 loss_val: 93.3832\n",
      "Epoch: 175 loss_train: 106.4072 loss_val: 90.9330\n",
      "Epoch: 176 loss_train: 105.6526 loss_val: 91.0607\n",
      "Epoch: 177 loss_train: 105.5096 loss_val: 95.1476\n",
      "Epoch: 178 loss_train: 106.2587 loss_val: 94.2113\n",
      "Epoch: 179 loss_train: 105.4945 loss_val: 91.0833\n",
      "Epoch: 180 loss_train: 104.3050 loss_val: 90.3581\n",
      "Epoch: 181 loss_train: 104.4191 loss_val: 90.4444\n",
      "Epoch: 182 loss_train: 104.1078 loss_val: 92.8307\n",
      "Epoch: 183 loss_train: 104.3383 loss_val: 92.2192\n",
      "Epoch: 184 loss_train: 103.7172 loss_val: 88.8055\n",
      "Epoch: 185 loss_train: 105.9289 loss_val: 92.2157\n",
      "Epoch: 186 loss_train: 103.8830 loss_val: 91.7766\n",
      "Epoch: 187 loss_train: 104.5097 loss_val: 91.6807\n",
      "Epoch: 188 loss_train: 103.8226 loss_val: 92.0473\n",
      "Epoch: 189 loss_train: 104.4574 loss_val: 90.6007\n",
      "Epoch: 190 loss_train: 103.3061 loss_val: 89.5861\n",
      "Epoch: 191 loss_train: 103.8323 loss_val: 90.0130\n",
      "Epoch: 192 loss_train: 104.3263 loss_val: 89.1668\n",
      "Epoch: 193 loss_train: 103.3927 loss_val: 89.3388\n",
      "Epoch: 194 loss_train: 102.8432 loss_val: 91.2838\n",
      "Epoch: 195 loss_train: 102.8938 loss_val: 89.6020\n",
      "Epoch: 196 loss_train: 105.8430 loss_val: 97.5080\n",
      "Epoch: 197 loss_train: 107.0565 loss_val: 93.0074\n",
      "Epoch: 198 loss_train: 104.8413 loss_val: 90.0097\n",
      "Epoch: 199 loss_train: 107.2859 loss_val: 89.3573\n",
      "Epoch: 200 loss_train: 104.4967 loss_val: 95.6880\n",
      "Epoch: 001 loss_train: 105.6333 loss_val: 1334.4009\n",
      "Epoch: 002 loss_train: 4077.5793 loss_val: 107.7219\n",
      "Epoch: 003 loss_train: 147.9235 loss_val: 175.8314\n",
      "Epoch: 004 loss_train: 183.0533 loss_val: 324.0821\n",
      "Epoch: 005 loss_train: 358.8343 loss_val: 350.9985\n",
      "Epoch: 006 loss_train: 420.0881 loss_val: 284.6840\n",
      "Epoch: 007 loss_train: 369.9055 loss_val: 223.2376\n",
      "Epoch: 008 loss_train: 275.1904 loss_val: 181.6254\n",
      "Epoch: 009 loss_train: 204.4107 loss_val: 165.9073\n",
      "Epoch: 010 loss_train: 178.6408 loss_val: 173.4954\n",
      "Epoch: 011 loss_train: 183.8946 loss_val: 191.2364\n",
      "Epoch: 012 loss_train: 201.7826 loss_val: 204.1994\n",
      "Epoch: 013 loss_train: 215.8884 loss_val: 204.5469\n",
      "Epoch: 014 loss_train: 216.3989 loss_val: 193.3156\n",
      "Epoch: 015 loss_train: 207.4804 loss_val: 176.6499\n",
      "Epoch: 016 loss_train: 187.0668 loss_val: 161.1586\n",
      "Epoch: 017 loss_train: 170.4969 loss_val: 151.3038\n",
      "Epoch: 018 loss_train: 159.5666 loss_val: 148.2151\n",
      "Epoch: 019 loss_train: 156.0790 loss_val: 150.4690\n",
      "Epoch: 020 loss_train: 157.9475 loss_val: 155.1857\n",
      "Epoch: 021 loss_train: 162.6337 loss_val: 159.4415\n",
      "Epoch: 022 loss_train: 166.6216 loss_val: 161.2311\n",
      "Epoch: 023 loss_train: 169.0621 loss_val: 159.9092\n",
      "Epoch: 024 loss_train: 167.7203 loss_val: 156.1081\n",
      "Epoch: 025 loss_train: 164.3917 loss_val: 151.1618\n",
      "Epoch: 026 loss_train: 159.1056 loss_val: 146.5258\n",
      "Epoch: 027 loss_train: 155.0451 loss_val: 143.1945\n",
      "Epoch: 028 loss_train: 152.6126 loss_val: 141.5468\n",
      "Epoch: 029 loss_train: 152.8271 loss_val: 141.9262\n",
      "Epoch: 030 loss_train: 152.6528 loss_val: 142.9895\n",
      "Epoch: 031 loss_train: 152.6598 loss_val: 143.5679\n",
      "Epoch: 032 loss_train: 156.6691 loss_val: 144.0632\n",
      "Epoch: 033 loss_train: 153.5630 loss_val: 143.6129\n",
      "Epoch: 034 loss_train: 153.4588 loss_val: 142.7872\n",
      "Epoch: 035 loss_train: 152.2887 loss_val: 141.9355\n",
      "Epoch: 036 loss_train: 150.2057 loss_val: 141.3845\n",
      "Epoch: 037 loss_train: 150.4246 loss_val: 141.1977\n",
      "Epoch: 038 loss_train: 149.1615 loss_val: 141.2696\n",
      "Epoch: 039 loss_train: 149.5798 loss_val: 141.3969\n",
      "Epoch: 040 loss_train: 148.8830 loss_val: 141.2382\n",
      "Epoch: 041 loss_train: 148.7505 loss_val: 140.5182\n",
      "Epoch: 042 loss_train: 148.1675 loss_val: 138.7268\n",
      "Epoch: 043 loss_train: 145.7927 loss_val: 135.6153\n",
      "Epoch: 044 loss_train: 171.7286 loss_val: 138.3540\n",
      "Epoch: 045 loss_train: 145.7763 loss_val: 140.0547\n",
      "Epoch: 046 loss_train: 147.1101 loss_val: 141.0579\n",
      "Epoch: 047 loss_train: 148.1001 loss_val: 141.6224\n",
      "Epoch: 048 loss_train: 148.8524 loss_val: 141.9169\n",
      "Epoch: 049 loss_train: 149.1812 loss_val: 142.0853\n",
      "Epoch: 050 loss_train: 149.3951 loss_val: 142.1471\n",
      "Epoch: 051 loss_train: 149.6004 loss_val: 142.0519\n",
      "Epoch: 052 loss_train: 149.2890 loss_val: 141.7459\n",
      "Epoch: 053 loss_train: 149.0827 loss_val: 141.2060\n",
      "Epoch: 054 loss_train: 148.3166 loss_val: 140.4344\n",
      "Epoch: 055 loss_train: 147.9689 loss_val: 139.3978\n",
      "Epoch: 056 loss_train: 147.2662 loss_val: 138.7511\n",
      "Epoch: 057 loss_train: 147.9642 loss_val: 138.9823\n",
      "Epoch: 058 loss_train: 149.3749 loss_val: 140.1421\n",
      "Epoch: 059 loss_train: 147.4494 loss_val: 140.6272\n",
      "Epoch: 060 loss_train: 147.7919 loss_val: 140.7199\n",
      "Epoch: 061 loss_train: 147.7957 loss_val: 140.5268\n",
      "Epoch: 062 loss_train: 147.2659 loss_val: 140.1360\n",
      "Epoch: 063 loss_train: 147.4896 loss_val: 139.6491\n",
      "Epoch: 064 loss_train: 146.5657 loss_val: 139.1011\n",
      "Epoch: 065 loss_train: 146.6600 loss_val: 138.5375\n",
      "Epoch: 066 loss_train: 145.8139 loss_val: 138.0406\n",
      "Epoch: 067 loss_train: 145.7537 loss_val: 137.5490\n",
      "Epoch: 068 loss_train: 145.3288 loss_val: 137.0089\n",
      "Epoch: 069 loss_train: 144.6811 loss_val: 136.4497\n",
      "Epoch: 070 loss_train: 144.7780 loss_val: 135.9563\n",
      "Epoch: 071 loss_train: 143.3823 loss_val: 135.4379\n",
      "Epoch: 072 loss_train: 143.1639 loss_val: 134.9674\n",
      "Epoch: 073 loss_train: 142.4115 loss_val: 134.4175\n",
      "Epoch: 074 loss_train: 143.2264 loss_val: 134.1876\n",
      "Epoch: 075 loss_train: 143.2181 loss_val: 134.2327\n",
      "Epoch: 076 loss_train: 143.7244 loss_val: 134.3647\n",
      "Epoch: 077 loss_train: 142.0492 loss_val: 134.4862\n",
      "Epoch: 078 loss_train: 141.7649 loss_val: 134.4802\n",
      "Epoch: 079 loss_train: 141.3390 loss_val: 134.3385\n",
      "Epoch: 080 loss_train: 142.6439 loss_val: 134.1412\n",
      "Epoch: 081 loss_train: 141.0684 loss_val: 133.8569\n",
      "Epoch: 082 loss_train: 141.3363 loss_val: 133.4954\n",
      "Epoch: 083 loss_train: 140.9667 loss_val: 133.0847\n",
      "Epoch: 084 loss_train: 140.5438 loss_val: 132.5544\n",
      "Epoch: 085 loss_train: 139.7236 loss_val: 131.8782\n",
      "Epoch: 086 loss_train: 139.2692 loss_val: 131.0658\n",
      "Epoch: 087 loss_train: 138.2573 loss_val: 130.1398\n",
      "Epoch: 088 loss_train: 139.5640 loss_val: 130.7279\n",
      "Epoch: 089 loss_train: 138.5068 loss_val: 131.3596\n",
      "Epoch: 090 loss_train: 138.1470 loss_val: 131.7206\n",
      "Epoch: 091 loss_train: 138.9105 loss_val: 131.6965\n",
      "Epoch: 092 loss_train: 138.8724 loss_val: 131.3070\n",
      "Epoch: 093 loss_train: 137.6055 loss_val: 130.6884\n",
      "Epoch: 094 loss_train: 137.4353 loss_val: 129.9397\n",
      "Epoch: 095 loss_train: 137.1753 loss_val: 129.1586\n",
      "Epoch: 096 loss_train: 136.0806 loss_val: 128.2195\n",
      "Epoch: 097 loss_train: 135.1640 loss_val: 127.0716\n",
      "Epoch: 098 loss_train: 134.9698 loss_val: 127.4312\n",
      "Epoch: 099 loss_train: 134.1257 loss_val: 127.8341\n",
      "Epoch: 100 loss_train: 134.2230 loss_val: 127.6739\n",
      "Epoch: 101 loss_train: 134.1213 loss_val: 127.2992\n",
      "Epoch: 102 loss_train: 134.0774 loss_val: 126.3048\n",
      "Epoch: 103 loss_train: 132.7643 loss_val: 124.5016\n",
      "Epoch: 104 loss_train: 133.6644 loss_val: 127.0392\n",
      "Epoch: 105 loss_train: 133.7407 loss_val: 128.5155\n",
      "Epoch: 106 loss_train: 134.9781 loss_val: 129.4286\n",
      "Epoch: 107 loss_train: 136.5453 loss_val: 129.8977\n",
      "Epoch: 108 loss_train: 136.3712 loss_val: 129.9392\n",
      "Epoch: 109 loss_train: 136.4364 loss_val: 129.6745\n",
      "Epoch: 110 loss_train: 136.3422 loss_val: 129.1844\n",
      "Epoch: 111 loss_train: 136.2427 loss_val: 128.5397\n",
      "Epoch: 112 loss_train: 135.8981 loss_val: 127.8351\n",
      "Epoch: 113 loss_train: 135.1493 loss_val: 127.1838\n",
      "Epoch: 114 loss_train: 134.2505 loss_val: 126.5776\n",
      "Epoch: 115 loss_train: 135.1808 loss_val: 126.3237\n",
      "Epoch: 116 loss_train: 133.4578 loss_val: 125.6907\n",
      "Epoch: 117 loss_train: 132.8855 loss_val: 124.8092\n",
      "Epoch: 118 loss_train: 134.7309 loss_val: 126.8619\n",
      "Epoch: 119 loss_train: 133.3854 loss_val: 128.2747\n",
      "Epoch: 120 loss_train: 134.8434 loss_val: 129.3776\n",
      "Epoch: 121 loss_train: 136.0860 loss_val: 130.0150\n",
      "Epoch: 122 loss_train: 136.5776 loss_val: 130.2500\n",
      "Epoch: 123 loss_train: 136.3976 loss_val: 130.1501\n",
      "Epoch: 124 loss_train: 136.2615 loss_val: 129.8251\n",
      "Epoch: 125 loss_train: 136.2468 loss_val: 129.3366\n",
      "Epoch: 126 loss_train: 135.6563 loss_val: 128.7657\n",
      "Epoch: 127 loss_train: 135.6548 loss_val: 128.1693\n",
      "Epoch: 128 loss_train: 134.7986 loss_val: 127.5696\n",
      "Epoch: 129 loss_train: 134.3218 loss_val: 126.9237\n",
      "Epoch: 130 loss_train: 139.2596 loss_val: 126.2609\n",
      "Epoch: 131 loss_train: 132.8413 loss_val: 124.9271\n",
      "Epoch: 132 loss_train: 130.9246 loss_val: 122.7856\n",
      "Epoch: 133 loss_train: 156.8496 loss_val: 131.1221\n",
      "Epoch: 134 loss_train: 137.3456 loss_val: 138.4898\n",
      "Epoch: 135 loss_train: 144.4138 loss_val: 144.1102\n",
      "Epoch: 136 loss_train: 150.8215 loss_val: 147.6475\n",
      "Epoch: 137 loss_train: 154.6768 loss_val: 149.4967\n",
      "Epoch: 138 loss_train: 156.4045 loss_val: 150.1245\n",
      "Epoch: 139 loss_train: 157.3806 loss_val: 149.4113\n",
      "Epoch: 140 loss_train: 156.6919 loss_val: 147.3661\n",
      "Epoch: 141 loss_train: 154.3779 loss_val: 144.5050\n",
      "Epoch: 142 loss_train: 151.3823 loss_val: 141.6138\n",
      "Epoch: 143 loss_train: 148.3037 loss_val: 139.3335\n",
      "Epoch: 144 loss_train: 145.6471 loss_val: 137.8774\n",
      "Epoch: 145 loss_train: 144.5532 loss_val: 136.8627\n",
      "Epoch: 146 loss_train: 143.5474 loss_val: 135.7509\n",
      "Epoch: 147 loss_train: 142.6241 loss_val: 134.5556\n",
      "Epoch: 148 loss_train: 141.3256 loss_val: 133.6031\n",
      "Epoch: 149 loss_train: 142.0666 loss_val: 133.0471\n",
      "Epoch: 150 loss_train: 141.1290 loss_val: 132.7537\n",
      "Epoch: 151 loss_train: 144.5133 loss_val: 132.6508\n",
      "Epoch: 152 loss_train: 142.3098 loss_val: 132.6689\n",
      "Epoch: 153 loss_train: 141.8052 loss_val: 132.8787\n",
      "Epoch: 154 loss_train: 140.5774 loss_val: 133.2337\n",
      "Epoch: 155 loss_train: 140.4802 loss_val: 133.5485\n",
      "Epoch: 156 loss_train: 140.4688 loss_val: 133.6697\n",
      "Epoch: 157 loss_train: 140.8860 loss_val: 133.6618\n",
      "Epoch: 158 loss_train: 140.5708 loss_val: 133.4798\n",
      "Epoch: 159 loss_train: 140.1599 loss_val: 133.0463\n",
      "Epoch: 160 loss_train: 139.3218 loss_val: 132.5605\n",
      "Epoch: 161 loss_train: 139.7019 loss_val: 132.0748\n",
      "Epoch: 162 loss_train: 142.8432 loss_val: 132.2141\n",
      "Epoch: 163 loss_train: 139.3916 loss_val: 132.3813\n",
      "Epoch: 164 loss_train: 139.3533 loss_val: 132.5523\n",
      "Epoch: 165 loss_train: 141.1233 loss_val: 133.0125\n",
      "Epoch: 166 loss_train: 139.4837 loss_val: 133.1494\n",
      "Epoch: 167 loss_train: 139.3005 loss_val: 132.8514\n",
      "Epoch: 168 loss_train: 139.1112 loss_val: 132.3309\n",
      "Epoch: 169 loss_train: 138.9073 loss_val: 131.7949\n",
      "Epoch: 170 loss_train: 138.5096 loss_val: 131.3683\n",
      "Epoch: 171 loss_train: 138.3612 loss_val: 130.9800\n",
      "Epoch: 172 loss_train: 137.9742 loss_val: 130.5963\n",
      "Epoch: 173 loss_train: 137.7639 loss_val: 130.1997\n",
      "Epoch: 174 loss_train: 137.7372 loss_val: 129.9394\n",
      "Epoch: 175 loss_train: 137.4497 loss_val: 129.9166\n",
      "Epoch: 176 loss_train: 138.9083 loss_val: 130.3764\n",
      "Epoch: 177 loss_train: 138.0539 loss_val: 130.8322\n",
      "Epoch: 178 loss_train: 137.1879 loss_val: 131.0083\n",
      "Epoch: 179 loss_train: 137.1236 loss_val: 131.0110\n",
      "Epoch: 180 loss_train: 137.3263 loss_val: 131.0032\n",
      "Epoch: 181 loss_train: 137.6670 loss_val: 130.9716\n",
      "Epoch: 182 loss_train: 137.2819 loss_val: 130.7623\n",
      "Epoch: 183 loss_train: 136.9455 loss_val: 130.3433\n",
      "Epoch: 184 loss_train: 136.6358 loss_val: 129.8394\n",
      "Epoch: 185 loss_train: 137.1799 loss_val: 129.8030\n",
      "Epoch: 186 loss_train: 137.1233 loss_val: 129.9693\n",
      "Epoch: 187 loss_train: 136.1525 loss_val: 129.8289\n",
      "Epoch: 188 loss_train: 136.0786 loss_val: 129.3610\n",
      "Epoch: 189 loss_train: 135.4493 loss_val: 128.7247\n",
      "Epoch: 190 loss_train: 135.1320 loss_val: 128.1270\n",
      "Epoch: 191 loss_train: 134.6951 loss_val: 127.6003\n",
      "Epoch: 192 loss_train: 134.7374 loss_val: 127.1961\n",
      "Epoch: 193 loss_train: 138.1359 loss_val: 127.9984\n",
      "Epoch: 194 loss_train: 133.8785 loss_val: 129.0980\n",
      "Epoch: 195 loss_train: 135.2567 loss_val: 130.0707\n",
      "Epoch: 196 loss_train: 135.7220 loss_val: 130.8525\n",
      "Epoch: 197 loss_train: 136.5186 loss_val: 130.6901\n",
      "Epoch: 198 loss_train: 136.0609 loss_val: 130.4330\n",
      "Epoch: 199 loss_train: 136.7564 loss_val: 130.1546\n",
      "Epoch: 200 loss_train: 136.1805 loss_val: 129.7100\n",
      "Epoch: 001 loss_train: 135.5462 loss_val: 150.6925\n",
      "Epoch: 002 loss_train: 155.7359 loss_val: 126.9765\n",
      "Epoch: 003 loss_train: 170.6328 loss_val: 130.5568\n",
      "Epoch: 004 loss_train: 141.9796 loss_val: 130.4924\n",
      "Epoch: 005 loss_train: 138.2779 loss_val: 126.9720\n",
      "Epoch: 006 loss_train: 134.8706 loss_val: 128.1509\n",
      "Epoch: 007 loss_train: 133.0467 loss_val: 132.3417\n",
      "Epoch: 008 loss_train: 137.6197 loss_val: 133.2027\n",
      "Epoch: 009 loss_train: 138.1792 loss_val: 130.0293\n",
      "Epoch: 010 loss_train: 135.1143 loss_val: 126.6258\n",
      "Epoch: 011 loss_train: 132.7232 loss_val: 125.5213\n",
      "Epoch: 012 loss_train: 132.1297 loss_val: 125.3413\n",
      "Epoch: 013 loss_train: 132.6109 loss_val: 123.9103\n",
      "Epoch: 014 loss_train: 131.3404 loss_val: 121.7124\n",
      "Epoch: 015 loss_train: 129.6420 loss_val: 120.8037\n",
      "Epoch: 016 loss_train: 127.3743 loss_val: 121.4947\n",
      "Epoch: 017 loss_train: 129.2362 loss_val: 121.7913\n",
      "Epoch: 018 loss_train: 129.3604 loss_val: 120.4590\n",
      "Epoch: 019 loss_train: 129.5471 loss_val: 118.9460\n",
      "Epoch: 020 loss_train: 127.5459 loss_val: 118.4268\n",
      "Epoch: 021 loss_train: 126.2910 loss_val: 119.3282\n",
      "Epoch: 022 loss_train: 125.6597 loss_val: 120.2964\n",
      "Epoch: 023 loss_train: 127.0952 loss_val: 120.4378\n",
      "Epoch: 024 loss_train: 127.3262 loss_val: 119.7226\n",
      "Epoch: 025 loss_train: 126.1850 loss_val: 118.6850\n",
      "Epoch: 026 loss_train: 124.8396 loss_val: 117.6845\n",
      "Epoch: 027 loss_train: 123.7984 loss_val: 116.7791\n",
      "Epoch: 028 loss_train: 124.9522 loss_val: 116.1707\n",
      "Epoch: 029 loss_train: 126.3065 loss_val: 117.0767\n",
      "Epoch: 030 loss_train: 126.1716 loss_val: 118.0064\n",
      "Epoch: 031 loss_train: 123.9295 loss_val: 118.5915\n",
      "Epoch: 032 loss_train: 124.3287 loss_val: 118.9984\n",
      "Epoch: 033 loss_train: 124.7485 loss_val: 119.2031\n",
      "Epoch: 034 loss_train: 124.7630 loss_val: 118.8985\n",
      "Epoch: 035 loss_train: 124.5632 loss_val: 118.1492\n",
      "Epoch: 036 loss_train: 123.9803 loss_val: 117.4404\n",
      "Epoch: 037 loss_train: 123.3015 loss_val: 117.1296\n",
      "Epoch: 038 loss_train: 122.5190 loss_val: 116.8383\n",
      "Epoch: 039 loss_train: 122.6174 loss_val: 115.9824\n",
      "Epoch: 040 loss_train: 122.3940 loss_val: 114.9627\n",
      "Epoch: 041 loss_train: 120.9211 loss_val: 114.0577\n",
      "Epoch: 042 loss_train: 123.8333 loss_val: 114.4904\n",
      "Epoch: 043 loss_train: 120.7648 loss_val: 115.2020\n",
      "Epoch: 044 loss_train: 120.6504 loss_val: 115.6102\n",
      "Epoch: 045 loss_train: 121.2912 loss_val: 115.2665\n",
      "Epoch: 046 loss_train: 121.6571 loss_val: 114.7803\n",
      "Epoch: 047 loss_train: 122.4512 loss_val: 114.6337\n",
      "Epoch: 048 loss_train: 120.0741 loss_val: 114.3526\n",
      "Epoch: 049 loss_train: 119.8569 loss_val: 114.0352\n",
      "Epoch: 050 loss_train: 119.6134 loss_val: 113.7220\n",
      "Epoch: 051 loss_train: 119.8535 loss_val: 113.4548\n",
      "Epoch: 052 loss_train: 118.9943 loss_val: 112.9222\n",
      "Epoch: 053 loss_train: 119.4492 loss_val: 112.7649\n",
      "Epoch: 054 loss_train: 119.7932 loss_val: 112.9562\n",
      "Epoch: 055 loss_train: 117.7707 loss_val: 112.8752\n",
      "Epoch: 056 loss_train: 118.4841 loss_val: 112.4093\n",
      "Epoch: 057 loss_train: 119.3074 loss_val: 111.9539\n",
      "Epoch: 058 loss_train: 116.1407 loss_val: 111.0951\n",
      "Epoch: 059 loss_train: 115.5577 loss_val: 109.9009\n",
      "Epoch: 060 loss_train: 117.4738 loss_val: 110.3119\n",
      "Epoch: 061 loss_train: 116.1710 loss_val: 111.1820\n",
      "Epoch: 062 loss_train: 116.9836 loss_val: 111.5262\n",
      "Epoch: 063 loss_train: 116.1026 loss_val: 111.1319\n",
      "Epoch: 064 loss_train: 117.7312 loss_val: 111.6228\n",
      "Epoch: 065 loss_train: 115.2129 loss_val: 111.4465\n",
      "Epoch: 066 loss_train: 115.1038 loss_val: 110.7076\n",
      "Epoch: 067 loss_train: 114.7216 loss_val: 109.8269\n",
      "Epoch: 068 loss_train: 113.9179 loss_val: 109.7423\n",
      "Epoch: 069 loss_train: 114.8148 loss_val: 109.6718\n",
      "Epoch: 070 loss_train: 113.9086 loss_val: 108.7020\n",
      "Epoch: 071 loss_train: 114.4756 loss_val: 108.3766\n",
      "Epoch: 072 loss_train: 111.9510 loss_val: 107.0769\n",
      "Epoch: 073 loss_train: 112.0633 loss_val: 106.0326\n",
      "Epoch: 074 loss_train: 111.8204 loss_val: 105.4798\n",
      "Epoch: 075 loss_train: 109.7771 loss_val: 105.0282\n",
      "Epoch: 076 loss_train: 111.0832 loss_val: 106.3256\n",
      "Epoch: 077 loss_train: 108.1327 loss_val: 105.7131\n",
      "Epoch: 078 loss_train: 107.4100 loss_val: 103.6905\n",
      "Epoch: 079 loss_train: 107.2095 loss_val: 104.2808\n",
      "Epoch: 080 loss_train: 105.6808 loss_val: 99.0595\n",
      "Epoch: 081 loss_train: 102.9967 loss_val: 97.4262\n",
      "Epoch: 082 loss_train: 99.8343 loss_val: 98.0971\n",
      "Epoch: 083 loss_train: 103.0976 loss_val: 79.9323\n",
      "Epoch: 084 loss_train: 124.4564 loss_val: 106.3749\n",
      "Epoch: 085 loss_train: 105.1628 loss_val: 113.7340\n",
      "Epoch: 086 loss_train: 114.0440 loss_val: 114.8253\n",
      "Epoch: 087 loss_train: 113.7856 loss_val: 113.5500\n",
      "Epoch: 088 loss_train: 110.5481 loss_val: 110.2336\n",
      "Epoch: 089 loss_train: 106.9602 loss_val: 93.6320\n",
      "Epoch: 090 loss_train: 103.8650 loss_val: 86.8713\n",
      "Epoch: 091 loss_train: 107.2927 loss_val: 90.1210\n",
      "Epoch: 092 loss_train: 104.2176 loss_val: 96.5109\n",
      "Epoch: 093 loss_train: 104.4450 loss_val: 100.1459\n",
      "Epoch: 094 loss_train: 104.1944 loss_val: 99.5963\n",
      "Epoch: 095 loss_train: 103.9819 loss_val: 95.7774\n",
      "Epoch: 096 loss_train: 105.2789 loss_val: 89.8838\n",
      "Epoch: 097 loss_train: 99.7829 loss_val: 87.5051\n",
      "Epoch: 098 loss_train: 99.9312 loss_val: 90.2448\n",
      "Epoch: 099 loss_train: 102.3078 loss_val: 92.3637\n",
      "Epoch: 100 loss_train: 100.8587 loss_val: 90.3587\n",
      "Epoch: 101 loss_train: 100.5883 loss_val: 89.2073\n",
      "Epoch: 102 loss_train: 99.4870 loss_val: 88.6125\n",
      "Epoch: 103 loss_train: 99.3060 loss_val: 88.4050\n",
      "Epoch: 104 loss_train: 97.5116 loss_val: 89.2472\n",
      "Epoch: 105 loss_train: 98.6981 loss_val: 89.0289\n",
      "Epoch: 106 loss_train: 97.9307 loss_val: 87.1098\n",
      "Epoch: 107 loss_train: 96.6874 loss_val: 87.0558\n",
      "Epoch: 108 loss_train: 96.3841 loss_val: 88.8231\n",
      "Epoch: 109 loss_train: 96.4003 loss_val: 87.8723\n",
      "Epoch: 110 loss_train: 95.5678 loss_val: 86.4324\n",
      "Epoch: 111 loss_train: 94.8987 loss_val: 86.9820\n",
      "Epoch: 112 loss_train: 95.0744 loss_val: 88.0179\n",
      "Epoch: 113 loss_train: 95.2977 loss_val: 86.2262\n",
      "Epoch: 114 loss_train: 93.8756 loss_val: 85.5848\n",
      "Epoch: 115 loss_train: 94.3409 loss_val: 87.0138\n",
      "Epoch: 116 loss_train: 94.2975 loss_val: 85.5958\n",
      "Epoch: 117 loss_train: 93.2636 loss_val: 85.3146\n",
      "Epoch: 118 loss_train: 93.2408 loss_val: 86.3408\n",
      "Epoch: 119 loss_train: 92.8418 loss_val: 84.2840\n",
      "Epoch: 120 loss_train: 93.9523 loss_val: 84.9237\n",
      "Epoch: 121 loss_train: 91.6403 loss_val: 84.6040\n",
      "Epoch: 122 loss_train: 92.1442 loss_val: 84.5948\n",
      "Epoch: 123 loss_train: 91.0742 loss_val: 83.4050\n",
      "Epoch: 124 loss_train: 90.7055 loss_val: 84.9471\n",
      "Epoch: 125 loss_train: 90.4216 loss_val: 83.5564\n",
      "Epoch: 126 loss_train: 90.6557 loss_val: 83.4636\n",
      "Epoch: 127 loss_train: 89.5963 loss_val: 83.4902\n",
      "Epoch: 128 loss_train: 89.7698 loss_val: 81.6175\n",
      "Epoch: 129 loss_train: 89.9049 loss_val: 83.6218\n",
      "Epoch: 130 loss_train: 89.8862 loss_val: 81.8314\n",
      "Epoch: 131 loss_train: 89.1050 loss_val: 81.8694\n",
      "Epoch: 132 loss_train: 89.0147 loss_val: 83.1303\n",
      "Epoch: 133 loss_train: 88.5808 loss_val: 81.2916\n",
      "Epoch: 134 loss_train: 87.7565 loss_val: 79.8059\n",
      "Epoch: 135 loss_train: 88.3969 loss_val: 86.4309\n",
      "Epoch: 136 loss_train: 90.9730 loss_val: 81.4536\n",
      "Epoch: 137 loss_train: 88.5801 loss_val: 80.3359\n",
      "Epoch: 138 loss_train: 88.7986 loss_val: 83.0303\n",
      "Epoch: 139 loss_train: 88.3966 loss_val: 78.6650\n",
      "Epoch: 140 loss_train: 91.4092 loss_val: 85.1125\n",
      "Epoch: 141 loss_train: 90.3468 loss_val: 84.2014\n",
      "Epoch: 142 loss_train: 91.2976 loss_val: 84.2922\n",
      "Epoch: 143 loss_train: 92.3690 loss_val: 83.5779\n",
      "Epoch: 144 loss_train: 90.4058 loss_val: 85.7009\n",
      "Epoch: 145 loss_train: 90.7390 loss_val: 81.3900\n",
      "Epoch: 146 loss_train: 89.6195 loss_val: 81.1163\n",
      "Epoch: 147 loss_train: 90.2152 loss_val: 84.8372\n",
      "Epoch: 148 loss_train: 90.6249 loss_val: 83.2507\n",
      "Epoch: 149 loss_train: 88.8352 loss_val: 80.7636\n",
      "Epoch: 150 loss_train: 89.1357 loss_val: 80.6712\n",
      "Epoch: 151 loss_train: 89.2976 loss_val: 82.3837\n",
      "Epoch: 152 loss_train: 88.9275 loss_val: 82.5583\n",
      "Epoch: 153 loss_train: 89.0089 loss_val: 79.8571\n",
      "Epoch: 154 loss_train: 88.1522 loss_val: 79.4608\n",
      "Epoch: 155 loss_train: 87.3745 loss_val: 80.3507\n",
      "Epoch: 156 loss_train: 87.2660 loss_val: 80.8747\n",
      "Epoch: 157 loss_train: 86.7946 loss_val: 78.1806\n",
      "Epoch: 158 loss_train: 86.6805 loss_val: 80.0119\n",
      "Epoch: 159 loss_train: 86.8416 loss_val: 79.9788\n",
      "Epoch: 160 loss_train: 86.1978 loss_val: 78.4876\n",
      "Epoch: 161 loss_train: 85.8587 loss_val: 79.0807\n",
      "Epoch: 162 loss_train: 86.0939 loss_val: 79.5995\n",
      "Epoch: 163 loss_train: 85.2153 loss_val: 78.7025\n",
      "Epoch: 164 loss_train: 85.3425 loss_val: 78.0186\n",
      "Epoch: 165 loss_train: 85.6396 loss_val: 78.4259\n",
      "Epoch: 166 loss_train: 85.4554 loss_val: 77.6101\n",
      "Epoch: 167 loss_train: 85.0655 loss_val: 76.8105\n",
      "Epoch: 168 loss_train: 84.7584 loss_val: 77.7419\n",
      "Epoch: 169 loss_train: 84.3236 loss_val: 77.6243\n",
      "Epoch: 170 loss_train: 84.1835 loss_val: 76.9774\n",
      "Epoch: 171 loss_train: 84.0793 loss_val: 77.2777\n",
      "Epoch: 172 loss_train: 84.4322 loss_val: 77.9083\n",
      "Epoch: 173 loss_train: 84.0240 loss_val: 77.1365\n",
      "Epoch: 174 loss_train: 83.1163 loss_val: 76.6145\n",
      "Epoch: 175 loss_train: 83.6856 loss_val: 77.2732\n",
      "Epoch: 176 loss_train: 83.9902 loss_val: 77.0830\n",
      "Epoch: 177 loss_train: 83.2561 loss_val: 76.2167\n",
      "Epoch: 178 loss_train: 82.8740 loss_val: 76.2051\n",
      "Epoch: 179 loss_train: 83.0529 loss_val: 76.6461\n",
      "Epoch: 180 loss_train: 82.7551 loss_val: 76.1459\n",
      "Epoch: 181 loss_train: 82.6242 loss_val: 75.4946\n",
      "Epoch: 182 loss_train: 82.2643 loss_val: 75.8781\n",
      "Epoch: 183 loss_train: 82.1964 loss_val: 75.4274\n",
      "Epoch: 184 loss_train: 82.1943 loss_val: 75.8766\n",
      "Epoch: 185 loss_train: 81.9327 loss_val: 75.1953\n",
      "Epoch: 186 loss_train: 81.9458 loss_val: 75.4996\n",
      "Epoch: 187 loss_train: 81.3315 loss_val: 77.6039\n",
      "Epoch: 188 loss_train: 83.0292 loss_val: 78.8055\n",
      "Epoch: 189 loss_train: 89.7180 loss_val: 95.1201\n",
      "Epoch: 190 loss_train: 98.4249 loss_val: 88.8653\n",
      "Epoch: 191 loss_train: 93.5615 loss_val: 85.4482\n",
      "Epoch: 192 loss_train: 91.5110 loss_val: 89.3754\n",
      "Epoch: 193 loss_train: 96.0281 loss_val: 85.5869\n",
      "Epoch: 194 loss_train: 91.3580 loss_val: 90.2624\n",
      "Epoch: 195 loss_train: 93.5567 loss_val: 91.1541\n",
      "Epoch: 196 loss_train: 94.0600 loss_val: 86.0078\n",
      "Epoch: 197 loss_train: 90.9287 loss_val: 84.7356\n",
      "Epoch: 198 loss_train: 91.0012 loss_val: 83.5129\n",
      "Epoch: 199 loss_train: 90.6741 loss_val: 84.4489\n",
      "Epoch: 200 loss_train: 89.3363 loss_val: 85.8024\n",
      "Epoch: 001 loss_train: 90.6652 loss_val: 94.3732\n",
      "Epoch: 002 loss_train: 112.4082 loss_val: 81.3080\n",
      "Epoch: 003 loss_train: 88.4594 loss_val: 87.1048\n",
      "Epoch: 004 loss_train: 91.1130 loss_val: 94.8351\n",
      "Epoch: 005 loss_train: 96.9809 loss_val: 94.5267\n",
      "Epoch: 006 loss_train: 96.9155 loss_val: 89.8306\n",
      "Epoch: 007 loss_train: 92.8528 loss_val: 86.0921\n",
      "Epoch: 008 loss_train: 90.9303 loss_val: 84.5682\n",
      "Epoch: 009 loss_train: 90.2440 loss_val: 83.5105\n",
      "Epoch: 010 loss_train: 90.8381 loss_val: 82.3454\n",
      "Epoch: 011 loss_train: 89.2893 loss_val: 82.0353\n",
      "Epoch: 012 loss_train: 89.5464 loss_val: 82.5245\n",
      "Epoch: 013 loss_train: 88.6613 loss_val: 82.8144\n",
      "Epoch: 014 loss_train: 88.6829 loss_val: 81.9955\n",
      "Epoch: 015 loss_train: 86.9114 loss_val: 80.3208\n",
      "Epoch: 016 loss_train: 85.0724 loss_val: 78.7991\n",
      "Epoch: 017 loss_train: 84.4796 loss_val: 78.2212\n",
      "Epoch: 018 loss_train: 84.2263 loss_val: 78.2326\n",
      "Epoch: 019 loss_train: 84.5610 loss_val: 77.9478\n",
      "Epoch: 020 loss_train: 84.5553 loss_val: 77.6726\n",
      "Epoch: 021 loss_train: 83.8360 loss_val: 78.0105\n",
      "Epoch: 022 loss_train: 84.5557 loss_val: 78.8702\n",
      "Epoch: 023 loss_train: 83.5789 loss_val: 78.9338\n",
      "Epoch: 024 loss_train: 83.7761 loss_val: 77.9483\n",
      "Epoch: 025 loss_train: 82.8354 loss_val: 77.0288\n",
      "Epoch: 026 loss_train: 82.9614 loss_val: 76.8390\n",
      "Epoch: 027 loss_train: 83.2668 loss_val: 77.1295\n",
      "Epoch: 028 loss_train: 82.9163 loss_val: 77.3656\n",
      "Epoch: 029 loss_train: 82.8973 loss_val: 77.2956\n",
      "Epoch: 030 loss_train: 82.5008 loss_val: 77.1059\n",
      "Epoch: 031 loss_train: 82.6177 loss_val: 76.8510\n",
      "Epoch: 032 loss_train: 82.4224 loss_val: 76.4352\n",
      "Epoch: 033 loss_train: 82.0154 loss_val: 76.4208\n",
      "Epoch: 034 loss_train: 81.8751 loss_val: 76.5591\n",
      "Epoch: 035 loss_train: 81.3893 loss_val: 76.5532\n",
      "Epoch: 036 loss_train: 82.3779 loss_val: 76.4644\n",
      "Epoch: 037 loss_train: 81.0917 loss_val: 76.2726\n",
      "Epoch: 038 loss_train: 80.9836 loss_val: 75.8904\n",
      "Epoch: 039 loss_train: 81.0943 loss_val: 75.7333\n",
      "Epoch: 040 loss_train: 80.9221 loss_val: 75.6797\n",
      "Epoch: 041 loss_train: 80.2556 loss_val: 75.7095\n",
      "Epoch: 042 loss_train: 80.1944 loss_val: 75.7693\n",
      "Epoch: 043 loss_train: 80.5174 loss_val: 75.7632\n",
      "Epoch: 044 loss_train: 80.7751 loss_val: 75.5858\n",
      "Epoch: 045 loss_train: 80.0847 loss_val: 75.5163\n",
      "Epoch: 046 loss_train: 80.1245 loss_val: 75.3260\n",
      "Epoch: 047 loss_train: 79.7043 loss_val: 75.3984\n",
      "Epoch: 048 loss_train: 79.7142 loss_val: 75.1801\n",
      "Epoch: 049 loss_train: 79.7959 loss_val: 75.0914\n",
      "Epoch: 050 loss_train: 80.0761 loss_val: 75.1739\n",
      "Epoch: 051 loss_train: 79.5681 loss_val: 75.3056\n",
      "Epoch: 052 loss_train: 79.5907 loss_val: 75.0967\n",
      "Epoch: 053 loss_train: 79.9328 loss_val: 74.9417\n",
      "Epoch: 054 loss_train: 79.6555 loss_val: 74.9621\n",
      "Epoch: 055 loss_train: 79.3678 loss_val: 75.2102\n",
      "Epoch: 056 loss_train: 79.1635 loss_val: 75.0440\n",
      "Epoch: 057 loss_train: 79.3954 loss_val: 74.7279\n",
      "Epoch: 058 loss_train: 79.3282 loss_val: 74.7426\n",
      "Epoch: 059 loss_train: 79.1623 loss_val: 74.8050\n",
      "Epoch: 060 loss_train: 79.2868 loss_val: 74.9296\n",
      "Epoch: 061 loss_train: 78.8975 loss_val: 74.7542\n",
      "Epoch: 062 loss_train: 78.9539 loss_val: 74.7072\n",
      "Epoch: 063 loss_train: 79.0867 loss_val: 74.8670\n",
      "Epoch: 064 loss_train: 79.3521 loss_val: 75.0831\n",
      "Epoch: 065 loss_train: 79.0306 loss_val: 74.6618\n",
      "Epoch: 066 loss_train: 79.0882 loss_val: 74.3888\n",
      "Epoch: 067 loss_train: 78.9845 loss_val: 74.6783\n",
      "Epoch: 068 loss_train: 78.8139 loss_val: 74.2788\n",
      "Epoch: 069 loss_train: 78.7519 loss_val: 74.0519\n",
      "Epoch: 070 loss_train: 78.7553 loss_val: 74.3020\n",
      "Epoch: 071 loss_train: 78.6282 loss_val: 74.3854\n",
      "Epoch: 072 loss_train: 79.0820 loss_val: 73.9326\n",
      "Epoch: 073 loss_train: 78.6995 loss_val: 73.8927\n",
      "Epoch: 074 loss_train: 79.1801 loss_val: 75.1325\n",
      "Epoch: 075 loss_train: 79.2632 loss_val: 74.2480\n",
      "Epoch: 076 loss_train: 78.4268 loss_val: 74.1477\n",
      "Epoch: 077 loss_train: 78.8152 loss_val: 74.1606\n",
      "Epoch: 078 loss_train: 78.3231 loss_val: 75.7093\n",
      "Epoch: 079 loss_train: 79.3255 loss_val: 74.2379\n",
      "Epoch: 080 loss_train: 78.2326 loss_val: 74.1166\n",
      "Epoch: 081 loss_train: 78.6362 loss_val: 74.1020\n",
      "Epoch: 082 loss_train: 78.1421 loss_val: 74.8088\n",
      "Epoch: 083 loss_train: 78.6758 loss_val: 74.0565\n",
      "Epoch: 084 loss_train: 78.1130 loss_val: 73.9137\n",
      "Epoch: 085 loss_train: 78.5269 loss_val: 74.1670\n",
      "Epoch: 086 loss_train: 78.2345 loss_val: 74.5111\n",
      "Epoch: 087 loss_train: 78.1613 loss_val: 73.9211\n",
      "Epoch: 088 loss_train: 77.8980 loss_val: 73.8167\n",
      "Epoch: 089 loss_train: 78.2491 loss_val: 73.8680\n",
      "Epoch: 090 loss_train: 78.0922 loss_val: 74.2812\n",
      "Epoch: 091 loss_train: 78.0374 loss_val: 73.8746\n",
      "Epoch: 092 loss_train: 77.7886 loss_val: 73.6584\n",
      "Epoch: 093 loss_train: 78.0892 loss_val: 73.7890\n",
      "Epoch: 094 loss_train: 77.6410 loss_val: 73.9503\n",
      "Epoch: 095 loss_train: 77.5564 loss_val: 73.4824\n",
      "Epoch: 096 loss_train: 77.6165 loss_val: 73.5046\n",
      "Epoch: 097 loss_train: 77.7267 loss_val: 73.8980\n",
      "Epoch: 098 loss_train: 77.9318 loss_val: 73.6106\n",
      "Epoch: 099 loss_train: 77.6751 loss_val: 73.4810\n",
      "Epoch: 100 loss_train: 77.9764 loss_val: 75.1676\n",
      "Epoch: 101 loss_train: 78.7113 loss_val: 74.3380\n",
      "Epoch: 102 loss_train: 78.2522 loss_val: 73.4841\n",
      "Epoch: 103 loss_train: 77.8070 loss_val: 73.4366\n",
      "Epoch: 104 loss_train: 77.5004 loss_val: 74.3800\n",
      "Epoch: 105 loss_train: 77.7493 loss_val: 74.3090\n",
      "Epoch: 106 loss_train: 77.9516 loss_val: 73.6365\n",
      "Epoch: 107 loss_train: 77.3134 loss_val: 73.4116\n",
      "Epoch: 108 loss_train: 77.4674 loss_val: 73.6640\n",
      "Epoch: 109 loss_train: 77.1566 loss_val: 73.5791\n",
      "Epoch: 110 loss_train: 77.1983 loss_val: 73.1398\n",
      "Epoch: 111 loss_train: 77.0536 loss_val: 73.1340\n",
      "Epoch: 112 loss_train: 77.2565 loss_val: 73.7434\n",
      "Epoch: 113 loss_train: 77.4894 loss_val: 72.9920\n",
      "Epoch: 114 loss_train: 77.5979 loss_val: 72.8876\n",
      "Epoch: 115 loss_train: 77.1475 loss_val: 74.1175\n",
      "Epoch: 116 loss_train: 77.2966 loss_val: 73.0291\n",
      "Epoch: 117 loss_train: 76.8789 loss_val: 72.8481\n",
      "Epoch: 118 loss_train: 77.3845 loss_val: 74.3340\n",
      "Epoch: 119 loss_train: 77.3239 loss_val: 73.8880\n",
      "Epoch: 120 loss_train: 77.4508 loss_val: 73.4396\n",
      "Epoch: 121 loss_train: 77.4985 loss_val: 73.3153\n",
      "Epoch: 122 loss_train: 77.3476 loss_val: 74.0157\n",
      "Epoch: 123 loss_train: 76.8733 loss_val: 74.5118\n",
      "Epoch: 124 loss_train: 77.4116 loss_val: 73.2992\n",
      "Epoch: 125 loss_train: 77.0291 loss_val: 72.9455\n",
      "Epoch: 126 loss_train: 77.6665 loss_val: 73.8043\n",
      "Epoch: 127 loss_train: 77.1929 loss_val: 74.3026\n",
      "Epoch: 128 loss_train: 77.7414 loss_val: 73.2639\n",
      "Epoch: 129 loss_train: 77.0305 loss_val: 72.9995\n",
      "Epoch: 130 loss_train: 76.9784 loss_val: 72.9193\n",
      "Epoch: 131 loss_train: 76.1937 loss_val: 73.9489\n",
      "Epoch: 132 loss_train: 76.7269 loss_val: 73.0845\n",
      "Epoch: 133 loss_train: 77.7098 loss_val: 73.0381\n",
      "Epoch: 134 loss_train: 77.1616 loss_val: 73.0627\n",
      "Epoch: 135 loss_train: 76.4198 loss_val: 72.5854\n",
      "Epoch: 136 loss_train: 76.2201 loss_val: 72.7308\n",
      "Epoch: 137 loss_train: 76.4586 loss_val: 73.0945\n",
      "Epoch: 138 loss_train: 76.7842 loss_val: 72.9905\n",
      "Epoch: 139 loss_train: 76.2978 loss_val: 72.6566\n",
      "Epoch: 140 loss_train: 76.3232 loss_val: 74.9191\n",
      "Epoch: 141 loss_train: 78.1503 loss_val: 72.6837\n",
      "Epoch: 142 loss_train: 76.2029 loss_val: 72.5707\n",
      "Epoch: 143 loss_train: 76.7001 loss_val: 73.6765\n",
      "Epoch: 144 loss_train: 76.5528 loss_val: 74.0670\n",
      "Epoch: 145 loss_train: 77.1219 loss_val: 73.2343\n",
      "Epoch: 146 loss_train: 76.9176 loss_val: 73.1475\n",
      "Epoch: 147 loss_train: 76.9333 loss_val: 73.6085\n",
      "Epoch: 148 loss_train: 76.6341 loss_val: 73.9251\n",
      "Epoch: 149 loss_train: 76.8540 loss_val: 73.3002\n",
      "Epoch: 150 loss_train: 76.4608 loss_val: 73.2588\n",
      "Epoch: 151 loss_train: 76.3275 loss_val: 73.1861\n",
      "Epoch: 152 loss_train: 76.2089 loss_val: 73.3805\n",
      "Epoch: 153 loss_train: 76.4318 loss_val: 72.6489\n",
      "Epoch: 154 loss_train: 76.1770 loss_val: 72.3399\n",
      "Epoch: 155 loss_train: 76.7331 loss_val: 73.4797\n",
      "Epoch: 156 loss_train: 76.3455 loss_val: 73.1226\n",
      "Epoch: 157 loss_train: 76.4531 loss_val: 72.4732\n",
      "Epoch: 158 loss_train: 76.6891 loss_val: 72.9059\n",
      "Epoch: 159 loss_train: 76.0860 loss_val: 73.2694\n",
      "Epoch: 160 loss_train: 76.1620 loss_val: 72.3726\n",
      "Epoch: 161 loss_train: 75.7853 loss_val: 72.4000\n",
      "Epoch: 162 loss_train: 75.7651 loss_val: 72.9975\n",
      "Epoch: 163 loss_train: 75.8867 loss_val: 73.1838\n",
      "Epoch: 164 loss_train: 76.2034 loss_val: 72.1530\n",
      "Epoch: 165 loss_train: 76.0658 loss_val: 72.1568\n",
      "Epoch: 166 loss_train: 75.6484 loss_val: 72.5876\n",
      "Epoch: 167 loss_train: 75.4505 loss_val: 72.2999\n",
      "Epoch: 168 loss_train: 75.7313 loss_val: 72.0480\n",
      "Epoch: 169 loss_train: 75.4187 loss_val: 72.2895\n",
      "Epoch: 170 loss_train: 75.5011 loss_val: 72.3454\n",
      "Epoch: 171 loss_train: 75.2881 loss_val: 71.8546\n",
      "Epoch: 172 loss_train: 75.5462 loss_val: 72.1326\n",
      "Epoch: 173 loss_train: 75.2260 loss_val: 71.9143\n",
      "Epoch: 174 loss_train: 75.0696 loss_val: 71.7720\n",
      "Epoch: 175 loss_train: 75.6431 loss_val: 74.0014\n",
      "Epoch: 176 loss_train: 76.3315 loss_val: 72.7325\n",
      "Epoch: 177 loss_train: 75.6319 loss_val: 72.3623\n",
      "Epoch: 178 loss_train: 76.4020 loss_val: 72.6882\n",
      "Epoch: 179 loss_train: 75.1912 loss_val: 73.2782\n",
      "Epoch: 180 loss_train: 75.5238 loss_val: 72.4873\n",
      "Epoch: 181 loss_train: 75.4072 loss_val: 72.2387\n",
      "Epoch: 182 loss_train: 75.8728 loss_val: 72.4432\n",
      "Epoch: 183 loss_train: 75.3483 loss_val: 72.8310\n",
      "Epoch: 184 loss_train: 75.7062 loss_val: 72.1861\n",
      "Epoch: 185 loss_train: 75.0473 loss_val: 72.1208\n",
      "Epoch: 186 loss_train: 75.8284 loss_val: 73.2626\n",
      "Epoch: 187 loss_train: 75.7202 loss_val: 73.2526\n",
      "Epoch: 188 loss_train: 75.6708 loss_val: 72.0967\n",
      "Epoch: 189 loss_train: 75.4093 loss_val: 72.1650\n",
      "Epoch: 190 loss_train: 75.4666 loss_val: 72.9784\n",
      "Epoch: 191 loss_train: 75.2413 loss_val: 73.1725\n",
      "Epoch: 192 loss_train: 75.3813 loss_val: 71.8369\n",
      "Epoch: 193 loss_train: 75.1927 loss_val: 71.9684\n",
      "Epoch: 194 loss_train: 75.1225 loss_val: 72.5681\n",
      "Epoch: 195 loss_train: 75.0480 loss_val: 71.6436\n",
      "Epoch: 196 loss_train: 74.6227 loss_val: 71.6935\n",
      "Epoch: 197 loss_train: 75.5362 loss_val: 72.4971\n",
      "Epoch: 198 loss_train: 74.6681 loss_val: 72.5641\n",
      "Epoch: 199 loss_train: 75.2367 loss_val: 72.0020\n",
      "Epoch: 200 loss_train: 74.6990 loss_val: 71.9974\n",
      "Epoch: 001 loss_train: 74.7640 loss_val: 83.4523\n",
      "Epoch: 002 loss_train: 83.5259 loss_val: 73.9431\n",
      "Epoch: 003 loss_train: 79.9736 loss_val: 72.1755\n",
      "Epoch: 004 loss_train: 75.6571 loss_val: 74.2304\n",
      "Epoch: 005 loss_train: 77.4217 loss_val: 74.7755\n",
      "Epoch: 006 loss_train: 77.1254 loss_val: 74.1242\n",
      "Epoch: 007 loss_train: 76.4461 loss_val: 73.2839\n",
      "Epoch: 008 loss_train: 75.2140 loss_val: 73.1421\n",
      "Epoch: 009 loss_train: 75.2812 loss_val: 73.9152\n",
      "Epoch: 010 loss_train: 75.9484 loss_val: 74.4641\n",
      "Epoch: 011 loss_train: 76.4821 loss_val: 74.1268\n",
      "Epoch: 012 loss_train: 76.2378 loss_val: 73.5306\n",
      "Epoch: 013 loss_train: 75.6340 loss_val: 73.2111\n",
      "Epoch: 014 loss_train: 74.4275 loss_val: 73.0799\n",
      "Epoch: 015 loss_train: 75.2147 loss_val: 73.0077\n",
      "Epoch: 016 loss_train: 75.1674 loss_val: 72.8171\n",
      "Epoch: 017 loss_train: 75.5136 loss_val: 72.6965\n",
      "Epoch: 018 loss_train: 75.8198 loss_val: 72.4522\n",
      "Epoch: 019 loss_train: 75.4643 loss_val: 72.3536\n",
      "Epoch: 020 loss_train: 74.4395 loss_val: 72.4451\n",
      "Epoch: 021 loss_train: 74.8385 loss_val: 72.5516\n",
      "Epoch: 022 loss_train: 75.1270 loss_val: 72.5028\n",
      "Epoch: 023 loss_train: 74.9029 loss_val: 72.2556\n",
      "Epoch: 024 loss_train: 74.8092 loss_val: 71.8379\n",
      "Epoch: 025 loss_train: 74.4267 loss_val: 71.5634\n",
      "Epoch: 026 loss_train: 74.4860 loss_val: 71.5059\n",
      "Epoch: 027 loss_train: 74.6031 loss_val: 71.6274\n",
      "Epoch: 028 loss_train: 74.6817 loss_val: 71.7099\n",
      "Epoch: 029 loss_train: 74.8610 loss_val: 71.6949\n",
      "Epoch: 030 loss_train: 74.5515 loss_val: 71.7603\n",
      "Epoch: 031 loss_train: 74.4731 loss_val: 71.8498\n",
      "Epoch: 032 loss_train: 74.7651 loss_val: 71.9773\n",
      "Epoch: 033 loss_train: 74.7749 loss_val: 71.8277\n",
      "Epoch: 034 loss_train: 74.1517 loss_val: 71.5923\n",
      "Epoch: 035 loss_train: 74.5094 loss_val: 71.5130\n",
      "Epoch: 036 loss_train: 74.2866 loss_val: 71.4142\n",
      "Epoch: 037 loss_train: 74.2776 loss_val: 71.3512\n",
      "Epoch: 038 loss_train: 74.1631 loss_val: 71.3988\n",
      "Epoch: 039 loss_train: 74.4641 loss_val: 71.4843\n",
      "Epoch: 040 loss_train: 74.2505 loss_val: 71.4504\n",
      "Epoch: 041 loss_train: 73.9784 loss_val: 71.4895\n",
      "Epoch: 042 loss_train: 74.3192 loss_val: 71.5529\n",
      "Epoch: 043 loss_train: 74.2549 loss_val: 71.5503\n",
      "Epoch: 044 loss_train: 73.8738 loss_val: 71.4997\n",
      "Epoch: 045 loss_train: 74.0298 loss_val: 71.4611\n",
      "Epoch: 046 loss_train: 73.9440 loss_val: 71.5127\n",
      "Epoch: 047 loss_train: 73.9368 loss_val: 71.5212\n",
      "Epoch: 048 loss_train: 73.6977 loss_val: 71.4074\n",
      "Epoch: 049 loss_train: 74.0492 loss_val: 71.3315\n",
      "Epoch: 050 loss_train: 73.9870 loss_val: 71.4290\n",
      "Epoch: 051 loss_train: 74.2532 loss_val: 71.5508\n",
      "Epoch: 052 loss_train: 74.0590 loss_val: 71.5001\n",
      "Epoch: 053 loss_train: 73.9915 loss_val: 71.4048\n",
      "Epoch: 054 loss_train: 74.3004 loss_val: 71.3646\n",
      "Epoch: 055 loss_train: 73.9852 loss_val: 71.4156\n",
      "Epoch: 056 loss_train: 74.0666 loss_val: 71.3951\n",
      "Epoch: 057 loss_train: 74.0033 loss_val: 71.2150\n",
      "Epoch: 058 loss_train: 74.1678 loss_val: 71.2212\n",
      "Epoch: 059 loss_train: 73.9665 loss_val: 71.3371\n",
      "Epoch: 060 loss_train: 73.5834 loss_val: 71.3885\n",
      "Epoch: 061 loss_train: 74.2116 loss_val: 71.3190\n",
      "Epoch: 062 loss_train: 73.6058 loss_val: 71.2654\n",
      "Epoch: 063 loss_train: 73.8938 loss_val: 71.2576\n",
      "Epoch: 064 loss_train: 73.6781 loss_val: 71.3309\n",
      "Epoch: 065 loss_train: 73.7587 loss_val: 71.2588\n",
      "Epoch: 066 loss_train: 73.8421 loss_val: 71.1883\n",
      "Epoch: 067 loss_train: 74.1442 loss_val: 71.1476\n",
      "Epoch: 068 loss_train: 73.7259 loss_val: 71.1271\n",
      "Epoch: 069 loss_train: 74.0715 loss_val: 71.2169\n",
      "Epoch: 070 loss_train: 73.9321 loss_val: 71.2801\n",
      "Epoch: 071 loss_train: 73.9917 loss_val: 71.2141\n",
      "Epoch: 072 loss_train: 73.6438 loss_val: 71.1892\n",
      "Epoch: 073 loss_train: 73.8713 loss_val: 71.1480\n",
      "Epoch: 074 loss_train: 73.6474 loss_val: 71.1931\n",
      "Epoch: 075 loss_train: 73.6388 loss_val: 71.2220\n",
      "Epoch: 076 loss_train: 73.5041 loss_val: 71.1040\n",
      "Epoch: 077 loss_train: 73.3854 loss_val: 71.1783\n",
      "Epoch: 078 loss_train: 73.5414 loss_val: 71.1504\n",
      "Epoch: 079 loss_train: 73.6686 loss_val: 71.1970\n",
      "Epoch: 080 loss_train: 73.8521 loss_val: 71.2128\n",
      "Epoch: 081 loss_train: 73.5408 loss_val: 71.2764\n",
      "Epoch: 082 loss_train: 73.8213 loss_val: 71.2922\n",
      "Epoch: 083 loss_train: 73.2516 loss_val: 71.2280\n",
      "Epoch: 084 loss_train: 73.4711 loss_val: 71.2335\n",
      "Epoch: 085 loss_train: 73.3992 loss_val: 71.1811\n",
      "Epoch: 086 loss_train: 73.3692 loss_val: 71.0594\n",
      "Epoch: 087 loss_train: 73.6745 loss_val: 71.1945\n",
      "Epoch: 088 loss_train: 73.3901 loss_val: 71.0006\n",
      "Epoch: 089 loss_train: 73.5070 loss_val: 70.9688\n",
      "Epoch: 090 loss_train: 73.4162 loss_val: 71.1979\n",
      "Epoch: 091 loss_train: 73.7661 loss_val: 71.2347\n",
      "Epoch: 092 loss_train: 73.6247 loss_val: 70.9765\n",
      "Epoch: 093 loss_train: 73.0620 loss_val: 70.9831\n",
      "Epoch: 094 loss_train: 73.6414 loss_val: 71.2333\n",
      "Epoch: 095 loss_train: 73.4481 loss_val: 71.1301\n",
      "Epoch: 096 loss_train: 73.5133 loss_val: 70.9815\n",
      "Epoch: 097 loss_train: 73.2092 loss_val: 71.1541\n",
      "Epoch: 098 loss_train: 73.3138 loss_val: 71.1803\n",
      "Epoch: 099 loss_train: 73.2108 loss_val: 71.0137\n",
      "Epoch: 100 loss_train: 73.1370 loss_val: 71.0593\n",
      "Epoch: 101 loss_train: 73.0699 loss_val: 71.0680\n",
      "Epoch: 102 loss_train: 72.8276 loss_val: 70.9430\n",
      "Epoch: 103 loss_train: 72.9243 loss_val: 70.9240\n",
      "Epoch: 104 loss_train: 73.3840 loss_val: 70.9194\n",
      "Epoch: 105 loss_train: 73.5495 loss_val: 70.9777\n",
      "Epoch: 106 loss_train: 73.5018 loss_val: 71.0000\n",
      "Epoch: 107 loss_train: 73.2005 loss_val: 71.0177\n",
      "Epoch: 108 loss_train: 73.3080 loss_val: 70.8862\n",
      "Epoch: 109 loss_train: 72.8853 loss_val: 70.7910\n",
      "Epoch: 110 loss_train: 73.4228 loss_val: 71.3603\n",
      "Epoch: 111 loss_train: 73.2043 loss_val: 71.0061\n",
      "Epoch: 112 loss_train: 73.3795 loss_val: 70.8740\n",
      "Epoch: 113 loss_train: 73.0900 loss_val: 70.9914\n",
      "Epoch: 114 loss_train: 73.1659 loss_val: 71.3039\n",
      "Epoch: 115 loss_train: 72.9558 loss_val: 71.1352\n",
      "Epoch: 116 loss_train: 72.9247 loss_val: 70.8068\n",
      "Epoch: 117 loss_train: 73.5892 loss_val: 70.8280\n",
      "Epoch: 118 loss_train: 73.0502 loss_val: 71.0080\n",
      "Epoch: 119 loss_train: 72.6870 loss_val: 71.0834\n",
      "Epoch: 120 loss_train: 73.3296 loss_val: 70.9763\n",
      "Epoch: 121 loss_train: 72.7490 loss_val: 70.9184\n",
      "Epoch: 122 loss_train: 73.0936 loss_val: 71.1238\n",
      "Epoch: 123 loss_train: 72.8737 loss_val: 70.9489\n",
      "Epoch: 124 loss_train: 73.0716 loss_val: 70.8602\n",
      "Epoch: 125 loss_train: 72.8861 loss_val: 70.8951\n",
      "Epoch: 126 loss_train: 73.4022 loss_val: 71.0056\n",
      "Epoch: 127 loss_train: 72.8970 loss_val: 70.8862\n",
      "Epoch: 128 loss_train: 72.7324 loss_val: 70.9411\n",
      "Epoch: 129 loss_train: 72.8254 loss_val: 70.9088\n",
      "Epoch: 130 loss_train: 72.5776 loss_val: 70.7973\n",
      "Epoch: 131 loss_train: 72.5318 loss_val: 70.7261\n",
      "Epoch: 132 loss_train: 72.8271 loss_val: 71.0500\n",
      "Epoch: 133 loss_train: 72.6090 loss_val: 70.8498\n",
      "Epoch: 134 loss_train: 72.7890 loss_val: 70.7364\n",
      "Epoch: 135 loss_train: 73.1824 loss_val: 71.0421\n",
      "Epoch: 136 loss_train: 72.3254 loss_val: 71.1142\n",
      "Epoch: 137 loss_train: 72.5300 loss_val: 70.7291\n",
      "Epoch: 138 loss_train: 72.8759 loss_val: 70.7848\n",
      "Epoch: 139 loss_train: 73.2099 loss_val: 71.1334\n",
      "Epoch: 140 loss_train: 72.5606 loss_val: 70.7648\n",
      "Epoch: 141 loss_train: 72.3927 loss_val: 70.5527\n",
      "Epoch: 142 loss_train: 73.0638 loss_val: 71.0463\n",
      "Epoch: 143 loss_train: 72.4370 loss_val: 71.1873\n",
      "Epoch: 144 loss_train: 72.4707 loss_val: 70.6390\n",
      "Epoch: 145 loss_train: 72.6289 loss_val: 70.7637\n",
      "Epoch: 146 loss_train: 72.5119 loss_val: 70.9344\n",
      "Epoch: 147 loss_train: 72.6677 loss_val: 70.6975\n",
      "Epoch: 148 loss_train: 72.5418 loss_val: 70.6772\n",
      "Epoch: 149 loss_train: 72.7994 loss_val: 70.9085\n",
      "Epoch: 150 loss_train: 72.4832 loss_val: 70.9428\n",
      "Epoch: 151 loss_train: 72.3862 loss_val: 70.6674\n",
      "Epoch: 152 loss_train: 72.2435 loss_val: 70.7210\n",
      "Epoch: 153 loss_train: 72.3161 loss_val: 70.8725\n",
      "Epoch: 154 loss_train: 72.5427 loss_val: 70.6893\n",
      "Epoch: 155 loss_train: 72.5496 loss_val: 70.6006\n",
      "Epoch: 156 loss_train: 72.3333 loss_val: 70.7503\n",
      "Epoch: 157 loss_train: 72.5169 loss_val: 70.7569\n",
      "Epoch: 158 loss_train: 72.4642 loss_val: 70.6972\n",
      "Epoch: 159 loss_train: 72.2037 loss_val: 70.7017\n",
      "Epoch: 160 loss_train: 72.5743 loss_val: 70.6839\n",
      "Epoch: 161 loss_train: 72.6126 loss_val: 70.6750\n",
      "Epoch: 162 loss_train: 72.4494 loss_val: 70.7614\n",
      "Epoch: 163 loss_train: 72.1591 loss_val: 70.6769\n",
      "Epoch: 164 loss_train: 72.3123 loss_val: 70.6775\n",
      "Epoch: 165 loss_train: 72.5850 loss_val: 70.5930\n",
      "Epoch: 166 loss_train: 72.1505 loss_val: 70.6864\n",
      "Epoch: 167 loss_train: 72.2172 loss_val: 70.5621\n",
      "Epoch: 168 loss_train: 72.2928 loss_val: 70.5635\n",
      "Epoch: 169 loss_train: 72.2350 loss_val: 70.8163\n",
      "Epoch: 170 loss_train: 72.1904 loss_val: 70.4879\n",
      "Epoch: 171 loss_train: 72.3418 loss_val: 70.4394\n",
      "Epoch: 172 loss_train: 72.0176 loss_val: 70.6065\n",
      "Epoch: 173 loss_train: 72.3598 loss_val: 70.5904\n",
      "Epoch: 174 loss_train: 71.9336 loss_val: 70.4998\n",
      "Epoch: 175 loss_train: 72.1967 loss_val: 70.5293\n",
      "Epoch: 176 loss_train: 72.4722 loss_val: 70.6451\n",
      "Epoch: 177 loss_train: 71.8209 loss_val: 70.3688\n",
      "Epoch: 178 loss_train: 72.0223 loss_val: 70.4983\n",
      "Epoch: 179 loss_train: 72.1847 loss_val: 70.6374\n",
      "Epoch: 180 loss_train: 72.3787 loss_val: 70.5893\n",
      "Epoch: 181 loss_train: 72.1721 loss_val: 70.5514\n",
      "Epoch: 182 loss_train: 72.1278 loss_val: 70.6038\n",
      "Epoch: 183 loss_train: 71.8379 loss_val: 70.7551\n",
      "Epoch: 184 loss_train: 71.9365 loss_val: 70.4463\n",
      "Epoch: 185 loss_train: 72.0561 loss_val: 70.3954\n",
      "Epoch: 186 loss_train: 72.0224 loss_val: 70.6566\n",
      "Epoch: 187 loss_train: 71.9541 loss_val: 70.4489\n",
      "Epoch: 188 loss_train: 71.4985 loss_val: 70.3100\n",
      "Epoch: 189 loss_train: 72.3898 loss_val: 70.4927\n",
      "Epoch: 190 loss_train: 72.3182 loss_val: 70.3623\n",
      "Epoch: 191 loss_train: 71.7615 loss_val: 70.5872\n",
      "Epoch: 192 loss_train: 71.7043 loss_val: 70.4156\n",
      "Epoch: 193 loss_train: 72.0935 loss_val: 70.4376\n",
      "Epoch: 194 loss_train: 71.8893 loss_val: 70.7421\n",
      "Epoch: 195 loss_train: 71.7975 loss_val: 70.6977\n",
      "Epoch: 196 loss_train: 72.2550 loss_val: 70.3445\n",
      "Epoch: 197 loss_train: 71.7127 loss_val: 70.3367\n",
      "Epoch: 198 loss_train: 71.9621 loss_val: 70.5237\n",
      "Epoch: 199 loss_train: 71.7409 loss_val: 70.3941\n",
      "Epoch: 200 loss_train: 71.6318 loss_val: 70.2964\n",
      "Epoch: 001 loss_train: 71.9245 loss_val: 77.8427\n",
      "Epoch: 002 loss_train: 76.6882 loss_val: 71.4653\n",
      "Epoch: 003 loss_train: 72.0385 loss_val: 72.3359\n",
      "Epoch: 004 loss_train: 75.8089 loss_val: 70.1874\n",
      "Epoch: 005 loss_train: 72.6578 loss_val: 70.7234\n",
      "Epoch: 006 loss_train: 71.9462 loss_val: 72.0438\n",
      "Epoch: 007 loss_train: 72.4853 loss_val: 72.9719\n",
      "Epoch: 008 loss_train: 72.9308 loss_val: 73.1546\n",
      "Epoch: 009 loss_train: 73.3403 loss_val: 72.6718\n",
      "Epoch: 010 loss_train: 72.9436 loss_val: 71.9224\n",
      "Epoch: 011 loss_train: 72.4943 loss_val: 71.3057\n",
      "Epoch: 012 loss_train: 72.0763 loss_val: 71.1191\n",
      "Epoch: 013 loss_train: 72.6056 loss_val: 71.1869\n",
      "Epoch: 014 loss_train: 72.3363 loss_val: 71.1809\n",
      "Epoch: 015 loss_train: 72.5420 loss_val: 71.1239\n",
      "Epoch: 016 loss_train: 72.3760 loss_val: 71.2185\n",
      "Epoch: 017 loss_train: 71.9942 loss_val: 71.5462\n",
      "Epoch: 018 loss_train: 72.3566 loss_val: 71.8879\n",
      "Epoch: 019 loss_train: 72.2770 loss_val: 71.9846\n",
      "Epoch: 020 loss_train: 72.3330 loss_val: 71.7671\n",
      "Epoch: 021 loss_train: 72.1382 loss_val: 71.3493\n",
      "Epoch: 022 loss_train: 71.9609 loss_val: 70.9090\n",
      "Epoch: 023 loss_train: 71.7513 loss_val: 70.6095\n",
      "Epoch: 024 loss_train: 71.7522 loss_val: 70.4812\n",
      "Epoch: 025 loss_train: 71.6462 loss_val: 70.4796\n",
      "Epoch: 026 loss_train: 71.9875 loss_val: 70.5405\n",
      "Epoch: 027 loss_train: 71.7445 loss_val: 70.6853\n",
      "Epoch: 028 loss_train: 71.4723 loss_val: 70.9053\n",
      "Epoch: 029 loss_train: 71.4931 loss_val: 71.0392\n",
      "Epoch: 030 loss_train: 71.5385 loss_val: 70.9776\n",
      "Epoch: 031 loss_train: 71.6993 loss_val: 70.7466\n",
      "Epoch: 032 loss_train: 71.5393 loss_val: 70.4645\n",
      "Epoch: 033 loss_train: 71.4291 loss_val: 70.2615\n",
      "Epoch: 034 loss_train: 71.7323 loss_val: 70.1506\n",
      "Epoch: 035 loss_train: 71.5704 loss_val: 70.0677\n",
      "Epoch: 036 loss_train: 71.4533 loss_val: 70.0737\n",
      "Epoch: 037 loss_train: 71.7636 loss_val: 70.1712\n",
      "Epoch: 038 loss_train: 71.0443 loss_val: 70.2988\n",
      "Epoch: 039 loss_train: 71.4815 loss_val: 70.3147\n",
      "Epoch: 040 loss_train: 71.7608 loss_val: 70.2248\n",
      "Epoch: 041 loss_train: 71.2739 loss_val: 70.1382\n",
      "Epoch: 042 loss_train: 71.1006 loss_val: 70.0462\n",
      "Epoch: 043 loss_train: 71.5863 loss_val: 70.0785\n",
      "Epoch: 044 loss_train: 71.2752 loss_val: 70.1113\n",
      "Epoch: 045 loss_train: 71.2527 loss_val: 70.0982\n",
      "Epoch: 046 loss_train: 70.9168 loss_val: 70.0643\n",
      "Epoch: 047 loss_train: 71.5088 loss_val: 70.0642\n",
      "Epoch: 048 loss_train: 71.3349 loss_val: 70.1747\n",
      "Epoch: 049 loss_train: 71.2305 loss_val: 70.2715\n",
      "Epoch: 050 loss_train: 71.3142 loss_val: 70.2843\n",
      "Epoch: 051 loss_train: 71.3359 loss_val: 70.2351\n",
      "Epoch: 052 loss_train: 71.2246 loss_val: 70.1770\n",
      "Epoch: 053 loss_train: 71.3342 loss_val: 70.1468\n",
      "Epoch: 054 loss_train: 71.4584 loss_val: 70.1644\n",
      "Epoch: 055 loss_train: 71.4900 loss_val: 70.2645\n",
      "Epoch: 056 loss_train: 71.1724 loss_val: 70.2161\n",
      "Epoch: 057 loss_train: 71.4363 loss_val: 70.1228\n",
      "Epoch: 058 loss_train: 71.2143 loss_val: 70.1062\n",
      "Epoch: 059 loss_train: 71.3408 loss_val: 70.1323\n",
      "Epoch: 060 loss_train: 71.3626 loss_val: 70.2195\n",
      "Epoch: 061 loss_train: 70.8601 loss_val: 70.3085\n",
      "Epoch: 062 loss_train: 70.9944 loss_val: 70.2354\n",
      "Epoch: 063 loss_train: 71.5835 loss_val: 70.1394\n",
      "Epoch: 064 loss_train: 71.3335 loss_val: 70.1300\n",
      "Epoch: 065 loss_train: 71.3370 loss_val: 70.0835\n",
      "Epoch: 066 loss_train: 71.2078 loss_val: 70.1104\n",
      "Epoch: 067 loss_train: 70.9733 loss_val: 70.1478\n",
      "Epoch: 068 loss_train: 71.0276 loss_val: 70.2191\n",
      "Epoch: 069 loss_train: 70.8945 loss_val: 70.1971\n",
      "Epoch: 070 loss_train: 70.9469 loss_val: 70.1568\n",
      "Epoch: 071 loss_train: 70.9276 loss_val: 70.1454\n",
      "Epoch: 072 loss_train: 71.1477 loss_val: 70.1232\n",
      "Epoch: 073 loss_train: 71.3868 loss_val: 70.0539\n",
      "Epoch: 074 loss_train: 71.1944 loss_val: 70.1210\n",
      "Epoch: 075 loss_train: 70.8520 loss_val: 70.1387\n",
      "Epoch: 076 loss_train: 70.8865 loss_val: 70.1951\n",
      "Epoch: 077 loss_train: 71.5383 loss_val: 70.1710\n",
      "Epoch: 078 loss_train: 71.3614 loss_val: 70.0439\n",
      "Epoch: 079 loss_train: 70.9983 loss_val: 70.0091\n",
      "Epoch: 080 loss_train: 71.0744 loss_val: 70.1004\n",
      "Epoch: 081 loss_train: 71.0825 loss_val: 70.1994\n",
      "Epoch: 082 loss_train: 70.9004 loss_val: 70.0364\n",
      "Epoch: 083 loss_train: 70.9315 loss_val: 69.9912\n",
      "Epoch: 084 loss_train: 70.4757 loss_val: 70.1232\n",
      "Epoch: 085 loss_train: 70.5825 loss_val: 70.1674\n",
      "Epoch: 086 loss_train: 70.7591 loss_val: 70.1057\n",
      "Epoch: 087 loss_train: 70.7592 loss_val: 70.0460\n",
      "Epoch: 088 loss_train: 70.7651 loss_val: 70.0576\n",
      "Epoch: 089 loss_train: 70.9309 loss_val: 70.1525\n",
      "Epoch: 090 loss_train: 71.1472 loss_val: 70.0814\n",
      "Epoch: 091 loss_train: 70.5512 loss_val: 70.0427\n",
      "Epoch: 092 loss_train: 71.1467 loss_val: 70.0543\n",
      "Epoch: 093 loss_train: 71.0441 loss_val: 70.2133\n",
      "Epoch: 094 loss_train: 71.0610 loss_val: 70.1993\n",
      "Epoch: 095 loss_train: 70.8104 loss_val: 70.0338\n",
      "Epoch: 096 loss_train: 70.3874 loss_val: 69.9687\n",
      "Epoch: 097 loss_train: 71.1568 loss_val: 70.0802\n",
      "Epoch: 098 loss_train: 71.1270 loss_val: 70.2025\n",
      "Epoch: 099 loss_train: 70.3009 loss_val: 70.1339\n",
      "Epoch: 100 loss_train: 70.7515 loss_val: 70.1167\n",
      "Epoch: 101 loss_train: 70.7495 loss_val: 70.0665\n",
      "Epoch: 102 loss_train: 71.0099 loss_val: 70.0961\n",
      "Epoch: 103 loss_train: 70.5128 loss_val: 70.1258\n",
      "Epoch: 104 loss_train: 70.6900 loss_val: 70.0495\n",
      "Epoch: 105 loss_train: 70.9178 loss_val: 70.0234\n",
      "Epoch: 106 loss_train: 71.1386 loss_val: 70.0075\n",
      "Epoch: 107 loss_train: 70.8183 loss_val: 69.9501\n",
      "Epoch: 108 loss_train: 70.4447 loss_val: 70.0086\n",
      "Epoch: 109 loss_train: 70.7304 loss_val: 70.0657\n",
      "Epoch: 110 loss_train: 70.8526 loss_val: 70.1246\n",
      "Epoch: 111 loss_train: 70.8488 loss_val: 70.1420\n",
      "Epoch: 112 loss_train: 70.5118 loss_val: 70.0715\n",
      "Epoch: 113 loss_train: 71.0089 loss_val: 70.0700\n",
      "Epoch: 114 loss_train: 70.6238 loss_val: 69.9926\n",
      "Epoch: 115 loss_train: 70.4964 loss_val: 69.9852\n",
      "Epoch: 116 loss_train: 70.9263 loss_val: 70.1398\n",
      "Epoch: 117 loss_train: 70.5687 loss_val: 70.1795\n",
      "Epoch: 118 loss_train: 70.9157 loss_val: 70.0749\n",
      "Epoch: 119 loss_train: 70.6450 loss_val: 70.0542\n",
      "Epoch: 120 loss_train: 70.7772 loss_val: 70.0172\n",
      "Epoch: 121 loss_train: 70.3441 loss_val: 69.9972\n",
      "Epoch: 122 loss_train: 70.5267 loss_val: 69.9565\n",
      "Epoch: 123 loss_train: 70.5974 loss_val: 69.9074\n",
      "Epoch: 124 loss_train: 70.6435 loss_val: 70.0066\n",
      "Epoch: 125 loss_train: 70.3611 loss_val: 70.1619\n",
      "Epoch: 126 loss_train: 70.5413 loss_val: 70.1131\n",
      "Epoch: 127 loss_train: 70.1125 loss_val: 69.9484\n",
      "Epoch: 128 loss_train: 70.6270 loss_val: 70.0602\n",
      "Epoch: 129 loss_train: 70.4885 loss_val: 70.1633\n",
      "Epoch: 130 loss_train: 70.5681 loss_val: 69.9759\n",
      "Epoch: 131 loss_train: 70.5561 loss_val: 69.8524\n",
      "Epoch: 132 loss_train: 70.1566 loss_val: 69.8927\n",
      "Epoch: 133 loss_train: 70.4326 loss_val: 70.1397\n",
      "Epoch: 134 loss_train: 70.8037 loss_val: 69.9634\n",
      "Epoch: 135 loss_train: 70.1960 loss_val: 69.8103\n",
      "Epoch: 136 loss_train: 70.5064 loss_val: 69.9467\n",
      "Epoch: 137 loss_train: 70.4415 loss_val: 70.3173\n",
      "Epoch: 138 loss_train: 70.4098 loss_val: 70.0913\n",
      "Epoch: 139 loss_train: 70.1408 loss_val: 69.8847\n",
      "Epoch: 140 loss_train: 70.1192 loss_val: 69.9958\n",
      "Epoch: 141 loss_train: 70.5211 loss_val: 70.1916\n",
      "Epoch: 142 loss_train: 70.5452 loss_val: 69.9918\n",
      "Epoch: 143 loss_train: 70.7877 loss_val: 69.8862\n",
      "Epoch: 144 loss_train: 70.7678 loss_val: 70.0076\n",
      "Epoch: 145 loss_train: 70.2651 loss_val: 70.0556\n",
      "Epoch: 146 loss_train: 70.3164 loss_val: 69.8694\n",
      "Epoch: 147 loss_train: 70.4776 loss_val: 69.8302\n",
      "Epoch: 148 loss_train: 70.5107 loss_val: 70.0113\n",
      "Epoch: 149 loss_train: 70.0957 loss_val: 69.8930\n",
      "Epoch: 150 loss_train: 70.7693 loss_val: 69.8261\n",
      "Epoch: 151 loss_train: 70.1281 loss_val: 69.9020\n",
      "Epoch: 152 loss_train: 70.4582 loss_val: 70.0028\n",
      "Epoch: 153 loss_train: 70.4090 loss_val: 69.8757\n",
      "Epoch: 154 loss_train: 69.9163 loss_val: 69.8459\n",
      "Epoch: 155 loss_train: 70.3454 loss_val: 69.9054\n",
      "Epoch: 156 loss_train: 70.3171 loss_val: 69.8153\n",
      "Epoch: 157 loss_train: 70.0538 loss_val: 69.8379\n",
      "Epoch: 158 loss_train: 70.4466 loss_val: 69.8876\n",
      "Epoch: 159 loss_train: 70.1425 loss_val: 69.8289\n",
      "Epoch: 160 loss_train: 70.2621 loss_val: 69.9293\n",
      "Epoch: 161 loss_train: 70.2400 loss_val: 69.9281\n",
      "Epoch: 162 loss_train: 70.3538 loss_val: 69.7684\n",
      "Epoch: 163 loss_train: 70.2374 loss_val: 69.7363\n",
      "Epoch: 164 loss_train: 70.0468 loss_val: 70.0019\n",
      "Epoch: 165 loss_train: 70.4774 loss_val: 69.9888\n",
      "Epoch: 166 loss_train: 69.8738 loss_val: 69.7448\n",
      "Epoch: 167 loss_train: 69.8683 loss_val: 69.7168\n",
      "Epoch: 168 loss_train: 70.1782 loss_val: 69.8513\n",
      "Epoch: 169 loss_train: 70.5145 loss_val: 69.8426\n",
      "Epoch: 170 loss_train: 70.3254 loss_val: 69.6416\n",
      "Epoch: 171 loss_train: 70.4881 loss_val: 69.6869\n",
      "Epoch: 172 loss_train: 69.8850 loss_val: 69.8519\n",
      "Epoch: 173 loss_train: 70.0653 loss_val: 69.7704\n",
      "Epoch: 174 loss_train: 70.0620 loss_val: 69.7487\n",
      "Epoch: 175 loss_train: 70.0111 loss_val: 69.7949\n",
      "Epoch: 176 loss_train: 70.0326 loss_val: 69.9055\n",
      "Epoch: 177 loss_train: 70.0266 loss_val: 69.7228\n",
      "Epoch: 178 loss_train: 70.1098 loss_val: 69.6411\n",
      "Epoch: 179 loss_train: 69.9291 loss_val: 69.7603\n",
      "Epoch: 180 loss_train: 70.2635 loss_val: 69.7746\n",
      "Epoch: 181 loss_train: 69.7943 loss_val: 69.7857\n",
      "Epoch: 182 loss_train: 69.9181 loss_val: 69.6820\n",
      "Epoch: 183 loss_train: 69.8399 loss_val: 69.7875\n",
      "Epoch: 184 loss_train: 70.1010 loss_val: 69.8850\n",
      "Epoch: 185 loss_train: 69.6888 loss_val: 69.6709\n",
      "Epoch: 186 loss_train: 69.9353 loss_val: 69.7419\n",
      "Epoch: 187 loss_train: 69.7550 loss_val: 69.9467\n",
      "Epoch: 188 loss_train: 69.8862 loss_val: 69.7846\n",
      "Epoch: 189 loss_train: 69.5437 loss_val: 69.7042\n",
      "Epoch: 190 loss_train: 69.9487 loss_val: 69.8009\n",
      "Epoch: 191 loss_train: 70.0250 loss_val: 69.9077\n",
      "Epoch: 192 loss_train: 69.7949 loss_val: 69.6011\n",
      "Epoch: 193 loss_train: 69.8344 loss_val: 69.6411\n",
      "Epoch: 194 loss_train: 69.7878 loss_val: 69.9016\n",
      "Epoch: 195 loss_train: 70.0117 loss_val: 69.7900\n",
      "Epoch: 196 loss_train: 69.8921 loss_val: 69.6584\n",
      "Epoch: 197 loss_train: 70.1859 loss_val: 69.8469\n",
      "Epoch: 198 loss_train: 69.8036 loss_val: 69.8224\n",
      "Epoch: 199 loss_train: 69.8041 loss_val: 69.6468\n",
      "Epoch: 200 loss_train: 69.6490 loss_val: 69.6859\n",
      "Epoch: 001 loss_train: 69.8672 loss_val: 71.0626\n",
      "Epoch: 002 loss_train: 70.1967 loss_val: 69.6158\n",
      "Epoch: 003 loss_train: 70.1315 loss_val: 69.5299\n",
      "Epoch: 004 loss_train: 70.3259 loss_val: 70.0276\n",
      "Epoch: 005 loss_train: 70.1212 loss_val: 70.8776\n",
      "Epoch: 006 loss_train: 70.3883 loss_val: 70.7478\n",
      "Epoch: 007 loss_train: 69.7358 loss_val: 70.0310\n",
      "Epoch: 008 loss_train: 69.9629 loss_val: 69.6309\n",
      "Epoch: 009 loss_train: 69.9295 loss_val: 69.5798\n",
      "Epoch: 010 loss_train: 69.6678 loss_val: 69.6494\n",
      "Epoch: 011 loss_train: 70.0022 loss_val: 70.0211\n",
      "Epoch: 012 loss_train: 70.0434 loss_val: 70.2984\n",
      "Epoch: 013 loss_train: 69.7208 loss_val: 70.1544\n",
      "Epoch: 014 loss_train: 69.8982 loss_val: 69.8066\n",
      "Epoch: 015 loss_train: 69.7415 loss_val: 69.5769\n",
      "Epoch: 016 loss_train: 69.8343 loss_val: 69.5283\n",
      "Epoch: 017 loss_train: 70.2044 loss_val: 69.6199\n",
      "Epoch: 018 loss_train: 69.8496 loss_val: 69.8567\n",
      "Epoch: 019 loss_train: 69.7091 loss_val: 70.0344\n",
      "Epoch: 020 loss_train: 69.6670 loss_val: 69.9998\n",
      "Epoch: 021 loss_train: 69.6799 loss_val: 69.8484\n",
      "Epoch: 022 loss_train: 69.8916 loss_val: 69.7530\n",
      "Epoch: 023 loss_train: 69.6795 loss_val: 69.7357\n",
      "Epoch: 024 loss_train: 69.7552 loss_val: 69.7639\n",
      "Epoch: 025 loss_train: 69.5731 loss_val: 69.8511\n",
      "Epoch: 026 loss_train: 69.6826 loss_val: 69.9084\n",
      "Epoch: 027 loss_train: 69.5110 loss_val: 69.8645\n",
      "Epoch: 028 loss_train: 69.6269 loss_val: 69.7471\n",
      "Epoch: 029 loss_train: 69.4361 loss_val: 69.6541\n",
      "Epoch: 030 loss_train: 69.7202 loss_val: 69.6240\n",
      "Epoch: 031 loss_train: 69.3892 loss_val: 69.6764\n",
      "Epoch: 032 loss_train: 70.1063 loss_val: 69.7710\n",
      "Epoch: 033 loss_train: 69.7215 loss_val: 69.8369\n",
      "Epoch: 034 loss_train: 69.6257 loss_val: 69.8312\n",
      "Epoch: 035 loss_train: 69.5519 loss_val: 69.7645\n",
      "Epoch: 036 loss_train: 69.5548 loss_val: 69.7372\n",
      "Epoch: 037 loss_train: 69.6206 loss_val: 69.7418\n",
      "Epoch: 038 loss_train: 69.6390 loss_val: 69.7903\n",
      "Epoch: 039 loss_train: 69.8839 loss_val: 69.8296\n",
      "Epoch: 040 loss_train: 69.4417 loss_val: 69.8475\n",
      "Epoch: 041 loss_train: 69.5571 loss_val: 69.8158\n",
      "Epoch: 042 loss_train: 69.9417 loss_val: 69.7387\n",
      "Epoch: 043 loss_train: 69.5586 loss_val: 69.6853\n",
      "Epoch: 044 loss_train: 69.5029 loss_val: 69.6909\n",
      "Epoch: 045 loss_train: 69.6601 loss_val: 69.7078\n",
      "Epoch: 046 loss_train: 69.3783 loss_val: 69.7900\n",
      "Epoch: 047 loss_train: 69.4372 loss_val: 69.8437\n",
      "Epoch: 048 loss_train: 69.5616 loss_val: 69.7609\n",
      "Epoch: 049 loss_train: 69.8907 loss_val: 69.6517\n",
      "Epoch: 050 loss_train: 69.5345 loss_val: 69.6166\n",
      "Epoch: 051 loss_train: 69.5214 loss_val: 69.6533\n",
      "Epoch: 052 loss_train: 69.8014 loss_val: 69.7278\n",
      "Epoch: 053 loss_train: 69.3681 loss_val: 69.7862\n",
      "Epoch: 054 loss_train: 69.2171 loss_val: 69.7492\n",
      "Epoch: 055 loss_train: 69.7622 loss_val: 69.7026\n",
      "Epoch: 056 loss_train: 69.5646 loss_val: 69.6496\n",
      "Epoch: 057 loss_train: 69.2867 loss_val: 69.6961\n",
      "Epoch: 058 loss_train: 69.3705 loss_val: 69.7762\n",
      "Epoch: 059 loss_train: 69.7009 loss_val: 69.8132\n",
      "Epoch: 060 loss_train: 69.8066 loss_val: 69.7405\n",
      "Epoch: 061 loss_train: 69.5883 loss_val: 69.7110\n",
      "Epoch: 062 loss_train: 69.5470 loss_val: 69.7205\n",
      "Epoch: 063 loss_train: 68.8637 loss_val: 69.6750\n",
      "Epoch: 064 loss_train: 69.1816 loss_val: 69.6228\n",
      "Epoch: 065 loss_train: 69.4066 loss_val: 69.5831\n",
      "Epoch: 066 loss_train: 69.5404 loss_val: 69.6075\n",
      "Epoch: 067 loss_train: 69.4404 loss_val: 69.6858\n",
      "Epoch: 068 loss_train: 69.5117 loss_val: 69.6604\n",
      "Epoch: 069 loss_train: 69.6265 loss_val: 69.6942\n",
      "Epoch: 070 loss_train: 69.2305 loss_val: 69.7055\n",
      "Epoch: 071 loss_train: 69.5359 loss_val: 69.6821\n",
      "Epoch: 072 loss_train: 69.2982 loss_val: 69.6412\n",
      "Epoch: 073 loss_train: 69.9168 loss_val: 69.6437\n",
      "Epoch: 074 loss_train: 69.6223 loss_val: 69.6914\n",
      "Epoch: 075 loss_train: 69.3414 loss_val: 69.7107\n",
      "Epoch: 076 loss_train: 69.5880 loss_val: 69.6438\n",
      "Epoch: 077 loss_train: 69.8008 loss_val: 69.6060\n",
      "Epoch: 078 loss_train: 69.6855 loss_val: 69.7583\n",
      "Epoch: 079 loss_train: 69.6961 loss_val: 69.7342\n",
      "Epoch: 080 loss_train: 69.5199 loss_val: 69.5740\n",
      "Epoch: 081 loss_train: 69.3173 loss_val: 69.4949\n",
      "Epoch: 082 loss_train: 69.3078 loss_val: 69.5642\n",
      "Epoch: 083 loss_train: 69.3970 loss_val: 69.7144\n",
      "Epoch: 084 loss_train: 68.9035 loss_val: 69.7624\n",
      "Epoch: 085 loss_train: 69.4321 loss_val: 69.6865\n",
      "Epoch: 086 loss_train: 69.7429 loss_val: 69.6109\n",
      "Epoch: 087 loss_train: 69.3377 loss_val: 69.6630\n",
      "Epoch: 088 loss_train: 69.2449 loss_val: 69.7146\n",
      "Epoch: 089 loss_train: 69.6351 loss_val: 69.6650\n",
      "Epoch: 090 loss_train: 69.1837 loss_val: 69.6157\n",
      "Epoch: 091 loss_train: 69.3907 loss_val: 69.6144\n",
      "Epoch: 092 loss_train: 69.2160 loss_val: 69.6868\n",
      "Epoch: 093 loss_train: 69.3885 loss_val: 69.7336\n",
      "Epoch: 094 loss_train: 69.4254 loss_val: 69.5966\n",
      "Epoch: 095 loss_train: 69.3659 loss_val: 69.4815\n",
      "Epoch: 096 loss_train: 69.2309 loss_val: 69.5240\n",
      "Epoch: 097 loss_train: 69.0159 loss_val: 69.6202\n",
      "Epoch: 098 loss_train: 69.2673 loss_val: 69.6428\n",
      "Epoch: 099 loss_train: 69.1379 loss_val: 69.5619\n",
      "Epoch: 100 loss_train: 69.2410 loss_val: 69.4827\n",
      "Epoch: 101 loss_train: 69.3002 loss_val: 69.5304\n",
      "Epoch: 102 loss_train: 69.3942 loss_val: 69.6283\n",
      "Epoch: 103 loss_train: 69.3252 loss_val: 69.6661\n",
      "Epoch: 104 loss_train: 69.4907 loss_val: 69.5849\n",
      "Epoch: 105 loss_train: 69.3781 loss_val: 69.5442\n",
      "Epoch: 106 loss_train: 69.2990 loss_val: 69.5005\n",
      "Epoch: 107 loss_train: 69.4665 loss_val: 69.6354\n",
      "Epoch: 108 loss_train: 69.4997 loss_val: 69.7518\n",
      "Epoch: 109 loss_train: 69.2812 loss_val: 69.6669\n",
      "Epoch: 110 loss_train: 69.4052 loss_val: 69.5481\n",
      "Epoch: 111 loss_train: 69.3296 loss_val: 69.4777\n",
      "Epoch: 112 loss_train: 69.0483 loss_val: 69.5518\n",
      "Epoch: 113 loss_train: 69.4609 loss_val: 69.6928\n",
      "Epoch: 114 loss_train: 69.3020 loss_val: 69.5894\n",
      "Epoch: 115 loss_train: 69.2432 loss_val: 69.4336\n",
      "Epoch: 116 loss_train: 69.0513 loss_val: 69.4595\n",
      "Epoch: 117 loss_train: 69.1034 loss_val: 69.6070\n",
      "Epoch: 118 loss_train: 69.2384 loss_val: 69.6797\n",
      "Epoch: 119 loss_train: 69.0816 loss_val: 69.6494\n",
      "Epoch: 120 loss_train: 69.2346 loss_val: 69.6050\n",
      "Epoch: 121 loss_train: 68.8903 loss_val: 69.5770\n",
      "Epoch: 122 loss_train: 68.8152 loss_val: 69.5637\n",
      "Epoch: 123 loss_train: 69.2125 loss_val: 69.4997\n",
      "Epoch: 124 loss_train: 69.1280 loss_val: 69.4823\n",
      "Epoch: 125 loss_train: 68.8621 loss_val: 69.4965\n",
      "Epoch: 126 loss_train: 69.2196 loss_val: 69.4988\n",
      "Epoch: 127 loss_train: 68.9934 loss_val: 69.5053\n",
      "Epoch: 128 loss_train: 69.2764 loss_val: 69.5742\n",
      "Epoch: 129 loss_train: 68.8902 loss_val: 69.6319\n",
      "Epoch: 130 loss_train: 69.2442 loss_val: 69.5426\n",
      "Epoch: 131 loss_train: 69.1108 loss_val: 69.4345\n",
      "Epoch: 132 loss_train: 69.0014 loss_val: 69.4196\n",
      "Epoch: 133 loss_train: 69.4735 loss_val: 69.5873\n",
      "Epoch: 134 loss_train: 68.8558 loss_val: 69.6310\n",
      "Epoch: 135 loss_train: 68.8713 loss_val: 69.5440\n",
      "Epoch: 136 loss_train: 69.1774 loss_val: 69.4516\n",
      "Epoch: 137 loss_train: 69.1922 loss_val: 69.4962\n",
      "Epoch: 138 loss_train: 69.6928 loss_val: 69.6580\n",
      "Epoch: 139 loss_train: 69.0117 loss_val: 69.5636\n",
      "Epoch: 140 loss_train: 69.0694 loss_val: 69.4401\n",
      "Epoch: 141 loss_train: 68.9583 loss_val: 69.5244\n",
      "Epoch: 142 loss_train: 68.8029 loss_val: 69.5806\n",
      "Epoch: 143 loss_train: 68.9708 loss_val: 69.4681\n",
      "Epoch: 144 loss_train: 68.7919 loss_val: 69.3838\n",
      "Epoch: 145 loss_train: 69.1515 loss_val: 69.4664\n",
      "Epoch: 146 loss_train: 69.2853 loss_val: 69.5882\n",
      "Epoch: 147 loss_train: 68.7306 loss_val: 69.4780\n",
      "Epoch: 148 loss_train: 68.8849 loss_val: 69.4016\n",
      "Epoch: 149 loss_train: 69.1161 loss_val: 69.4941\n",
      "Epoch: 150 loss_train: 68.7681 loss_val: 69.5883\n",
      "Epoch: 151 loss_train: 68.7962 loss_val: 69.5379\n",
      "Epoch: 152 loss_train: 69.1314 loss_val: 69.4487\n",
      "Epoch: 153 loss_train: 68.8312 loss_val: 69.4461\n",
      "Epoch: 154 loss_train: 68.8397 loss_val: 69.5597\n",
      "Epoch: 155 loss_train: 68.7759 loss_val: 69.6630\n",
      "Epoch: 156 loss_train: 68.7665 loss_val: 69.5580\n",
      "Epoch: 157 loss_train: 68.9252 loss_val: 69.3750\n",
      "Epoch: 158 loss_train: 69.0958 loss_val: 69.4010\n",
      "Epoch: 159 loss_train: 69.0071 loss_val: 69.6240\n",
      "Epoch: 160 loss_train: 68.9945 loss_val: 69.5349\n",
      "Epoch: 161 loss_train: 69.1115 loss_val: 69.3719\n",
      "Epoch: 162 loss_train: 69.2019 loss_val: 69.4577\n",
      "Epoch: 163 loss_train: 68.9719 loss_val: 69.5395\n",
      "Epoch: 164 loss_train: 69.0353 loss_val: 69.5644\n",
      "Epoch: 165 loss_train: 68.9911 loss_val: 69.3839\n",
      "Epoch: 166 loss_train: 68.7121 loss_val: 69.3021\n",
      "Epoch: 167 loss_train: 69.1635 loss_val: 69.4411\n",
      "Epoch: 168 loss_train: 68.8923 loss_val: 69.4547\n",
      "Epoch: 169 loss_train: 68.9264 loss_val: 69.3876\n",
      "Epoch: 170 loss_train: 69.0436 loss_val: 69.3373\n",
      "Epoch: 171 loss_train: 69.0523 loss_val: 69.5393\n",
      "Epoch: 172 loss_train: 68.7659 loss_val: 69.5468\n",
      "Epoch: 173 loss_train: 68.9885 loss_val: 69.4451\n",
      "Epoch: 174 loss_train: 68.7200 loss_val: 69.4287\n",
      "Epoch: 175 loss_train: 68.8358 loss_val: 69.4981\n",
      "Epoch: 176 loss_train: 68.9939 loss_val: 69.4622\n",
      "Epoch: 177 loss_train: 68.6928 loss_val: 69.2823\n",
      "Epoch: 178 loss_train: 68.7472 loss_val: 69.2494\n",
      "Epoch: 179 loss_train: 68.5244 loss_val: 69.5376\n",
      "Epoch: 180 loss_train: 68.9863 loss_val: 69.6261\n",
      "Epoch: 181 loss_train: 69.0924 loss_val: 69.4809\n",
      "Epoch: 182 loss_train: 68.5547 loss_val: 69.4371\n",
      "Epoch: 183 loss_train: 69.1891 loss_val: 69.4983\n",
      "Epoch: 184 loss_train: 69.0296 loss_val: 69.6367\n",
      "Epoch: 185 loss_train: 68.7515 loss_val: 69.4761\n",
      "Epoch: 186 loss_train: 68.9699 loss_val: 69.2069\n",
      "Epoch: 187 loss_train: 68.8014 loss_val: 69.3043\n",
      "Epoch: 188 loss_train: 68.7656 loss_val: 69.5975\n",
      "Epoch: 189 loss_train: 68.8906 loss_val: 69.5635\n",
      "Epoch: 190 loss_train: 69.0819 loss_val: 69.3700\n",
      "Epoch: 191 loss_train: 68.9043 loss_val: 69.3590\n",
      "Epoch: 192 loss_train: 68.7083 loss_val: 69.5664\n",
      "Epoch: 193 loss_train: 68.6449 loss_val: 69.5126\n",
      "Epoch: 194 loss_train: 68.9692 loss_val: 69.3183\n",
      "Epoch: 195 loss_train: 68.9363 loss_val: 69.2931\n",
      "Epoch: 196 loss_train: 69.0469 loss_val: 69.4061\n",
      "Epoch: 197 loss_train: 68.6457 loss_val: 69.4259\n",
      "Epoch: 198 loss_train: 68.9091 loss_val: 69.3642\n",
      "Epoch: 199 loss_train: 68.5629 loss_val: 69.2820\n",
      "Epoch: 200 loss_train: 68.7066 loss_val: 69.3633\n",
      "Epoch: 001 loss_train: 68.4975 loss_val: 69.6127\n",
      "Epoch: 002 loss_train: 69.5403 loss_val: 69.4170\n",
      "Epoch: 003 loss_train: 68.6228 loss_val: 70.0191\n",
      "Epoch: 004 loss_train: 69.2044 loss_val: 70.0232\n",
      "Epoch: 005 loss_train: 69.0913 loss_val: 69.6404\n",
      "Epoch: 006 loss_train: 68.6712 loss_val: 69.3125\n",
      "Epoch: 007 loss_train: 68.3051 loss_val: 69.2307\n",
      "Epoch: 008 loss_train: 69.0020 loss_val: 69.3384\n",
      "Epoch: 009 loss_train: 68.9774 loss_val: 69.5051\n",
      "Epoch: 010 loss_train: 69.0213 loss_val: 69.6801\n",
      "Epoch: 011 loss_train: 69.3962 loss_val: 69.8391\n",
      "Epoch: 012 loss_train: 68.8233 loss_val: 69.9328\n",
      "Epoch: 013 loss_train: 68.4365 loss_val: 69.8940\n",
      "Epoch: 014 loss_train: 68.4830 loss_val: 69.7283\n",
      "Epoch: 015 loss_train: 68.4327 loss_val: 69.4832\n",
      "Epoch: 016 loss_train: 68.7200 loss_val: 69.2720\n",
      "Epoch: 017 loss_train: 68.5950 loss_val: 69.1219\n",
      "Epoch: 018 loss_train: 68.7823 loss_val: 69.0664\n",
      "Epoch: 019 loss_train: 68.7169 loss_val: 69.0889\n",
      "Epoch: 020 loss_train: 68.5646 loss_val: 69.1803\n",
      "Epoch: 021 loss_train: 68.6513 loss_val: 69.3277\n",
      "Epoch: 022 loss_train: 68.8548 loss_val: 69.4989\n",
      "Epoch: 023 loss_train: 68.5135 loss_val: 69.6088\n",
      "Epoch: 024 loss_train: 68.9514 loss_val: 69.6308\n",
      "Epoch: 025 loss_train: 68.7968 loss_val: 69.5625\n",
      "Epoch: 026 loss_train: 68.6846 loss_val: 69.4496\n",
      "Epoch: 027 loss_train: 68.4516 loss_val: 69.3394\n",
      "Epoch: 028 loss_train: 68.5661 loss_val: 69.2674\n",
      "Epoch: 029 loss_train: 68.7034 loss_val: 69.2430\n",
      "Epoch: 030 loss_train: 68.3956 loss_val: 69.2593\n",
      "Epoch: 031 loss_train: 68.8065 loss_val: 69.3095\n",
      "Epoch: 032 loss_train: 68.9191 loss_val: 69.3707\n",
      "Epoch: 033 loss_train: 68.6812 loss_val: 69.4033\n",
      "Epoch: 034 loss_train: 68.7354 loss_val: 69.3915\n",
      "Epoch: 035 loss_train: 68.9164 loss_val: 69.3469\n",
      "Epoch: 036 loss_train: 68.9390 loss_val: 69.3105\n",
      "Epoch: 037 loss_train: 68.1797 loss_val: 69.2943\n",
      "Epoch: 038 loss_train: 68.9224 loss_val: 69.3059\n",
      "Epoch: 039 loss_train: 68.7286 loss_val: 69.3444\n",
      "Epoch: 040 loss_train: 68.4604 loss_val: 69.4014\n",
      "Epoch: 041 loss_train: 68.4759 loss_val: 69.4423\n",
      "Epoch: 042 loss_train: 68.7441 loss_val: 69.4481\n",
      "Epoch: 043 loss_train: 68.5849 loss_val: 69.3997\n",
      "Epoch: 044 loss_train: 68.2360 loss_val: 69.3291\n",
      "Epoch: 045 loss_train: 68.7244 loss_val: 69.2778\n",
      "Epoch: 046 loss_train: 68.7840 loss_val: 69.2812\n",
      "Epoch: 047 loss_train: 68.5555 loss_val: 69.3012\n",
      "Epoch: 048 loss_train: 68.5371 loss_val: 69.3227\n",
      "Epoch: 049 loss_train: 68.5781 loss_val: 69.3492\n",
      "Epoch: 050 loss_train: 68.5311 loss_val: 69.3706\n",
      "Epoch: 051 loss_train: 68.6604 loss_val: 69.3631\n",
      "Epoch: 052 loss_train: 68.8526 loss_val: 69.3509\n",
      "Epoch: 053 loss_train: 68.4649 loss_val: 69.3154\n",
      "Epoch: 054 loss_train: 68.5202 loss_val: 69.3092\n",
      "Epoch: 055 loss_train: 68.5750 loss_val: 69.2959\n",
      "Epoch: 056 loss_train: 68.7761 loss_val: 69.3135\n",
      "Epoch: 057 loss_train: 68.4477 loss_val: 69.3281\n",
      "Epoch: 058 loss_train: 68.6730 loss_val: 69.3421\n",
      "Epoch: 059 loss_train: 68.5797 loss_val: 69.3363\n",
      "Epoch: 060 loss_train: 68.6631 loss_val: 69.3285\n",
      "Epoch: 061 loss_train: 69.1065 loss_val: 69.3071\n",
      "Epoch: 062 loss_train: 68.8239 loss_val: 69.2952\n",
      "Epoch: 063 loss_train: 68.5770 loss_val: 69.2898\n",
      "Epoch: 064 loss_train: 68.1832 loss_val: 69.2966\n",
      "Epoch: 065 loss_train: 68.5932 loss_val: 69.2860\n",
      "Epoch: 066 loss_train: 68.2989 loss_val: 69.2912\n",
      "Epoch: 067 loss_train: 68.4465 loss_val: 69.3223\n",
      "Epoch: 068 loss_train: 68.5294 loss_val: 69.3491\n",
      "Epoch: 069 loss_train: 68.4919 loss_val: 69.3416\n",
      "Epoch: 070 loss_train: 68.1794 loss_val: 69.2898\n",
      "Epoch: 071 loss_train: 68.8533 loss_val: 69.2670\n",
      "Epoch: 072 loss_train: 68.6169 loss_val: 69.2782\n",
      "Epoch: 073 loss_train: 68.9798 loss_val: 69.3047\n",
      "Epoch: 074 loss_train: 68.0534 loss_val: 69.3203\n",
      "Epoch: 075 loss_train: 68.7175 loss_val: 69.3639\n",
      "Epoch: 076 loss_train: 68.6623 loss_val: 69.3772\n",
      "Epoch: 077 loss_train: 68.5979 loss_val: 69.3382\n",
      "Epoch: 078 loss_train: 68.2434 loss_val: 69.2853\n",
      "Epoch: 079 loss_train: 68.5771 loss_val: 69.2553\n",
      "Epoch: 080 loss_train: 68.5957 loss_val: 69.2504\n",
      "Epoch: 081 loss_train: 68.7957 loss_val: 69.2844\n",
      "Epoch: 082 loss_train: 68.5830 loss_val: 69.2984\n",
      "Epoch: 083 loss_train: 68.6022 loss_val: 69.2925\n",
      "Epoch: 084 loss_train: 68.5521 loss_val: 69.2620\n",
      "Epoch: 085 loss_train: 68.5442 loss_val: 69.2351\n",
      "Epoch: 086 loss_train: 68.7901 loss_val: 69.2518\n",
      "Epoch: 087 loss_train: 68.3298 loss_val: 69.2711\n",
      "Epoch: 088 loss_train: 68.7858 loss_val: 69.3002\n",
      "Epoch: 089 loss_train: 68.4417 loss_val: 69.3302\n",
      "Epoch: 090 loss_train: 68.7098 loss_val: 69.3488\n",
      "Epoch: 091 loss_train: 68.0597 loss_val: 69.3095\n",
      "Epoch: 092 loss_train: 68.3723 loss_val: 69.2508\n",
      "Epoch: 093 loss_train: 68.4204 loss_val: 69.2358\n",
      "Epoch: 094 loss_train: 68.2810 loss_val: 69.2397\n",
      "Epoch: 095 loss_train: 68.6152 loss_val: 69.2405\n",
      "Epoch: 096 loss_train: 68.5680 loss_val: 69.2229\n",
      "Epoch: 097 loss_train: 68.5186 loss_val: 69.1983\n",
      "Epoch: 098 loss_train: 68.4347 loss_val: 69.2121\n",
      "Epoch: 099 loss_train: 68.3927 loss_val: 69.2478\n",
      "Epoch: 100 loss_train: 68.3787 loss_val: 69.2724\n",
      "Epoch: 101 loss_train: 68.4737 loss_val: 69.2835\n",
      "Epoch: 102 loss_train: 68.3204 loss_val: 69.2799\n",
      "Epoch: 103 loss_train: 68.4357 loss_val: 69.2992\n",
      "Epoch: 104 loss_train: 68.5986 loss_val: 69.3086\n",
      "Epoch: 105 loss_train: 68.5407 loss_val: 69.2997\n",
      "Epoch: 106 loss_train: 68.3048 loss_val: 69.2911\n",
      "Epoch: 107 loss_train: 68.6243 loss_val: 69.2800\n",
      "Epoch: 108 loss_train: 68.6039 loss_val: 69.2681\n",
      "Epoch: 109 loss_train: 68.2346 loss_val: 69.2896\n",
      "Epoch: 110 loss_train: 68.3729 loss_val: 69.2902\n",
      "Epoch: 111 loss_train: 68.4975 loss_val: 69.2724\n",
      "Epoch: 112 loss_train: 68.1855 loss_val: 69.2315\n",
      "Epoch: 113 loss_train: 68.6473 loss_val: 69.1977\n",
      "Epoch: 114 loss_train: 68.3127 loss_val: 69.2070\n",
      "Epoch: 115 loss_train: 68.4281 loss_val: 69.2271\n",
      "Epoch: 116 loss_train: 68.7163 loss_val: 69.2631\n",
      "Epoch: 117 loss_train: 68.5181 loss_val: 69.2664\n",
      "Epoch: 118 loss_train: 68.1616 loss_val: 69.2598\n",
      "Epoch: 119 loss_train: 68.4027 loss_val: 69.2378\n",
      "Epoch: 120 loss_train: 68.6408 loss_val: 69.2468\n",
      "Epoch: 121 loss_train: 68.2773 loss_val: 69.2578\n",
      "Epoch: 122 loss_train: 68.6009 loss_val: 69.2398\n",
      "Epoch: 123 loss_train: 68.2131 loss_val: 69.2540\n",
      "Epoch: 124 loss_train: 68.3830 loss_val: 69.2748\n",
      "Epoch: 125 loss_train: 68.0872 loss_val: 69.2650\n",
      "Epoch: 126 loss_train: 68.4608 loss_val: 69.2484\n",
      "Epoch: 127 loss_train: 68.4501 loss_val: 69.2314\n",
      "Epoch: 128 loss_train: 68.5288 loss_val: 69.2547\n",
      "Epoch: 129 loss_train: 68.3984 loss_val: 69.2495\n",
      "Epoch: 130 loss_train: 68.4456 loss_val: 69.2134\n",
      "Epoch: 131 loss_train: 68.1576 loss_val: 69.1941\n",
      "Epoch: 132 loss_train: 68.3336 loss_val: 69.2284\n",
      "Epoch: 133 loss_train: 68.7665 loss_val: 69.2455\n",
      "Epoch: 134 loss_train: 68.2694 loss_val: 69.2338\n",
      "Epoch: 135 loss_train: 68.4641 loss_val: 69.2074\n",
      "Epoch: 136 loss_train: 68.4917 loss_val: 69.2068\n",
      "Epoch: 137 loss_train: 68.3596 loss_val: 69.2142\n",
      "Epoch: 138 loss_train: 68.7688 loss_val: 69.2691\n",
      "Epoch: 139 loss_train: 68.2804 loss_val: 69.2934\n",
      "Epoch: 140 loss_train: 68.2725 loss_val: 69.2651\n",
      "Epoch: 141 loss_train: 68.3435 loss_val: 69.2276\n",
      "Epoch: 142 loss_train: 68.1837 loss_val: 69.2140\n",
      "Epoch: 143 loss_train: 68.7234 loss_val: 69.1960\n",
      "Epoch: 144 loss_train: 68.1922 loss_val: 69.1918\n",
      "Epoch: 145 loss_train: 68.4344 loss_val: 69.2399\n",
      "Epoch: 146 loss_train: 68.3742 loss_val: 69.2653\n",
      "Epoch: 147 loss_train: 68.3866 loss_val: 69.2860\n",
      "Epoch: 148 loss_train: 68.5848 loss_val: 69.2803\n",
      "Epoch: 149 loss_train: 68.1420 loss_val: 69.2753\n",
      "Epoch: 150 loss_train: 68.3407 loss_val: 69.2843\n",
      "Epoch: 151 loss_train: 68.3048 loss_val: 69.2999\n",
      "Epoch: 152 loss_train: 68.0866 loss_val: 69.3195\n",
      "Epoch: 153 loss_train: 68.1964 loss_val: 69.3026\n",
      "Epoch: 154 loss_train: 68.0656 loss_val: 69.2585\n",
      "Epoch: 155 loss_train: 68.7025 loss_val: 69.2045\n",
      "Epoch: 156 loss_train: 67.8747 loss_val: 69.1385\n",
      "Epoch: 157 loss_train: 68.2647 loss_val: 69.1095\n",
      "Epoch: 158 loss_train: 68.4870 loss_val: 69.1410\n",
      "Epoch: 159 loss_train: 68.4812 loss_val: 69.2207\n",
      "Epoch: 160 loss_train: 68.2795 loss_val: 69.2627\n",
      "Epoch: 161 loss_train: 68.2504 loss_val: 69.2132\n",
      "Epoch: 162 loss_train: 68.3853 loss_val: 69.1341\n",
      "Epoch: 163 loss_train: 68.3972 loss_val: 69.0938\n",
      "Epoch: 164 loss_train: 68.5465 loss_val: 69.1454\n",
      "Epoch: 165 loss_train: 68.3643 loss_val: 69.1849\n",
      "Epoch: 166 loss_train: 68.3410 loss_val: 69.1941\n",
      "Epoch: 167 loss_train: 68.2184 loss_val: 69.1508\n",
      "Epoch: 168 loss_train: 68.2393 loss_val: 69.1304\n",
      "Epoch: 169 loss_train: 68.1737 loss_val: 69.1506\n",
      "Epoch: 170 loss_train: 68.6939 loss_val: 69.1797\n",
      "Epoch: 171 loss_train: 68.1078 loss_val: 69.1691\n",
      "Epoch: 172 loss_train: 68.2843 loss_val: 69.1883\n",
      "Epoch: 173 loss_train: 67.8833 loss_val: 69.2299\n",
      "Epoch: 174 loss_train: 68.5543 loss_val: 69.2366\n",
      "Epoch: 175 loss_train: 68.1801 loss_val: 69.2298\n",
      "Epoch: 176 loss_train: 68.3213 loss_val: 69.2242\n",
      "Epoch: 177 loss_train: 68.8135 loss_val: 69.2507\n",
      "Epoch: 178 loss_train: 68.2249 loss_val: 69.2387\n",
      "Epoch: 179 loss_train: 68.1205 loss_val: 69.2099\n",
      "Epoch: 180 loss_train: 68.3438 loss_val: 69.1702\n",
      "Epoch: 181 loss_train: 68.3628 loss_val: 69.1308\n",
      "Epoch: 182 loss_train: 68.5462 loss_val: 69.1756\n",
      "Epoch: 183 loss_train: 67.9827 loss_val: 69.2326\n",
      "Epoch: 184 loss_train: 68.3612 loss_val: 69.2348\n",
      "Epoch: 185 loss_train: 68.4600 loss_val: 69.1912\n",
      "Epoch: 186 loss_train: 68.3737 loss_val: 69.1640\n",
      "Epoch: 187 loss_train: 68.1621 loss_val: 69.1498\n",
      "Epoch: 188 loss_train: 68.4132 loss_val: 69.1610\n",
      "Epoch: 189 loss_train: 68.4849 loss_val: 69.2060\n",
      "Epoch: 190 loss_train: 68.0376 loss_val: 69.2230\n",
      "Epoch: 191 loss_train: 68.2488 loss_val: 69.1570\n",
      "Epoch: 192 loss_train: 68.2308 loss_val: 69.0713\n",
      "Epoch: 193 loss_train: 68.3796 loss_val: 69.0629\n",
      "Epoch: 194 loss_train: 68.1880 loss_val: 69.1171\n",
      "Epoch: 195 loss_train: 68.0258 loss_val: 69.1832\n",
      "Epoch: 196 loss_train: 68.6596 loss_val: 69.1889\n",
      "Epoch: 197 loss_train: 68.1138 loss_val: 69.1561\n",
      "Epoch: 198 loss_train: 68.4961 loss_val: 69.1757\n",
      "Epoch: 199 loss_train: 68.1702 loss_val: 69.2051\n",
      "Epoch: 200 loss_train: 68.3580 loss_val: 69.1845\n",
      "Epoch: 001 loss_train: 68.1431 loss_val: 69.1443\n",
      "Epoch: 002 loss_train: 68.0196 loss_val: 69.3182\n",
      "Epoch: 003 loss_train: 68.4424 loss_val: 69.0924\n",
      "Epoch: 004 loss_train: 68.0644 loss_val: 68.9687\n",
      "Epoch: 005 loss_train: 68.0524 loss_val: 69.0000\n",
      "Epoch: 006 loss_train: 68.2240 loss_val: 69.1182\n",
      "Epoch: 007 loss_train: 68.3400 loss_val: 69.2841\n",
      "Epoch: 008 loss_train: 68.1645 loss_val: 69.3581\n",
      "Epoch: 009 loss_train: 68.0526 loss_val: 69.3236\n",
      "Epoch: 010 loss_train: 68.4318 loss_val: 69.2318\n",
      "Epoch: 011 loss_train: 68.2226 loss_val: 69.1480\n",
      "Epoch: 012 loss_train: 68.5110 loss_val: 69.1023\n",
      "Epoch: 013 loss_train: 68.6723 loss_val: 69.1411\n",
      "Epoch: 014 loss_train: 68.2440 loss_val: 69.2082\n",
      "Epoch: 015 loss_train: 68.2576 loss_val: 69.2491\n",
      "Epoch: 016 loss_train: 68.4287 loss_val: 69.2094\n",
      "Epoch: 017 loss_train: 68.3109 loss_val: 69.1747\n",
      "Epoch: 018 loss_train: 68.0717 loss_val: 69.1464\n",
      "Epoch: 019 loss_train: 68.2475 loss_val: 69.1549\n",
      "Epoch: 020 loss_train: 68.2332 loss_val: 69.1680\n",
      "Epoch: 021 loss_train: 68.2430 loss_val: 69.1930\n",
      "Epoch: 022 loss_train: 68.0833 loss_val: 69.2065\n",
      "Epoch: 023 loss_train: 68.2225 loss_val: 69.1977\n",
      "Epoch: 024 loss_train: 67.9210 loss_val: 69.1551\n",
      "Epoch: 025 loss_train: 68.1292 loss_val: 69.1134\n",
      "Epoch: 026 loss_train: 68.1382 loss_val: 69.1021\n",
      "Epoch: 027 loss_train: 68.1782 loss_val: 69.1146\n",
      "Epoch: 028 loss_train: 68.3664 loss_val: 69.1308\n",
      "Epoch: 029 loss_train: 67.9448 loss_val: 69.1235\n",
      "Epoch: 030 loss_train: 68.4893 loss_val: 69.1228\n",
      "Epoch: 031 loss_train: 68.2666 loss_val: 69.1345\n",
      "Epoch: 032 loss_train: 68.1459 loss_val: 69.1564\n",
      "Epoch: 033 loss_train: 68.4351 loss_val: 69.1909\n",
      "Epoch: 034 loss_train: 68.2022 loss_val: 69.1940\n",
      "Epoch: 035 loss_train: 68.3051 loss_val: 69.1791\n",
      "Epoch: 036 loss_train: 68.4343 loss_val: 69.1500\n",
      "Epoch: 037 loss_train: 68.0019 loss_val: 69.1368\n",
      "Epoch: 038 loss_train: 68.0796 loss_val: 69.1308\n",
      "Epoch: 039 loss_train: 68.1662 loss_val: 69.1586\n",
      "Epoch: 040 loss_train: 68.0267 loss_val: 69.1807\n",
      "Epoch: 041 loss_train: 68.1278 loss_val: 69.1627\n",
      "Epoch: 042 loss_train: 68.1280 loss_val: 69.1339\n",
      "Epoch: 043 loss_train: 68.0551 loss_val: 69.1130\n",
      "Epoch: 044 loss_train: 68.4350 loss_val: 69.1163\n",
      "Epoch: 045 loss_train: 68.1262 loss_val: 69.1305\n",
      "Epoch: 046 loss_train: 68.4974 loss_val: 69.1394\n",
      "Epoch: 047 loss_train: 68.0702 loss_val: 69.1504\n",
      "Epoch: 048 loss_train: 68.1535 loss_val: 69.1196\n",
      "Epoch: 049 loss_train: 68.3657 loss_val: 69.0967\n",
      "Epoch: 050 loss_train: 68.3170 loss_val: 69.0941\n",
      "Epoch: 051 loss_train: 68.2145 loss_val: 69.1175\n",
      "Epoch: 052 loss_train: 67.9411 loss_val: 69.1150\n",
      "Epoch: 053 loss_train: 68.2441 loss_val: 69.1158\n",
      "Epoch: 054 loss_train: 68.2379 loss_val: 69.1257\n",
      "Epoch: 055 loss_train: 68.0044 loss_val: 69.1322\n",
      "Epoch: 056 loss_train: 68.1668 loss_val: 69.1224\n",
      "Epoch: 057 loss_train: 68.0150 loss_val: 69.1123\n",
      "Epoch: 058 loss_train: 68.3293 loss_val: 69.1312\n",
      "Epoch: 059 loss_train: 67.8577 loss_val: 69.1494\n",
      "Epoch: 060 loss_train: 68.4897 loss_val: 69.1643\n",
      "Epoch: 061 loss_train: 68.3559 loss_val: 69.1489\n",
      "Epoch: 062 loss_train: 68.1712 loss_val: 69.1266\n",
      "Epoch: 063 loss_train: 68.0999 loss_val: 69.1038\n",
      "Epoch: 064 loss_train: 68.3476 loss_val: 69.1242\n",
      "Epoch: 065 loss_train: 67.6258 loss_val: 69.1399\n",
      "Epoch: 066 loss_train: 67.9560 loss_val: 69.1383\n",
      "Epoch: 067 loss_train: 67.8358 loss_val: 69.1103\n",
      "Epoch: 068 loss_train: 68.0335 loss_val: 69.0859\n",
      "Epoch: 069 loss_train: 68.4669 loss_val: 69.0958\n",
      "Epoch: 070 loss_train: 68.1418 loss_val: 69.1410\n",
      "Epoch: 071 loss_train: 68.1956 loss_val: 69.1515\n",
      "Epoch: 072 loss_train: 68.1644 loss_val: 69.1113\n",
      "Epoch: 073 loss_train: 68.0435 loss_val: 69.0578\n",
      "Epoch: 074 loss_train: 68.2285 loss_val: 69.0254\n",
      "Epoch: 075 loss_train: 68.1059 loss_val: 69.0761\n",
      "Epoch: 076 loss_train: 68.0173 loss_val: 69.1367\n",
      "Epoch: 077 loss_train: 67.8688 loss_val: 69.1779\n",
      "Epoch: 078 loss_train: 68.1098 loss_val: 69.1330\n",
      "Epoch: 079 loss_train: 68.2102 loss_val: 69.0692\n",
      "Epoch: 080 loss_train: 67.8710 loss_val: 69.0158\n",
      "Epoch: 081 loss_train: 68.0304 loss_val: 69.0255\n",
      "Epoch: 082 loss_train: 67.8652 loss_val: 69.0777\n",
      "Epoch: 083 loss_train: 68.1914 loss_val: 69.1053\n",
      "Epoch: 084 loss_train: 68.0122 loss_val: 69.1031\n",
      "Epoch: 085 loss_train: 68.2986 loss_val: 69.0551\n",
      "Epoch: 086 loss_train: 67.5880 loss_val: 69.0438\n",
      "Epoch: 087 loss_train: 68.3580 loss_val: 69.0722\n",
      "Epoch: 088 loss_train: 67.9792 loss_val: 69.1135\n",
      "Epoch: 089 loss_train: 68.0117 loss_val: 69.1171\n",
      "Epoch: 090 loss_train: 68.3680 loss_val: 69.1210\n",
      "Epoch: 091 loss_train: 67.8624 loss_val: 69.1050\n",
      "Epoch: 092 loss_train: 68.2762 loss_val: 69.0635\n",
      "Epoch: 093 loss_train: 67.9569 loss_val: 69.0691\n",
      "Epoch: 094 loss_train: 68.2219 loss_val: 69.1042\n",
      "Epoch: 095 loss_train: 68.2340 loss_val: 69.1393\n",
      "Epoch: 096 loss_train: 68.0403 loss_val: 69.1557\n",
      "Epoch: 097 loss_train: 67.8357 loss_val: 69.1148\n",
      "Epoch: 098 loss_train: 68.1043 loss_val: 69.0864\n",
      "Epoch: 099 loss_train: 68.0609 loss_val: 69.0825\n",
      "Epoch: 100 loss_train: 67.9648 loss_val: 69.1112\n",
      "Epoch: 101 loss_train: 67.9090 loss_val: 69.0906\n",
      "Epoch: 102 loss_train: 67.9329 loss_val: 69.0579\n",
      "Epoch: 103 loss_train: 68.2591 loss_val: 69.0286\n",
      "Epoch: 104 loss_train: 68.2796 loss_val: 69.0087\n",
      "Epoch: 105 loss_train: 68.0536 loss_val: 69.0124\n",
      "Epoch: 106 loss_train: 68.2076 loss_val: 69.0689\n",
      "Epoch: 107 loss_train: 67.9052 loss_val: 69.0981\n",
      "Epoch: 108 loss_train: 68.2759 loss_val: 69.1168\n",
      "Epoch: 109 loss_train: 68.1979 loss_val: 69.0835\n",
      "Epoch: 110 loss_train: 68.1849 loss_val: 69.0825\n",
      "Epoch: 111 loss_train: 68.0199 loss_val: 69.0536\n",
      "Epoch: 112 loss_train: 68.0549 loss_val: 69.0111\n",
      "Epoch: 113 loss_train: 68.3064 loss_val: 68.9927\n",
      "Epoch: 114 loss_train: 68.2803 loss_val: 69.0481\n",
      "Epoch: 115 loss_train: 68.0498 loss_val: 69.1120\n",
      "Epoch: 116 loss_train: 68.3327 loss_val: 69.0894\n",
      "Epoch: 117 loss_train: 68.1472 loss_val: 69.0544\n",
      "Epoch: 118 loss_train: 68.0261 loss_val: 69.0442\n",
      "Epoch: 119 loss_train: 68.1419 loss_val: 69.0479\n",
      "Epoch: 120 loss_train: 68.0565 loss_val: 69.0634\n",
      "Epoch: 121 loss_train: 68.2250 loss_val: 69.0810\n",
      "Epoch: 122 loss_train: 68.0840 loss_val: 69.0659\n",
      "Epoch: 123 loss_train: 67.8957 loss_val: 69.0071\n",
      "Epoch: 124 loss_train: 67.8918 loss_val: 68.9699\n",
      "Epoch: 125 loss_train: 68.3688 loss_val: 68.9723\n",
      "Epoch: 126 loss_train: 68.0247 loss_val: 69.0002\n",
      "Epoch: 127 loss_train: 67.5588 loss_val: 69.0324\n",
      "Epoch: 128 loss_train: 68.5077 loss_val: 69.0384\n",
      "Epoch: 129 loss_train: 68.2029 loss_val: 69.0443\n",
      "Epoch: 130 loss_train: 67.9025 loss_val: 69.0185\n",
      "Epoch: 131 loss_train: 67.5976 loss_val: 68.9863\n",
      "Epoch: 132 loss_train: 68.1199 loss_val: 68.9878\n",
      "Epoch: 133 loss_train: 68.2306 loss_val: 69.0151\n",
      "Epoch: 134 loss_train: 68.0132 loss_val: 69.0293\n",
      "Epoch: 135 loss_train: 68.1778 loss_val: 69.0476\n",
      "Epoch: 136 loss_train: 68.5095 loss_val: 69.0613\n",
      "Epoch: 137 loss_train: 68.1430 loss_val: 69.0767\n",
      "Epoch: 138 loss_train: 67.7929 loss_val: 69.0452\n",
      "Epoch: 139 loss_train: 68.3455 loss_val: 69.0364\n",
      "Epoch: 140 loss_train: 67.8010 loss_val: 69.0406\n",
      "Epoch: 141 loss_train: 68.0430 loss_val: 69.0330\n",
      "Epoch: 142 loss_train: 67.6064 loss_val: 69.0223\n",
      "Epoch: 143 loss_train: 68.3422 loss_val: 69.0233\n",
      "Epoch: 144 loss_train: 68.0473 loss_val: 69.0761\n",
      "Epoch: 145 loss_train: 68.1449 loss_val: 69.1141\n",
      "Epoch: 146 loss_train: 68.2054 loss_val: 69.1190\n",
      "Epoch: 147 loss_train: 67.9610 loss_val: 69.0936\n",
      "Epoch: 148 loss_train: 68.2447 loss_val: 69.0732\n",
      "Epoch: 149 loss_train: 68.2671 loss_val: 69.0417\n",
      "Epoch: 150 loss_train: 67.4689 loss_val: 69.0210\n",
      "Epoch: 151 loss_train: 67.6900 loss_val: 69.0176\n",
      "Epoch: 152 loss_train: 68.2335 loss_val: 69.0586\n",
      "Epoch: 153 loss_train: 67.8456 loss_val: 69.0887\n",
      "Epoch: 154 loss_train: 68.2449 loss_val: 69.0840\n",
      "Epoch: 155 loss_train: 67.9752 loss_val: 69.0963\n",
      "Epoch: 156 loss_train: 68.0915 loss_val: 69.1206\n",
      "Epoch: 157 loss_train: 68.2161 loss_val: 69.1030\n",
      "Epoch: 158 loss_train: 67.9676 loss_val: 69.0516\n",
      "Epoch: 159 loss_train: 68.1047 loss_val: 69.0271\n",
      "Epoch: 160 loss_train: 67.7801 loss_val: 69.0157\n",
      "Epoch: 161 loss_train: 68.1442 loss_val: 69.0361\n",
      "Epoch: 162 loss_train: 67.8784 loss_val: 69.0701\n",
      "Epoch: 163 loss_train: 67.7818 loss_val: 69.0585\n",
      "Epoch: 164 loss_train: 68.2192 loss_val: 69.0463\n",
      "Epoch: 165 loss_train: 67.4035 loss_val: 69.0376\n",
      "Epoch: 166 loss_train: 67.8930 loss_val: 69.0154\n",
      "Epoch: 167 loss_train: 67.7487 loss_val: 69.0084\n",
      "Epoch: 168 loss_train: 68.1729 loss_val: 69.0378\n",
      "Epoch: 169 loss_train: 68.3986 loss_val: 69.0825\n",
      "Epoch: 170 loss_train: 68.0388 loss_val: 69.0942\n",
      "Epoch: 171 loss_train: 67.9574 loss_val: 69.0700\n",
      "Epoch: 172 loss_train: 68.3657 loss_val: 69.0514\n",
      "Epoch: 173 loss_train: 67.6463 loss_val: 69.0422\n",
      "Epoch: 174 loss_train: 67.8126 loss_val: 69.0146\n",
      "Epoch: 175 loss_train: 67.7693 loss_val: 68.9805\n",
      "Epoch: 176 loss_train: 67.8476 loss_val: 68.9807\n",
      "Epoch: 177 loss_train: 67.6396 loss_val: 68.9606\n",
      "Epoch: 178 loss_train: 67.7369 loss_val: 68.9549\n",
      "Epoch: 179 loss_train: 68.2814 loss_val: 68.9519\n",
      "Epoch: 180 loss_train: 68.1919 loss_val: 69.0103\n",
      "Epoch: 181 loss_train: 67.2860 loss_val: 69.0851\n",
      "Epoch: 182 loss_train: 67.5918 loss_val: 69.1012\n",
      "Epoch: 183 loss_train: 67.7800 loss_val: 69.0767\n",
      "Epoch: 184 loss_train: 68.0795 loss_val: 69.0232\n",
      "Epoch: 185 loss_train: 68.0089 loss_val: 69.0233\n",
      "Epoch: 186 loss_train: 67.9834 loss_val: 69.0411\n",
      "Epoch: 187 loss_train: 67.8538 loss_val: 69.0351\n",
      "Epoch: 188 loss_train: 67.6924 loss_val: 69.0099\n",
      "Epoch: 189 loss_train: 68.1821 loss_val: 68.9755\n",
      "Epoch: 190 loss_train: 67.7981 loss_val: 68.9755\n",
      "Epoch: 191 loss_train: 67.8113 loss_val: 69.0162\n",
      "Epoch: 192 loss_train: 67.7094 loss_val: 69.0380\n",
      "Epoch: 193 loss_train: 68.2709 loss_val: 69.0626\n",
      "Epoch: 194 loss_train: 68.2490 loss_val: 69.0950\n",
      "Epoch: 195 loss_train: 68.2696 loss_val: 69.1079\n",
      "Epoch: 196 loss_train: 67.6269 loss_val: 69.0396\n",
      "Epoch: 197 loss_train: 67.9051 loss_val: 68.9593\n",
      "Epoch: 198 loss_train: 67.9742 loss_val: 68.9243\n",
      "Epoch: 199 loss_train: 67.8009 loss_val: 68.9314\n",
      "Epoch: 200 loss_train: 68.0530 loss_val: 69.0115\n",
      "Epoch: 001 loss_train: 68.1954 loss_val: 68.9000\n",
      "Epoch: 002 loss_train: 67.9594 loss_val: 69.0261\n",
      "Epoch: 003 loss_train: 67.9873 loss_val: 69.0524\n",
      "Epoch: 004 loss_train: 67.8103 loss_val: 69.0416\n",
      "Epoch: 005 loss_train: 67.9743 loss_val: 69.0098\n",
      "Epoch: 006 loss_train: 68.0307 loss_val: 69.0508\n",
      "Epoch: 007 loss_train: 68.0788 loss_val: 69.0699\n",
      "Epoch: 008 loss_train: 67.9142 loss_val: 69.0694\n",
      "Epoch: 009 loss_train: 67.6696 loss_val: 69.0332\n",
      "Epoch: 010 loss_train: 67.4801 loss_val: 68.9971\n",
      "Epoch: 011 loss_train: 67.6590 loss_val: 68.9786\n",
      "Epoch: 012 loss_train: 67.8273 loss_val: 68.9905\n",
      "Epoch: 013 loss_train: 68.1689 loss_val: 69.0017\n",
      "Epoch: 014 loss_train: 67.8584 loss_val: 69.0087\n",
      "Epoch: 015 loss_train: 67.7447 loss_val: 68.9995\n",
      "Epoch: 016 loss_train: 67.9385 loss_val: 68.9978\n",
      "Epoch: 017 loss_train: 68.2522 loss_val: 69.0026\n",
      "Epoch: 018 loss_train: 68.3754 loss_val: 69.0113\n",
      "Epoch: 019 loss_train: 68.0817 loss_val: 69.0091\n",
      "Epoch: 020 loss_train: 67.6126 loss_val: 69.0031\n",
      "Epoch: 021 loss_train: 67.9788 loss_val: 68.9797\n",
      "Epoch: 022 loss_train: 67.8588 loss_val: 68.9565\n",
      "Epoch: 023 loss_train: 67.6943 loss_val: 68.9567\n",
      "Epoch: 024 loss_train: 67.7581 loss_val: 68.9913\n",
      "Epoch: 025 loss_train: 67.4962 loss_val: 69.0297\n",
      "Epoch: 026 loss_train: 68.0987 loss_val: 69.0415\n",
      "Epoch: 027 loss_train: 68.0455 loss_val: 69.0335\n",
      "Epoch: 028 loss_train: 67.9419 loss_val: 69.0040\n",
      "Epoch: 029 loss_train: 68.0505 loss_val: 68.9831\n",
      "Epoch: 030 loss_train: 67.7173 loss_val: 68.9729\n",
      "Epoch: 031 loss_train: 68.0883 loss_val: 68.9669\n",
      "Epoch: 032 loss_train: 68.0053 loss_val: 68.9922\n",
      "Epoch: 033 loss_train: 68.1415 loss_val: 69.0230\n",
      "Epoch: 034 loss_train: 67.8467 loss_val: 69.0331\n",
      "Epoch: 035 loss_train: 67.7506 loss_val: 69.0232\n",
      "Epoch: 036 loss_train: 67.4683 loss_val: 68.9794\n",
      "Epoch: 037 loss_train: 67.5969 loss_val: 68.9322\n",
      "Epoch: 038 loss_train: 67.6818 loss_val: 68.9048\n",
      "Epoch: 039 loss_train: 68.1340 loss_val: 68.9181\n",
      "Epoch: 040 loss_train: 68.0836 loss_val: 68.9585\n",
      "Epoch: 041 loss_train: 67.9859 loss_val: 69.0132\n",
      "Epoch: 042 loss_train: 67.7997 loss_val: 69.0390\n",
      "Epoch: 043 loss_train: 67.7238 loss_val: 69.0210\n",
      "Epoch: 044 loss_train: 67.8622 loss_val: 68.9846\n",
      "Epoch: 045 loss_train: 67.9506 loss_val: 68.9498\n",
      "Epoch: 046 loss_train: 67.8742 loss_val: 68.9323\n",
      "Epoch: 047 loss_train: 68.1773 loss_val: 68.9490\n",
      "Epoch: 048 loss_train: 67.9754 loss_val: 68.9844\n",
      "Epoch: 049 loss_train: 67.6183 loss_val: 69.0133\n",
      "Epoch: 050 loss_train: 67.8450 loss_val: 69.0354\n",
      "Epoch: 051 loss_train: 67.8092 loss_val: 69.0307\n",
      "Epoch: 052 loss_train: 67.7890 loss_val: 68.9926\n",
      "Epoch: 053 loss_train: 67.9616 loss_val: 68.9780\n",
      "Epoch: 054 loss_train: 67.7399 loss_val: 68.9631\n",
      "Epoch: 055 loss_train: 67.8936 loss_val: 68.9662\n",
      "Epoch: 056 loss_train: 67.9732 loss_val: 68.9618\n",
      "Epoch: 057 loss_train: 68.1816 loss_val: 68.9715\n",
      "Epoch: 058 loss_train: 67.4939 loss_val: 68.9677\n",
      "Epoch: 059 loss_train: 67.7247 loss_val: 68.9660\n",
      "Epoch: 060 loss_train: 67.7740 loss_val: 68.9431\n",
      "Epoch: 061 loss_train: 68.0443 loss_val: 68.9148\n",
      "Epoch: 062 loss_train: 67.8851 loss_val: 68.9108\n",
      "Epoch: 063 loss_train: 67.8475 loss_val: 68.9139\n",
      "Epoch: 064 loss_train: 67.8165 loss_val: 68.9139\n",
      "Epoch: 065 loss_train: 68.0436 loss_val: 68.9127\n",
      "Epoch: 066 loss_train: 67.9312 loss_val: 68.9385\n",
      "Epoch: 067 loss_train: 67.7928 loss_val: 68.9719\n",
      "Epoch: 068 loss_train: 68.0232 loss_val: 68.9942\n",
      "Epoch: 069 loss_train: 67.7065 loss_val: 68.9890\n",
      "Epoch: 070 loss_train: 67.9847 loss_val: 68.9722\n",
      "Epoch: 071 loss_train: 68.1451 loss_val: 68.9559\n",
      "Epoch: 072 loss_train: 67.7946 loss_val: 68.9668\n",
      "Epoch: 073 loss_train: 67.7531 loss_val: 69.0006\n",
      "Epoch: 074 loss_train: 67.4858 loss_val: 69.0241\n",
      "Epoch: 075 loss_train: 68.2659 loss_val: 69.0207\n",
      "Epoch: 076 loss_train: 67.7040 loss_val: 68.9869\n",
      "Epoch: 077 loss_train: 67.8804 loss_val: 68.9603\n",
      "Epoch: 078 loss_train: 68.1944 loss_val: 68.9534\n",
      "Epoch: 079 loss_train: 67.9269 loss_val: 68.9707\n",
      "Epoch: 080 loss_train: 68.2322 loss_val: 68.9858\n",
      "Epoch: 081 loss_train: 67.7500 loss_val: 68.9947\n",
      "Epoch: 082 loss_train: 67.6645 loss_val: 68.9987\n",
      "Epoch: 083 loss_train: 68.0571 loss_val: 68.9849\n",
      "Epoch: 084 loss_train: 67.8283 loss_val: 68.9596\n",
      "Epoch: 085 loss_train: 68.0872 loss_val: 68.9337\n",
      "Epoch: 086 loss_train: 67.7835 loss_val: 68.9247\n",
      "Epoch: 087 loss_train: 67.9549 loss_val: 68.9463\n",
      "Epoch: 088 loss_train: 68.2704 loss_val: 68.9793\n",
      "Epoch: 089 loss_train: 67.7218 loss_val: 69.0021\n",
      "Epoch: 090 loss_train: 67.7890 loss_val: 69.0097\n",
      "Epoch: 091 loss_train: 68.0203 loss_val: 68.9926\n",
      "Epoch: 092 loss_train: 67.9627 loss_val: 68.9562\n",
      "Epoch: 093 loss_train: 67.9284 loss_val: 68.9273\n",
      "Epoch: 094 loss_train: 68.0806 loss_val: 68.9158\n",
      "Epoch: 095 loss_train: 67.8559 loss_val: 68.9225\n",
      "Epoch: 096 loss_train: 67.8200 loss_val: 68.9174\n",
      "Epoch: 097 loss_train: 67.7629 loss_val: 68.9187\n",
      "Epoch: 098 loss_train: 67.7993 loss_val: 68.9161\n",
      "Epoch: 099 loss_train: 67.7933 loss_val: 68.9262\n",
      "Epoch: 100 loss_train: 67.8693 loss_val: 68.9523\n",
      "Epoch: 101 loss_train: 67.9844 loss_val: 68.9688\n",
      "Epoch: 102 loss_train: 68.0669 loss_val: 68.9703\n",
      "Epoch: 103 loss_train: 68.0696 loss_val: 68.9837\n",
      "Epoch: 104 loss_train: 67.9301 loss_val: 68.9968\n",
      "Epoch: 105 loss_train: 67.8637 loss_val: 68.9896\n",
      "Epoch: 106 loss_train: 67.9358 loss_val: 68.9861\n",
      "Epoch: 107 loss_train: 67.7984 loss_val: 68.9841\n",
      "Epoch: 108 loss_train: 67.7104 loss_val: 68.9875\n",
      "Epoch: 109 loss_train: 67.7402 loss_val: 68.9905\n",
      "Epoch: 110 loss_train: 67.9867 loss_val: 69.0023\n",
      "Epoch: 111 loss_train: 67.7238 loss_val: 69.0149\n",
      "Epoch: 112 loss_train: 67.7735 loss_val: 69.0188\n",
      "Epoch: 113 loss_train: 67.8471 loss_val: 69.0141\n",
      "Epoch: 114 loss_train: 67.6566 loss_val: 68.9880\n",
      "Epoch: 115 loss_train: 67.7511 loss_val: 68.9510\n",
      "Epoch: 116 loss_train: 67.8949 loss_val: 68.9334\n",
      "Epoch: 117 loss_train: 67.6506 loss_val: 68.9390\n",
      "Epoch: 118 loss_train: 67.7127 loss_val: 68.9580\n",
      "Epoch: 119 loss_train: 67.6428 loss_val: 68.9718\n",
      "Epoch: 120 loss_train: 67.9986 loss_val: 68.9738\n",
      "Epoch: 121 loss_train: 67.9961 loss_val: 68.9605\n",
      "Epoch: 122 loss_train: 67.8741 loss_val: 68.9249\n",
      "Epoch: 123 loss_train: 67.8452 loss_val: 68.8920\n",
      "Epoch: 124 loss_train: 67.7403 loss_val: 68.8671\n",
      "Epoch: 125 loss_train: 67.6566 loss_val: 68.8695\n",
      "Epoch: 126 loss_train: 67.7957 loss_val: 68.8902\n",
      "Epoch: 127 loss_train: 67.8313 loss_val: 68.9215\n",
      "Epoch: 128 loss_train: 67.7511 loss_val: 68.9495\n",
      "Epoch: 129 loss_train: 67.8673 loss_val: 68.9629\n",
      "Epoch: 130 loss_train: 67.8574 loss_val: 68.9600\n",
      "Epoch: 131 loss_train: 67.8666 loss_val: 68.9348\n",
      "Epoch: 132 loss_train: 67.7665 loss_val: 68.9315\n",
      "Epoch: 133 loss_train: 68.0753 loss_val: 68.9273\n",
      "Epoch: 134 loss_train: 67.7219 loss_val: 68.9149\n",
      "Epoch: 135 loss_train: 67.7950 loss_val: 68.9188\n",
      "Epoch: 136 loss_train: 67.9262 loss_val: 68.9398\n",
      "Epoch: 137 loss_train: 67.8451 loss_val: 68.9738\n",
      "Epoch: 138 loss_train: 67.7906 loss_val: 69.0290\n",
      "Epoch: 139 loss_train: 68.0057 loss_val: 69.0302\n",
      "Epoch: 140 loss_train: 68.0920 loss_val: 69.0147\n",
      "Epoch: 141 loss_train: 67.8841 loss_val: 68.9800\n",
      "Epoch: 142 loss_train: 67.7650 loss_val: 68.9458\n",
      "Epoch: 143 loss_train: 67.6497 loss_val: 68.9213\n",
      "Epoch: 144 loss_train: 67.5790 loss_val: 68.9237\n",
      "Epoch: 145 loss_train: 67.9611 loss_val: 68.9544\n",
      "Epoch: 146 loss_train: 67.7745 loss_val: 68.9613\n",
      "Epoch: 147 loss_train: 67.7438 loss_val: 68.9542\n",
      "Epoch: 148 loss_train: 67.9076 loss_val: 68.9243\n",
      "Epoch: 149 loss_train: 68.0513 loss_val: 68.8887\n",
      "Epoch: 150 loss_train: 67.8124 loss_val: 68.8884\n",
      "Epoch: 151 loss_train: 67.6243 loss_val: 68.8993\n",
      "Epoch: 152 loss_train: 67.5712 loss_val: 68.9195\n",
      "Epoch: 153 loss_train: 68.1228 loss_val: 68.9292\n",
      "Epoch: 154 loss_train: 67.6450 loss_val: 68.9509\n",
      "Epoch: 155 loss_train: 67.7373 loss_val: 68.9545\n",
      "Epoch: 156 loss_train: 67.7555 loss_val: 68.9368\n",
      "Epoch: 157 loss_train: 68.1205 loss_val: 68.9196\n",
      "Epoch: 158 loss_train: 67.8466 loss_val: 68.9131\n",
      "Epoch: 159 loss_train: 67.5323 loss_val: 68.9184\n",
      "Epoch: 160 loss_train: 67.8610 loss_val: 68.9438\n",
      "Epoch: 161 loss_train: 68.0441 loss_val: 68.9746\n",
      "Epoch: 162 loss_train: 67.7404 loss_val: 69.0027\n",
      "Epoch: 163 loss_train: 67.8689 loss_val: 69.0068\n",
      "Epoch: 164 loss_train: 67.9392 loss_val: 68.9695\n",
      "Epoch: 165 loss_train: 67.5620 loss_val: 68.9245\n",
      "Epoch: 166 loss_train: 68.0878 loss_val: 68.8967\n",
      "Epoch: 167 loss_train: 67.9903 loss_val: 68.8986\n",
      "Epoch: 168 loss_train: 67.6850 loss_val: 68.9121\n",
      "Epoch: 169 loss_train: 67.5242 loss_val: 68.9311\n",
      "Epoch: 170 loss_train: 67.5247 loss_val: 68.9220\n",
      "Epoch: 171 loss_train: 68.2041 loss_val: 68.8991\n",
      "Epoch: 172 loss_train: 67.8559 loss_val: 68.8832\n",
      "Epoch: 173 loss_train: 67.3108 loss_val: 68.8703\n",
      "Epoch: 174 loss_train: 67.9312 loss_val: 68.8822\n",
      "Epoch: 175 loss_train: 67.7648 loss_val: 68.9179\n",
      "Epoch: 176 loss_train: 67.4463 loss_val: 68.9469\n",
      "Epoch: 177 loss_train: 67.8619 loss_val: 68.9504\n",
      "Epoch: 178 loss_train: 67.6040 loss_val: 68.9314\n",
      "Epoch: 179 loss_train: 67.6892 loss_val: 68.9100\n",
      "Epoch: 180 loss_train: 67.8539 loss_val: 68.9015\n",
      "Epoch: 181 loss_train: 67.6334 loss_val: 68.9096\n",
      "Epoch: 182 loss_train: 67.9748 loss_val: 68.9105\n",
      "Epoch: 183 loss_train: 67.6483 loss_val: 68.8987\n",
      "Epoch: 184 loss_train: 67.7306 loss_val: 68.8941\n",
      "Epoch: 185 loss_train: 67.9261 loss_val: 68.8663\n",
      "Epoch: 186 loss_train: 67.8986 loss_val: 68.8430\n",
      "Epoch: 187 loss_train: 67.8903 loss_val: 68.8536\n",
      "Epoch: 188 loss_train: 67.7775 loss_val: 68.8866\n",
      "Epoch: 189 loss_train: 67.8220 loss_val: 68.9401\n",
      "Epoch: 190 loss_train: 67.6830 loss_val: 68.9777\n",
      "Epoch: 191 loss_train: 67.8228 loss_val: 68.9837\n",
      "Epoch: 192 loss_train: 67.7408 loss_val: 68.9648\n",
      "Epoch: 193 loss_train: 67.9075 loss_val: 68.9596\n",
      "Epoch: 194 loss_train: 68.0347 loss_val: 68.9656\n",
      "Epoch: 195 loss_train: 67.8009 loss_val: 68.9568\n",
      "Epoch: 196 loss_train: 67.7971 loss_val: 68.9444\n",
      "Epoch: 197 loss_train: 67.6349 loss_val: 68.9211\n",
      "Epoch: 198 loss_train: 67.2677 loss_val: 68.9079\n",
      "Epoch: 199 loss_train: 67.4389 loss_val: 68.9099\n",
      "Epoch: 200 loss_train: 67.8100 loss_val: 68.9294\n",
      "Epoch: 001 loss_train: 67.6639 loss_val: 68.8339\n",
      "Epoch: 002 loss_train: 67.7980 loss_val: 68.8934\n",
      "Epoch: 003 loss_train: 67.6401 loss_val: 68.9611\n",
      "Epoch: 004 loss_train: 67.7644 loss_val: 68.9911\n",
      "Epoch: 005 loss_train: 68.0471 loss_val: 68.9660\n",
      "Epoch: 006 loss_train: 68.0963 loss_val: 68.9385\n",
      "Epoch: 007 loss_train: 67.9718 loss_val: 68.9207\n",
      "Epoch: 008 loss_train: 67.6795 loss_val: 68.9092\n",
      "Epoch: 009 loss_train: 67.8460 loss_val: 68.8999\n",
      "Epoch: 010 loss_train: 67.7169 loss_val: 68.8849\n",
      "Epoch: 011 loss_train: 68.0148 loss_val: 68.8694\n",
      "Epoch: 012 loss_train: 68.0287 loss_val: 68.8748\n",
      "Epoch: 013 loss_train: 68.0353 loss_val: 68.8780\n",
      "Epoch: 014 loss_train: 67.4780 loss_val: 68.8798\n",
      "Epoch: 015 loss_train: 68.2727 loss_val: 68.8860\n",
      "Epoch: 016 loss_train: 67.9810 loss_val: 68.8962\n",
      "Epoch: 017 loss_train: 68.1125 loss_val: 68.9155\n",
      "Epoch: 018 loss_train: 67.7031 loss_val: 68.9334\n",
      "Epoch: 019 loss_train: 68.1124 loss_val: 68.9424\n",
      "Epoch: 020 loss_train: 67.9557 loss_val: 68.9534\n",
      "Epoch: 021 loss_train: 67.7369 loss_val: 68.9645\n",
      "Epoch: 022 loss_train: 67.8264 loss_val: 68.9506\n",
      "Epoch: 023 loss_train: 67.7188 loss_val: 68.9348\n",
      "Epoch: 024 loss_train: 67.9802 loss_val: 68.9319\n",
      "Epoch: 025 loss_train: 67.8878 loss_val: 68.9289\n",
      "Epoch: 026 loss_train: 67.5654 loss_val: 68.9242\n",
      "Epoch: 027 loss_train: 67.7860 loss_val: 68.9239\n",
      "Epoch: 028 loss_train: 67.8284 loss_val: 68.9221\n",
      "Epoch: 029 loss_train: 67.7466 loss_val: 68.9342\n",
      "Epoch: 030 loss_train: 67.7596 loss_val: 68.9436\n",
      "Epoch: 031 loss_train: 67.8028 loss_val: 68.9493\n",
      "Epoch: 032 loss_train: 67.5656 loss_val: 68.9406\n",
      "Epoch: 033 loss_train: 67.8182 loss_val: 68.9254\n",
      "Epoch: 034 loss_train: 67.7513 loss_val: 68.9168\n",
      "Epoch: 035 loss_train: 67.6330 loss_val: 68.9064\n",
      "Epoch: 036 loss_train: 67.6055 loss_val: 68.9075\n",
      "Epoch: 037 loss_train: 67.5932 loss_val: 68.9083\n",
      "Epoch: 038 loss_train: 67.5563 loss_val: 68.9154\n",
      "Epoch: 039 loss_train: 67.8079 loss_val: 68.9293\n",
      "Epoch: 040 loss_train: 67.2961 loss_val: 68.9377\n",
      "Epoch: 041 loss_train: 67.5081 loss_val: 68.9479\n",
      "Epoch: 042 loss_train: 67.3995 loss_val: 68.9461\n",
      "Epoch: 043 loss_train: 67.7147 loss_val: 68.9418\n",
      "Epoch: 044 loss_train: 68.1847 loss_val: 68.9375\n",
      "Epoch: 045 loss_train: 67.6677 loss_val: 68.9215\n",
      "Epoch: 046 loss_train: 67.4935 loss_val: 68.9010\n",
      "Epoch: 047 loss_train: 67.9651 loss_val: 68.8881\n",
      "Epoch: 048 loss_train: 68.1070 loss_val: 68.8802\n",
      "Epoch: 049 loss_train: 67.6550 loss_val: 68.8771\n",
      "Epoch: 050 loss_train: 67.6544 loss_val: 68.8767\n",
      "Epoch: 051 loss_train: 67.6457 loss_val: 68.8806\n",
      "Epoch: 052 loss_train: 67.5379 loss_val: 68.8888\n",
      "Epoch: 053 loss_train: 67.5209 loss_val: 68.8923\n",
      "Epoch: 054 loss_train: 67.5244 loss_val: 68.8982\n",
      "Epoch: 055 loss_train: 67.7700 loss_val: 68.8971\n",
      "Epoch: 056 loss_train: 67.7254 loss_val: 68.8951\n",
      "Epoch: 057 loss_train: 67.7731 loss_val: 68.9000\n",
      "Epoch: 058 loss_train: 67.6943 loss_val: 68.9110\n",
      "Epoch: 059 loss_train: 67.6704 loss_val: 68.9317\n",
      "Epoch: 060 loss_train: 67.7415 loss_val: 68.9424\n",
      "Epoch: 061 loss_train: 67.7555 loss_val: 68.9482\n",
      "Epoch: 062 loss_train: 67.7503 loss_val: 68.9493\n",
      "Epoch: 063 loss_train: 67.7198 loss_val: 68.9397\n",
      "Epoch: 064 loss_train: 67.5550 loss_val: 68.9122\n",
      "Epoch: 065 loss_train: 67.5954 loss_val: 68.8813\n",
      "Epoch: 066 loss_train: 67.6504 loss_val: 68.8611\n",
      "Epoch: 067 loss_train: 67.4072 loss_val: 68.8554\n",
      "Epoch: 068 loss_train: 67.8987 loss_val: 68.8649\n",
      "Epoch: 069 loss_train: 67.8054 loss_val: 68.8897\n",
      "Epoch: 070 loss_train: 67.9677 loss_val: 68.9206\n",
      "Epoch: 071 loss_train: 67.8376 loss_val: 68.9296\n",
      "Epoch: 072 loss_train: 67.9064 loss_val: 68.9279\n",
      "Epoch: 073 loss_train: 67.5409 loss_val: 68.9185\n",
      "Epoch: 074 loss_train: 67.9781 loss_val: 68.9035\n",
      "Epoch: 075 loss_train: 67.8171 loss_val: 68.8955\n",
      "Epoch: 076 loss_train: 67.7028 loss_val: 68.8923\n",
      "Epoch: 077 loss_train: 67.9064 loss_val: 68.8903\n",
      "Epoch: 078 loss_train: 67.5903 loss_val: 68.8939\n",
      "Epoch: 079 loss_train: 67.6390 loss_val: 68.9117\n",
      "Epoch: 080 loss_train: 67.7558 loss_val: 68.9244\n",
      "Epoch: 081 loss_train: 67.5009 loss_val: 68.9220\n",
      "Epoch: 082 loss_train: 67.5286 loss_val: 68.9203\n",
      "Epoch: 083 loss_train: 67.7070 loss_val: 68.9190\n",
      "Epoch: 084 loss_train: 67.8332 loss_val: 68.9022\n",
      "Epoch: 085 loss_train: 67.6677 loss_val: 68.8838\n",
      "Epoch: 086 loss_train: 67.7188 loss_val: 68.8597\n",
      "Epoch: 087 loss_train: 67.6686 loss_val: 68.8416\n",
      "Epoch: 088 loss_train: 67.8781 loss_val: 68.8417\n",
      "Epoch: 089 loss_train: 67.7904 loss_val: 68.8545\n",
      "Epoch: 090 loss_train: 67.6005 loss_val: 68.8801\n",
      "Epoch: 091 loss_train: 67.6407 loss_val: 68.9118\n",
      "Epoch: 092 loss_train: 67.6779 loss_val: 68.9416\n",
      "Epoch: 093 loss_train: 67.8801 loss_val: 68.9547\n",
      "Epoch: 094 loss_train: 67.6604 loss_val: 68.9523\n",
      "Epoch: 095 loss_train: 67.8195 loss_val: 68.9244\n",
      "Epoch: 096 loss_train: 67.6953 loss_val: 68.8902\n",
      "Epoch: 097 loss_train: 67.7215 loss_val: 68.8598\n",
      "Epoch: 098 loss_train: 67.7974 loss_val: 68.8451\n",
      "Epoch: 099 loss_train: 67.7064 loss_val: 68.8392\n",
      "Epoch: 100 loss_train: 67.4801 loss_val: 68.8532\n",
      "Epoch: 101 loss_train: 67.9266 loss_val: 68.8803\n",
      "Epoch: 102 loss_train: 67.4783 loss_val: 68.9064\n",
      "Epoch: 103 loss_train: 67.6972 loss_val: 68.9267\n",
      "Epoch: 104 loss_train: 67.8538 loss_val: 68.9397\n",
      "Epoch: 105 loss_train: 67.4868 loss_val: 68.9457\n",
      "Epoch: 106 loss_train: 67.7919 loss_val: 68.9468\n",
      "Epoch: 107 loss_train: 68.0735 loss_val: 68.9373\n",
      "Epoch: 108 loss_train: 67.9437 loss_val: 68.9204\n",
      "Epoch: 109 loss_train: 67.6842 loss_val: 68.9088\n",
      "Epoch: 110 loss_train: 67.5002 loss_val: 68.8969\n",
      "Epoch: 111 loss_train: 67.5397 loss_val: 68.8850\n",
      "Epoch: 112 loss_train: 67.7821 loss_val: 68.8743\n",
      "Epoch: 113 loss_train: 67.3892 loss_val: 68.8758\n",
      "Epoch: 114 loss_train: 67.5930 loss_val: 68.8800\n",
      "Epoch: 115 loss_train: 67.7114 loss_val: 68.8859\n",
      "Epoch: 116 loss_train: 67.4395 loss_val: 68.8910\n",
      "Epoch: 117 loss_train: 67.8178 loss_val: 68.8887\n",
      "Epoch: 118 loss_train: 67.6255 loss_val: 68.8816\n",
      "Epoch: 119 loss_train: 67.7496 loss_val: 68.8667\n",
      "Epoch: 120 loss_train: 67.5261 loss_val: 68.8519\n",
      "Epoch: 121 loss_train: 67.5357 loss_val: 68.8399\n",
      "Epoch: 122 loss_train: 67.7051 loss_val: 68.8307\n",
      "Epoch: 123 loss_train: 67.6747 loss_val: 68.8327\n",
      "Epoch: 124 loss_train: 67.7010 loss_val: 68.8408\n",
      "Epoch: 125 loss_train: 67.7886 loss_val: 68.8593\n",
      "Epoch: 126 loss_train: 67.7487 loss_val: 68.8801\n",
      "Epoch: 127 loss_train: 67.7338 loss_val: 68.8855\n",
      "Epoch: 128 loss_train: 67.6746 loss_val: 68.8778\n",
      "Epoch: 129 loss_train: 67.9115 loss_val: 68.8643\n",
      "Epoch: 130 loss_train: 67.0927 loss_val: 68.8490\n",
      "Epoch: 131 loss_train: 67.6972 loss_val: 68.8309\n",
      "Epoch: 132 loss_train: 67.8297 loss_val: 68.8242\n",
      "Epoch: 133 loss_train: 67.9105 loss_val: 68.8193\n",
      "Epoch: 134 loss_train: 67.5119 loss_val: 68.8207\n",
      "Epoch: 135 loss_train: 67.3123 loss_val: 68.8302\n",
      "Epoch: 136 loss_train: 67.6421 loss_val: 68.8432\n",
      "Epoch: 137 loss_train: 67.7187 loss_val: 68.8564\n",
      "Epoch: 138 loss_train: 67.7656 loss_val: 68.8714\n",
      "Epoch: 139 loss_train: 67.7104 loss_val: 68.8828\n",
      "Epoch: 140 loss_train: 67.6336 loss_val: 68.8820\n",
      "Epoch: 141 loss_train: 67.8111 loss_val: 68.8751\n",
      "Epoch: 142 loss_train: 68.1685 loss_val: 68.8656\n",
      "Epoch: 143 loss_train: 67.7885 loss_val: 68.8616\n",
      "Epoch: 144 loss_train: 67.8610 loss_val: 68.8691\n",
      "Epoch: 145 loss_train: 67.6820 loss_val: 68.8836\n",
      "Epoch: 146 loss_train: 68.0026 loss_val: 68.9030\n",
      "Epoch: 147 loss_train: 67.6576 loss_val: 68.9199\n",
      "Epoch: 148 loss_train: 67.7566 loss_val: 68.9246\n",
      "Epoch: 149 loss_train: 67.5856 loss_val: 68.9102\n",
      "Epoch: 150 loss_train: 67.6141 loss_val: 68.8818\n",
      "Epoch: 151 loss_train: 67.3987 loss_val: 68.8561\n",
      "Epoch: 152 loss_train: 67.5056 loss_val: 68.8425\n",
      "Epoch: 153 loss_train: 67.6136 loss_val: 68.8370\n",
      "Epoch: 154 loss_train: 67.5908 loss_val: 68.8359\n",
      "Epoch: 155 loss_train: 67.7114 loss_val: 68.8420\n",
      "Epoch: 156 loss_train: 67.5157 loss_val: 68.8524\n",
      "Epoch: 157 loss_train: 67.6595 loss_val: 68.8569\n",
      "Epoch: 158 loss_train: 67.7740 loss_val: 68.8607\n",
      "Epoch: 159 loss_train: 67.9927 loss_val: 68.8675\n",
      "Epoch: 160 loss_train: 68.0499 loss_val: 68.8635\n",
      "Epoch: 161 loss_train: 67.9431 loss_val: 68.8562\n",
      "Epoch: 162 loss_train: 67.6377 loss_val: 68.8513\n",
      "Epoch: 163 loss_train: 67.6915 loss_val: 68.8496\n",
      "Epoch: 164 loss_train: 67.6140 loss_val: 68.8506\n",
      "Epoch: 165 loss_train: 67.5640 loss_val: 68.8538\n",
      "Epoch: 166 loss_train: 67.9479 loss_val: 68.8647\n",
      "Epoch: 167 loss_train: 67.5506 loss_val: 68.8831\n",
      "Epoch: 168 loss_train: 67.6640 loss_val: 68.8998\n",
      "Epoch: 169 loss_train: 67.8282 loss_val: 68.9140\n",
      "Epoch: 170 loss_train: 67.4649 loss_val: 68.9138\n",
      "Epoch: 171 loss_train: 67.4463 loss_val: 68.9054\n",
      "Epoch: 172 loss_train: 67.6934 loss_val: 68.8835\n",
      "Epoch: 173 loss_train: 67.9746 loss_val: 68.8679\n",
      "Epoch: 174 loss_train: 67.8675 loss_val: 68.8582\n",
      "Epoch: 175 loss_train: 68.1173 loss_val: 68.8518\n",
      "Epoch: 176 loss_train: 67.3868 loss_val: 68.8540\n",
      "Epoch: 177 loss_train: 67.5821 loss_val: 68.8596\n",
      "Epoch: 178 loss_train: 67.4163 loss_val: 68.8606\n",
      "Epoch: 179 loss_train: 67.7860 loss_val: 68.8668\n",
      "Epoch: 180 loss_train: 67.8694 loss_val: 68.8790\n",
      "Epoch: 181 loss_train: 68.0278 loss_val: 68.8944\n",
      "Epoch: 182 loss_train: 67.6479 loss_val: 68.8966\n",
      "Epoch: 183 loss_train: 67.6290 loss_val: 68.8887\n",
      "Epoch: 184 loss_train: 67.8638 loss_val: 68.8831\n",
      "Epoch: 185 loss_train: 67.4296 loss_val: 68.8734\n",
      "Epoch: 186 loss_train: 67.9917 loss_val: 68.8596\n",
      "Epoch: 187 loss_train: 67.5544 loss_val: 68.8440\n",
      "Epoch: 188 loss_train: 67.5194 loss_val: 68.8377\n",
      "Epoch: 189 loss_train: 67.8014 loss_val: 68.8464\n",
      "Epoch: 190 loss_train: 67.4314 loss_val: 68.8494\n",
      "Epoch: 191 loss_train: 67.8204 loss_val: 68.8589\n",
      "Epoch: 192 loss_train: 67.8932 loss_val: 68.8637\n",
      "Epoch: 193 loss_train: 67.7169 loss_val: 68.8558\n",
      "Epoch: 194 loss_train: 67.6850 loss_val: 68.8447\n",
      "Epoch: 195 loss_train: 67.5277 loss_val: 68.8264\n",
      "Epoch: 196 loss_train: 67.8643 loss_val: 68.8058\n",
      "Epoch: 197 loss_train: 67.7652 loss_val: 68.7973\n",
      "Epoch: 198 loss_train: 67.8379 loss_val: 68.8101\n",
      "Epoch: 199 loss_train: 67.4611 loss_val: 68.8310\n",
      "Epoch: 200 loss_train: 67.6931 loss_val: 68.8500\n",
      "Epoch: 001 loss_train: 67.6547 loss_val: 68.8016\n",
      "Epoch: 002 loss_train: 67.7058 loss_val: 68.8047\n",
      "Epoch: 003 loss_train: 67.3644 loss_val: 68.8223\n",
      "Epoch: 004 loss_train: 67.7530 loss_val: 68.8519\n",
      "Epoch: 005 loss_train: 67.2601 loss_val: 68.8688\n",
      "Epoch: 006 loss_train: 67.4275 loss_val: 68.8629\n",
      "Epoch: 007 loss_train: 67.8269 loss_val: 68.8467\n",
      "Epoch: 008 loss_train: 67.4685 loss_val: 68.8329\n",
      "Epoch: 009 loss_train: 67.6766 loss_val: 68.8248\n",
      "Epoch: 010 loss_train: 68.0256 loss_val: 68.8200\n",
      "Epoch: 011 loss_train: 67.7533 loss_val: 68.8194\n",
      "Epoch: 012 loss_train: 67.8574 loss_val: 68.8281\n",
      "Epoch: 013 loss_train: 67.7417 loss_val: 68.8368\n",
      "Epoch: 014 loss_train: 67.3417 loss_val: 68.8466\n",
      "Epoch: 015 loss_train: 67.5500 loss_val: 68.8526\n",
      "Epoch: 016 loss_train: 67.6148 loss_val: 68.8586\n",
      "Epoch: 017 loss_train: 67.7984 loss_val: 68.8630\n",
      "Epoch: 018 loss_train: 67.4208 loss_val: 68.8591\n",
      "Epoch: 019 loss_train: 67.2661 loss_val: 68.8508\n",
      "Epoch: 020 loss_train: 67.4600 loss_val: 68.8407\n",
      "Epoch: 021 loss_train: 67.8301 loss_val: 68.8304\n",
      "Epoch: 022 loss_train: 67.5805 loss_val: 68.8172\n",
      "Epoch: 023 loss_train: 67.5583 loss_val: 68.8059\n",
      "Epoch: 024 loss_train: 67.7622 loss_val: 68.7983\n",
      "Epoch: 025 loss_train: 67.6855 loss_val: 68.8004\n",
      "Epoch: 026 loss_train: 67.3675 loss_val: 68.8024\n",
      "Epoch: 027 loss_train: 67.7937 loss_val: 68.8133\n",
      "Epoch: 028 loss_train: 67.4918 loss_val: 68.8215\n",
      "Epoch: 029 loss_train: 67.6393 loss_val: 68.8342\n",
      "Epoch: 030 loss_train: 67.6083 loss_val: 68.8449\n",
      "Epoch: 031 loss_train: 67.6540 loss_val: 68.8546\n",
      "Epoch: 032 loss_train: 67.8820 loss_val: 68.8604\n",
      "Epoch: 033 loss_train: 67.7621 loss_val: 68.8681\n",
      "Epoch: 034 loss_train: 67.7629 loss_val: 68.8713\n",
      "Epoch: 035 loss_train: 67.8186 loss_val: 68.8697\n",
      "Epoch: 036 loss_train: 67.4335 loss_val: 68.8680\n",
      "Epoch: 037 loss_train: 67.8451 loss_val: 68.8683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-3b1ca6db4297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindex_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss_trains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/INF554/myEnv/lib64/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/INF554/myEnv/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "lr=2e-2\n",
    "for i in range(15):\n",
    "    lr/=2\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_x)\n",
    "        loss_train = loss(output.reshape(-1), hindex_train_x)\n",
    "        loss_trains.append(loss_train.item())\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        output= model(X_val_x)\n",
    "\n",
    "        loss_val = loss(output.reshape(-1), hindex_val_x)\n",
    "        loss_vals.append(loss_val.item())\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\"lr step: \", str(i),\n",
    "                'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                'loss_val: {:.4f}'.format(loss_val.item()))\n",
    "        if (epoch>100 and loss_val.item()>loss_train.item()*1.05):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.plot(loss_vals[400:])\n",
    "plt.plot(loss_trains[400:])\n",
    "plt.legend({\"Validation Loss\", \"Training Loss\"})\n",
    "plt.title(\"Layout : 256-64, lr : from 1e-2 to 4.8e-6\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8828125e-06\n"
     ]
    }
   ],
   "source": [
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Global/256_64_0.1_lr15.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_embedding_glob=np.load(\"Global/full_embedding_matrix.npy\")\n",
    "idx=range(n_train)\n",
    "nodes_train=[abs_nodeID_Train[i] for i in idx]\n",
    "X_train_glob = torch.tensor([full_embedding_glob[nodeID_abs_Graph[node]][1:] for node in nodes_train], dtype=torch.float32)\n",
    "hindex_train_glob=torch.tensor([abs_hindex_Train[i] for i in idx], dtype=torch.float32)\n",
    "\n",
    "X_test_glob = torch.tensor([full_embedding_glob[nodeID_abs_Graph[node]][1:] for node in nodeID_abs_Test.keys()], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 116486.0938\n",
      "Epoch: 002 loss_train: 7547118.5000\n",
      "Epoch: 003 loss_train: 384990.5625\n",
      "Epoch: 004 loss_train: 1541.2650\n",
      "Epoch: 005 loss_train: 230.1860\n",
      "Epoch: 006 loss_train: 232.0374\n",
      "Epoch: 007 loss_train: 241.9996\n",
      "Epoch: 008 loss_train: 216.2106\n",
      "Epoch: 009 loss_train: 219.7001\n",
      "Epoch: 010 loss_train: 210.5331\n",
      "Epoch: 011 loss_train: 215.8348\n",
      "Epoch: 012 loss_train: 220.6859\n",
      "Epoch: 013 loss_train: 218.3036\n",
      "Epoch: 014 loss_train: 210.9053\n",
      "Epoch: 015 loss_train: 211.5571\n",
      "Epoch: 016 loss_train: 197.7526\n",
      "Epoch: 017 loss_train: 203.0508\n",
      "Epoch: 018 loss_train: 190.7409\n",
      "Epoch: 019 loss_train: 203.8139\n",
      "Epoch: 020 loss_train: 187.9013\n",
      "Epoch: 021 loss_train: 203.3395\n",
      "Epoch: 022 loss_train: 184.5765\n",
      "Epoch: 023 loss_train: 202.7844\n",
      "Epoch: 024 loss_train: 181.1175\n",
      "Epoch: 025 loss_train: 193.5916\n",
      "Epoch: 026 loss_train: 178.8073\n"
     ]
    }
   ],
   "source": [
    "n_dim=X_train_x.shape[1]\n",
    "model_glob=MLP(n_dim,256,64,0.2)\n",
    "\n",
    "loss_trains_glob=[]\n",
    "loss = nn.MSELoss()\n",
    "lr=2e-2\n",
    "for i in range(10):\n",
    "    lr/=2\n",
    "    optimizer = optim.Adam(model_glob.parameters(), lr=lr)\n",
    "    for epoch in range(200):\n",
    "        model_glob.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glob(X_train_glob)\n",
    "        loss_train_glob = loss(output.reshape(-1), hindex_train_glob)\n",
    "        loss_trains_glob.append(loss_train_glob.item())\n",
    "        loss_train_glob.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "                'loss_train: {:.4f}'.format(loss_train_glob.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1404962b70>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.plot(loss_trains_glob[400:])\n",
    "plt.title(\"Whole set : Layout : 256-64, lr : from 1e-2 to \", str(lr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_glob.state_dict(), \"Global/full_train_model_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_test=[abs_nodeID_Test[i] for i in range(n_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred=model_glob(X_test_glob)\n",
    "submission=dict([(nodes_test[i], _pred[i]) for i in range(len(X_test_glob))])\n",
    "with open(\"submissions/deepwalk_MLP_full_emb_submission_2.csv\", 'w') as f:\n",
    "    f.write(\"author,hindex\\n\")\n",
    "    for k,h in submission.items():\n",
    "        f.write(str(k)+\",\"+str(h.item())+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "571ac4587ae4eeb6b02353bd76aeaaf0ceca15cf49d684242a7eb1fb5d42efb7"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('myEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
