{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Global embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory : /users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project/Global, Project directory : /users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model as LinearModels\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "from scipy import stats\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "NathanPath=\"d:\\Documents\\Info\\INF554\\INF554_Kaggle_Project\"\n",
    "NathanPath=\"/users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project\"\n",
    "\n",
    "project_path = str(Path(os.getcwd()).parent.absolute())\n",
    "print(\"Current directory : \" + os.getcwd() + \", Project directory : \" + project_path)\n",
    "\n",
    "os.chdir(project_path)\n",
    "os.chdir(NathanPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=np.load(\"Global/paper_vectors.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_abstracts=vectors[:,0].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624181\n",
      "2908511069\n",
      "3603\n"
     ]
    }
   ],
   "source": [
    "print(len(id_abstracts))\n",
    "print(np.max(id_abstracts))\n",
    "print(np.min(id_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_id_abstracts=dict([(a,b) for a,b in enumerate(id_abstracts)])\n",
    "id_abstracts_num=dict([(b,a) for a,b in enumerate(id_abstracts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624181"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_abstracts_num.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/author_papers.txt\") as f:\n",
    "    authors_papers=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=vectors.shape[1]-1\n",
    "authors_vectors=np.zeros((len(authors_papers), n_dim+1), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1510273386', '1827736641', '1588673897', '2252711322', '2123653597']\n",
      "1510273386\n",
      "58046\n"
     ]
    }
   ],
   "source": [
    "papers=authors_papers[0].split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "print(papers)\n",
    "p=papers[0]\n",
    "print(int(p))\n",
    "print(id_abstracts_num.get(int(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880\n"
     ]
    }
   ],
   "source": [
    "s=0\n",
    "for i,author in enumerate(authors_papers):\n",
    "    papers=author.split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "    vector=np.zeros(n_dim)\n",
    "    no_fail=False\n",
    "    for p in papers:\n",
    "        try:\n",
    "            vector+=vectors[id_abstracts_num[int(p)], 1:]\n",
    "            no_fail=True\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if (not no_fail):\n",
    "        s+=1\n",
    "    authors_vectors[i][0]=int(author.split(\":\")[0])\n",
    "    if (np.linalg.norm(vector)>0):\n",
    "        vector=vector/np.linalg.norm(vector)\n",
    "    authors_vectors[i][1:]=vector.copy()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Global/authors_vectors.npy\", authors_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_improved=np.load(\"DeepWalk/embeddings_improved.npy\")\n",
    "authors_vectors=np.load(\"Global/authors_vectors.npy\")\n",
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217801, 135)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_improved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217801, 135)\n",
      "(217801, 151)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_improved.shape)\n",
    "print(authors_vectors.shape)\n",
    "\n",
    "auth_vec_num_id_author=dict([(a,b) for a,b in enumerate(authors_vectors[:,0])])\n",
    "id_author_auth_vec_num=dict([(b,a) for a,b in enumerate(authors_vectors[:,0])])\n",
    "\n",
    "graph_num_id_author=dict([(a,b) for a,b in enumerate(G.nodes)])\n",
    "id_author_graph_num=dict([(b,a) for a,b in enumerate(G.nodes)])\n",
    "\n",
    "emb_num_id_author=dict([(a,int(b)) for a,b in enumerate(embeddings_improved[:,0])])\n",
    "id_author_emb_num=dict([(int(b),a) for a,b in enumerate(embeddings_improved[:,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=G.number_of_nodes()\n",
    "n_emb=embeddings_improved.shape[1]-1\n",
    "n_abs=authors_vectors.shape[1]-1\n",
    "n_dim_tot=1+n_emb+n_abs\n",
    "full_matrix=np.zeros((n_nodes, n_dim_tot), dtype=np.float64)\n",
    "for i in range(n_nodes):\n",
    "    node=graph_num_id_author[i]\n",
    "    full_matrix[i,0]=node\n",
    "    full_matrix[i,1:1+n_emb]=embeddings_improved[id_author_emb_num[node],1:].copy()\n",
    "    full_matrix[i,1+n_emb:]=authors_vectors[id_author_auth_vec_num[node],1:].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Global/full_embedding_matrix.npy\", full_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217801, 285)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP model\"\"\"\n",
    "    def __init__(self, n_feat, n_hidden_1, n_hidden_2, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z0 = self.relu(self.fc1(x))\n",
    "        z0 = self.dropout(z0)\n",
    "        z1 = self.relu(self.fc2(z0))\n",
    "        out = self.fc3(z1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X,Y):\n",
    "    if (len(X)!=len(Y)):\n",
    "        print(\"Sizes not identical\")\n",
    "        return -1\n",
    "    return (X-Y)@(X-Y) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'author': np.int64})\n",
    "full_embedding=np.load(\"Global/full_embedding_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_nodeID_Train=dict(df_train[\"author\"])\n",
    "nodeID_abs_Train=dict([(b,a) for a,b in abs_nodeID_Train.items()])\n",
    "\n",
    "abs_nodeID_Test=dict(df_test[\"author\"])\n",
    "nodeID_abs_Test=dict([(b,a) for a,b in abs_nodeID_Test.items()])\n",
    "\n",
    "abs_hindex_Train=dict(df_train[\"hindex\"])\n",
    "\n",
    "abs_nodeID_Graph=dict(enumerate(G.nodes))\n",
    "nodeID_abs_Graph=dict([(b,a) for a,b in enumerate(G.nodes)])\n",
    "\n",
    "n=G.number_of_nodes()\n",
    "n_train=abs_nodeID_Train.__len__()\n",
    "n_test=abs_nodeID_Test.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Careful, those indexes are related to the TRAIN set, not to the global graph indexing\n",
    "idx=np.random.permutation(n_train)\n",
    "idx_train=idx[:int(0.8*n_train)]\n",
    "idx_val=idx[int(0.8*n_train):]\n",
    "\n",
    "nodes_train=[abs_nodeID_Train[i] for i in idx_train]\n",
    "nodes_val=[abs_nodeID_Train[i] for i in idx_val]\n",
    "\n",
    "X_train_x = torch.tensor([full_embedding[nodeID_abs_Graph[node]][1:] for node in nodes_train], dtype=torch.float32)\n",
    "X_val_x = torch.tensor([full_embedding[nodeID_abs_Graph[node]][1:] for node in nodes_val], dtype=torch.float32)\n",
    "\n",
    "hindex_train_x=torch.tensor([abs_hindex_Train[i] for i in idx_train], dtype=torch.float32)\n",
    "hindex_val_x=torch.tensor([abs_hindex_Train[i] for i in idx_val], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Training on split set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 24456.9258 loss_val: 23239666.0000\n",
      "Epoch: 002 loss_train: 21595168.0000 loss_val: 936665.3750\n",
      "Epoch: 003 loss_train: 993967.0000 loss_val: 34175.3438\n",
      "Epoch: 004 loss_train: 26273.2520 loss_val: 98878.7500\n",
      "Epoch: 005 loss_train: 91960.1562 loss_val: 119127.1484\n",
      "Epoch: 006 loss_train: 111189.6094 loss_val: 119311.7031\n",
      "Epoch: 007 loss_train: 111050.9609 loss_val: 92813.2031\n",
      "Epoch: 008 loss_train: 87200.9844 loss_val: 63171.0078\n",
      "Epoch: 009 loss_train: 62643.6602 loss_val: 31367.2793\n",
      "Epoch: 010 loss_train: 29473.5566 loss_val: 3923.4714\n",
      "Epoch: 011 loss_train: 5213.5225 loss_val: 9772.4307\n",
      "Epoch: 012 loss_train: 10233.6201 loss_val: 20047.4629\n",
      "Epoch: 013 loss_train: 17810.2305 loss_val: 10254.8555\n",
      "Epoch: 014 loss_train: 8837.1348 loss_val: 2154.5027\n",
      "Epoch: 015 loss_train: 1601.2268 loss_val: 1177.2576\n",
      "Epoch: 016 loss_train: 5542.6445 loss_val: 544.7256\n",
      "Epoch: 017 loss_train: 1865.1047 loss_val: 201.5049\n",
      "Epoch: 018 loss_train: 548.5706 loss_val: 192.2957\n",
      "Epoch: 019 loss_train: 272.2757 loss_val: 194.5842\n",
      "Epoch: 020 loss_train: 230.9754 loss_val: 200.4130\n",
      "Epoch: 021 loss_train: 231.2373 loss_val: 220.7141\n",
      "Epoch: 022 loss_train: 225.8229 loss_val: 221.5612\n",
      "Epoch: 023 loss_train: 227.1296 loss_val: 221.7934\n",
      "Epoch: 024 loss_train: 226.9527 loss_val: 221.9180\n",
      "Epoch: 025 loss_train: 227.2354 loss_val: 221.9281\n",
      "Epoch: 026 loss_train: 226.7220 loss_val: 221.7602\n",
      "Epoch: 027 loss_train: 227.3321 loss_val: 221.3594\n",
      "Epoch: 028 loss_train: 227.0193 loss_val: 220.7428\n",
      "Epoch: 029 loss_train: 226.1400 loss_val: 219.9482\n",
      "Epoch: 030 loss_train: 225.6999 loss_val: 219.0898\n",
      "Epoch: 031 loss_train: 224.8450 loss_val: 218.2958\n",
      "Epoch: 032 loss_train: 224.9409 loss_val: 217.6571\n",
      "Epoch: 033 loss_train: 223.2279 loss_val: 217.0038\n",
      "Epoch: 034 loss_train: 223.3434 loss_val: 216.4547\n",
      "Epoch: 035 loss_train: 222.0783 loss_val: 215.7563\n",
      "Epoch: 036 loss_train: 221.7059 loss_val: 214.9471\n",
      "Epoch: 037 loss_train: 220.7446 loss_val: 214.0127\n",
      "Epoch: 038 loss_train: 219.7835 loss_val: 212.9788\n",
      "Epoch: 039 loss_train: 218.9175 loss_val: 211.8945\n",
      "Epoch: 040 loss_train: 217.9485 loss_val: 210.8068\n",
      "Epoch: 041 loss_train: 216.7632 loss_val: 209.5833\n",
      "Epoch: 042 loss_train: 215.9528 loss_val: 208.2147\n",
      "Epoch: 043 loss_train: 215.4631 loss_val: 207.1392\n",
      "Epoch: 044 loss_train: 216.4255 loss_val: 206.4587\n",
      "Epoch: 045 loss_train: 212.4214 loss_val: 205.6484\n",
      "Epoch: 046 loss_train: 211.8446 loss_val: 204.7284\n",
      "Epoch: 047 loss_train: 210.9411 loss_val: 203.6316\n",
      "Epoch: 048 loss_train: 211.7670 loss_val: 202.8468\n",
      "Epoch: 049 loss_train: 209.5675 loss_val: 201.9340\n",
      "Epoch: 050 loss_train: 208.6958 loss_val: 200.8331\n",
      "Epoch: 051 loss_train: 208.1415 loss_val: 199.7191\n",
      "Epoch: 052 loss_train: 206.7082 loss_val: 198.4350\n",
      "Epoch: 053 loss_train: 205.3403 loss_val: 196.9971\n",
      "Epoch: 054 loss_train: 204.0460 loss_val: 195.3816\n",
      "Epoch: 055 loss_train: 202.8086 loss_val: 193.5645\n",
      "Epoch: 056 loss_train: 200.5323 loss_val: 191.4778\n",
      "Epoch: 057 loss_train: 198.4106 loss_val: 189.0102\n",
      "Epoch: 058 loss_train: 195.5739 loss_val: 185.9878\n",
      "Epoch: 059 loss_train: 191.9121 loss_val: 182.1369\n",
      "Epoch: 060 loss_train: 205.6407 loss_val: 182.6043\n",
      "Epoch: 061 loss_train: 190.1052 loss_val: 182.8959\n",
      "Epoch: 062 loss_train: 189.1471 loss_val: 182.6519\n",
      "Epoch: 063 loss_train: 189.1999 loss_val: 182.0322\n",
      "Epoch: 064 loss_train: 188.9486 loss_val: 181.0029\n",
      "Epoch: 065 loss_train: 188.7271 loss_val: 179.9345\n",
      "Epoch: 066 loss_train: 187.3124 loss_val: 178.5539\n",
      "Epoch: 067 loss_train: 186.9626 loss_val: 177.2964\n",
      "Epoch: 068 loss_train: 225.0204 loss_val: 178.8327\n",
      "Epoch: 069 loss_train: 187.1622 loss_val: 179.8078\n",
      "Epoch: 070 loss_train: 188.3270 loss_val: 180.3630\n",
      "Epoch: 071 loss_train: 189.0544 loss_val: 180.5731\n",
      "Epoch: 072 loss_train: 189.8474 loss_val: 180.4863\n",
      "Epoch: 073 loss_train: 189.3912 loss_val: 180.1537\n",
      "Epoch: 074 loss_train: 189.3460 loss_val: 179.6506\n",
      "Epoch: 075 loss_train: 189.0768 loss_val: 179.0186\n",
      "Epoch: 076 loss_train: 188.6917 loss_val: 178.2660\n",
      "Epoch: 077 loss_train: 188.3740 loss_val: 177.3998\n",
      "Epoch: 078 loss_train: 187.5387 loss_val: 176.4291\n",
      "Epoch: 079 loss_train: 186.7461 loss_val: 175.3632\n",
      "Epoch: 080 loss_train: 185.8439 loss_val: 174.2034\n",
      "Epoch: 081 loss_train: 184.6468 loss_val: 172.9093\n",
      "Epoch: 082 loss_train: 183.7005 loss_val: 171.4474\n",
      "Epoch: 083 loss_train: 182.2165 loss_val: 169.7856\n",
      "Epoch: 084 loss_train: 181.0912 loss_val: 167.9264\n",
      "Epoch: 085 loss_train: 179.1797 loss_val: 165.8634\n",
      "Epoch: 086 loss_train: 177.1367 loss_val: 163.5314\n",
      "Epoch: 087 loss_train: 175.8301 loss_val: 161.2740\n",
      "Epoch: 088 loss_train: 176.1783 loss_val: 160.0737\n",
      "Epoch: 089 loss_train: 178.9761 loss_val: 160.7781\n",
      "Epoch: 090 loss_train: 173.6692 loss_val: 161.2841\n",
      "Epoch: 091 loss_train: 172.8241 loss_val: 162.0459\n",
      "Epoch: 092 loss_train: 173.4958 loss_val: 162.5684\n",
      "Epoch: 093 loss_train: 171.8396 loss_val: 162.4484\n",
      "Epoch: 094 loss_train: 171.2607 loss_val: 161.8940\n",
      "Epoch: 095 loss_train: 171.1467 loss_val: 160.9014\n",
      "Epoch: 096 loss_train: 170.0189 loss_val: 159.3798\n",
      "Epoch: 097 loss_train: 168.2119 loss_val: 157.6250\n",
      "Epoch: 098 loss_train: 171.8672 loss_val: 156.9154\n",
      "Epoch: 099 loss_train: 168.1148 loss_val: 157.0830\n",
      "Epoch: 100 loss_train: 169.3331 loss_val: 157.6740\n",
      "Epoch: 101 loss_train: 166.4272 loss_val: 157.3297\n",
      "Epoch: 102 loss_train: 166.1425 loss_val: 156.5693\n",
      "Epoch: 103 loss_train: 165.4876 loss_val: 155.1982\n",
      "Epoch: 104 loss_train: 164.3199 loss_val: 152.7930\n",
      "Epoch: 105 loss_train: 162.7868 loss_val: 149.7913\n",
      "Epoch: 106 loss_train: 168.8080 loss_val: 152.8742\n",
      "Epoch: 107 loss_train: 162.1947 loss_val: 154.7735\n",
      "Epoch: 108 loss_train: 164.4436 loss_val: 155.4694\n",
      "Epoch: 109 loss_train: 164.8399 loss_val: 155.0623\n",
      "Epoch: 110 loss_train: 164.2798 loss_val: 153.3443\n",
      "Epoch: 111 loss_train: 163.1591 loss_val: 151.1337\n",
      "Epoch: 112 loss_train: 160.8625 loss_val: 148.1423\n",
      "Epoch: 113 loss_train: 166.7458 loss_val: 148.7469\n",
      "Epoch: 114 loss_train: 159.0638 loss_val: 149.1174\n",
      "Epoch: 115 loss_train: 158.1194 loss_val: 148.5400\n",
      "Epoch: 116 loss_train: 158.0335 loss_val: 146.3544\n",
      "Epoch: 117 loss_train: 155.8028 loss_val: 142.3792\n",
      "Epoch: 118 loss_train: 159.3622 loss_val: 145.7762\n",
      "Epoch: 119 loss_train: 154.2900 loss_val: 147.9069\n",
      "Epoch: 120 loss_train: 155.5567 loss_val: 148.0868\n",
      "Epoch: 121 loss_train: 154.7500 loss_val: 146.3439\n",
      "Epoch: 122 loss_train: 153.9474 loss_val: 143.7276\n",
      "Epoch: 123 loss_train: 152.9259 loss_val: 140.6876\n",
      "Epoch: 124 loss_train: 150.1784 loss_val: 139.7865\n",
      "Epoch: 125 loss_train: 153.7747 loss_val: 143.7357\n",
      "Epoch: 126 loss_train: 151.0747 loss_val: 144.8845\n",
      "Epoch: 127 loss_train: 152.2699 loss_val: 144.0624\n",
      "Epoch: 128 loss_train: 152.1037 loss_val: 142.2812\n",
      "Epoch: 129 loss_train: 151.1659 loss_val: 139.9682\n",
      "Epoch: 130 loss_train: 149.4262 loss_val: 137.2393\n",
      "Epoch: 131 loss_train: 147.0810 loss_val: 133.7039\n",
      "Epoch: 132 loss_train: 150.4523 loss_val: 135.7990\n",
      "Epoch: 133 loss_train: 144.5280 loss_val: 137.7622\n",
      "Epoch: 134 loss_train: 144.4912 loss_val: 137.6505\n",
      "Epoch: 135 loss_train: 146.3625 loss_val: 136.9005\n",
      "Epoch: 136 loss_train: 144.4005 loss_val: 134.9681\n",
      "Epoch: 137 loss_train: 143.8723 loss_val: 131.8201\n",
      "Epoch: 138 loss_train: 145.2859 loss_val: 134.8319\n",
      "Epoch: 139 loss_train: 142.5285 loss_val: 135.8611\n",
      "Epoch: 140 loss_train: 144.2229 loss_val: 135.2426\n",
      "Epoch: 141 loss_train: 143.1098 loss_val: 133.2442\n",
      "Epoch: 142 loss_train: 141.6309 loss_val: 130.6326\n",
      "Epoch: 143 loss_train: 141.1206 loss_val: 128.9363\n",
      "Epoch: 144 loss_train: 139.0596 loss_val: 128.9406\n",
      "Epoch: 145 loss_train: 138.8650 loss_val: 130.8115\n",
      "Epoch: 146 loss_train: 139.8194 loss_val: 132.3044\n",
      "Epoch: 147 loss_train: 139.1941 loss_val: 130.8236\n",
      "Epoch: 148 loss_train: 137.4992 loss_val: 126.3111\n",
      "Epoch: 149 loss_train: 137.0784 loss_val: 124.9503\n",
      "Epoch: 150 loss_train: 136.6604 loss_val: 127.1914\n",
      "Epoch: 151 loss_train: 134.3986 loss_val: 127.8185\n",
      "Epoch: 152 loss_train: 135.6645 loss_val: 127.9750\n",
      "Epoch: 153 loss_train: 132.9809 loss_val: 124.3217\n",
      "Epoch: 154 loss_train: 131.0369 loss_val: 120.9946\n",
      "Epoch: 155 loss_train: 135.3805 loss_val: 128.5247\n",
      "Epoch: 156 loss_train: 133.2516 loss_val: 128.7150\n",
      "Epoch: 157 loss_train: 133.2476 loss_val: 123.9537\n",
      "Epoch: 158 loss_train: 130.6171 loss_val: 119.0412\n",
      "Epoch: 159 loss_train: 130.0598 loss_val: 119.8181\n",
      "Epoch: 160 loss_train: 128.8089 loss_val: 121.8874\n",
      "Epoch: 161 loss_train: 129.0913 loss_val: 121.2399\n",
      "Epoch: 162 loss_train: 127.9955 loss_val: 116.8478\n",
      "Epoch: 163 loss_train: 128.5922 loss_val: 119.0371\n",
      "Epoch: 164 loss_train: 127.4385 loss_val: 120.3011\n",
      "Epoch: 165 loss_train: 127.8400 loss_val: 118.4825\n",
      "Epoch: 166 loss_train: 125.3400 loss_val: 112.7095\n",
      "Epoch: 167 loss_train: 125.1103 loss_val: 114.6384\n",
      "Epoch: 168 loss_train: 125.9805 loss_val: 118.4710\n",
      "Epoch: 169 loss_train: 123.3093 loss_val: 114.2393\n",
      "Epoch: 170 loss_train: 123.1875 loss_val: 110.4279\n",
      "Epoch: 171 loss_train: 121.5025 loss_val: 107.8745\n",
      "Epoch: 172 loss_train: 120.0692 loss_val: 100.7525\n",
      "Epoch: 173 loss_train: 118.8021 loss_val: 99.9756\n",
      "Epoch: 174 loss_train: 116.8551 loss_val: 98.4493\n",
      "Epoch: 175 loss_train: 115.0703 loss_val: 97.0827\n",
      "Epoch: 176 loss_train: 114.7760 loss_val: 98.6420\n",
      "Epoch: 177 loss_train: 112.8787 loss_val: 100.0732\n",
      "Epoch: 178 loss_train: 113.1499 loss_val: 98.0052\n",
      "Epoch: 179 loss_train: 112.6774 loss_val: 96.3696\n",
      "Epoch: 180 loss_train: 116.1863 loss_val: 101.7415\n",
      "Epoch: 181 loss_train: 115.8610 loss_val: 99.6398\n",
      "Epoch: 182 loss_train: 112.4958 loss_val: 95.1666\n",
      "Epoch: 183 loss_train: 113.2418 loss_val: 94.8004\n",
      "Epoch: 184 loss_train: 118.0676 loss_val: 99.8341\n",
      "Epoch: 185 loss_train: 112.4666 loss_val: 108.2479\n",
      "Epoch: 186 loss_train: 117.7591 loss_val: 105.3499\n",
      "Epoch: 187 loss_train: 117.3011 loss_val: 98.0259\n",
      "Epoch: 188 loss_train: 112.9038 loss_val: 96.2313\n",
      "Epoch: 189 loss_train: 113.5401 loss_val: 96.1661\n",
      "Epoch: 190 loss_train: 113.3264 loss_val: 100.1843\n",
      "Epoch: 191 loss_train: 112.0647 loss_val: 103.2622\n",
      "Epoch: 192 loss_train: 113.7222 loss_val: 99.2777\n",
      "Epoch: 193 loss_train: 110.1348 loss_val: 94.3599\n",
      "Epoch: 194 loss_train: 108.8840 loss_val: 94.6694\n",
      "Epoch: 195 loss_train: 110.4965 loss_val: 96.5189\n",
      "Epoch: 196 loss_train: 107.3194 loss_val: 99.3855\n",
      "Epoch: 197 loss_train: 110.0675 loss_val: 94.0935\n",
      "Epoch: 198 loss_train: 106.1124 loss_val: 93.6118\n",
      "Epoch: 199 loss_train: 110.7045 loss_val: 95.8937\n",
      "Epoch: 200 loss_train: 107.0311 loss_val: 98.4300\n",
      "Epoch: 201 loss_train: 107.9700 loss_val: 94.1345\n",
      "Epoch: 202 loss_train: 105.9567 loss_val: 92.9555\n",
      "Epoch: 203 loss_train: 107.2607 loss_val: 94.4892\n",
      "Epoch: 204 loss_train: 106.6177 loss_val: 96.1634\n",
      "Epoch: 205 loss_train: 106.4651 loss_val: 95.0144\n",
      "Epoch: 206 loss_train: 105.8140 loss_val: 92.7501\n",
      "Epoch: 207 loss_train: 105.2001 loss_val: 92.4137\n",
      "Epoch: 208 loss_train: 105.8430 loss_val: 93.9158\n",
      "Epoch: 209 loss_train: 104.3696 loss_val: 95.8317\n",
      "Epoch: 210 loss_train: 105.1869 loss_val: 94.1920\n",
      "Epoch: 211 loss_train: 104.5759 loss_val: 92.0922\n",
      "Epoch: 212 loss_train: 104.6792 loss_val: 92.0603\n",
      "Epoch: 213 loss_train: 105.6132 loss_val: 93.7597\n",
      "Epoch: 214 loss_train: 103.9374 loss_val: 92.8630\n",
      "Epoch: 215 loss_train: 102.5409 loss_val: 92.0119\n",
      "Epoch: 216 loss_train: 103.4457 loss_val: 92.0213\n",
      "Epoch: 217 loss_train: 105.7148 loss_val: 92.3891\n",
      "Epoch: 218 loss_train: 111.6715 loss_val: 100.0127\n",
      "Epoch: 219 loss_train: 108.0847 loss_val: 101.2801\n",
      "Epoch: 220 loss_train: 109.7768 loss_val: 98.3629\n",
      "Epoch: 221 loss_train: 108.3587 loss_val: 96.1439\n",
      "Epoch: 222 loss_train: 108.5723 loss_val: 92.3300\n",
      "Epoch: 223 loss_train: 106.3120 loss_val: 95.3922\n",
      "Epoch: 224 loss_train: 108.4378 loss_val: 96.4159\n",
      "Epoch: 225 loss_train: 107.9104 loss_val: 94.3663\n",
      "Epoch: 226 loss_train: 105.8783 loss_val: 94.9076\n",
      "Epoch: 227 loss_train: 107.2047 loss_val: 95.0467\n",
      "Epoch: 228 loss_train: 104.6953 loss_val: 95.1900\n",
      "Epoch: 229 loss_train: 103.5047 loss_val: 94.8065\n",
      "Epoch: 230 loss_train: 103.4840 loss_val: 92.1093\n",
      "Epoch: 231 loss_train: 104.6100 loss_val: 91.5737\n",
      "Epoch: 232 loss_train: 103.7456 loss_val: 91.4283\n",
      "Epoch: 233 loss_train: 101.5262 loss_val: 92.4600\n",
      "Epoch: 234 loss_train: 101.2006 loss_val: 92.6062\n",
      "Epoch: 235 loss_train: 101.9864 loss_val: 92.4790\n",
      "Epoch: 236 loss_train: 102.5155 loss_val: 92.1295\n",
      "Epoch: 237 loss_train: 101.6216 loss_val: 91.3306\n",
      "Epoch: 238 loss_train: 100.9430 loss_val: 89.7042\n",
      "Epoch: 239 loss_train: 100.3090 loss_val: 89.9949\n",
      "Epoch: 240 loss_train: 100.5951 loss_val: 90.8070\n",
      "Epoch: 241 loss_train: 101.3344 loss_val: 91.1955\n",
      "Epoch: 242 loss_train: 100.7491 loss_val: 89.5117\n",
      "Epoch: 243 loss_train: 99.0743 loss_val: 88.9550\n",
      "Epoch: 244 loss_train: 100.7423 loss_val: 90.4870\n",
      "Epoch: 245 loss_train: 100.0979 loss_val: 90.9136\n",
      "Epoch: 246 loss_train: 99.8546 loss_val: 89.8270\n",
      "Epoch: 247 loss_train: 99.5703 loss_val: 89.0508\n",
      "Epoch: 248 loss_train: 99.0948 loss_val: 89.2328\n",
      "Epoch: 249 loss_train: 99.2843 loss_val: 88.9763\n",
      "Epoch: 250 loss_train: 100.7478 loss_val: 88.6668\n",
      "Epoch: 251 loss_train: 99.4514 loss_val: 88.9582\n",
      "Epoch: 252 loss_train: 98.8315 loss_val: 89.1034\n",
      "Epoch: 253 loss_train: 98.8679 loss_val: 89.0036\n",
      "Epoch: 254 loss_train: 99.6000 loss_val: 88.7909\n",
      "Epoch: 255 loss_train: 98.4082 loss_val: 88.4689\n",
      "Epoch: 256 loss_train: 98.4611 loss_val: 88.6919\n",
      "Epoch: 257 loss_train: 98.0237 loss_val: 88.4835\n",
      "Epoch: 258 loss_train: 98.3486 loss_val: 87.7938\n",
      "Epoch: 259 loss_train: 98.4507 loss_val: 87.7585\n",
      "Epoch: 260 loss_train: 98.3695 loss_val: 88.0219\n",
      "Epoch: 261 loss_train: 98.3725 loss_val: 87.9055\n",
      "Epoch: 262 loss_train: 98.1176 loss_val: 87.8990\n",
      "Epoch: 263 loss_train: 98.0262 loss_val: 88.1811\n",
      "Epoch: 264 loss_train: 97.8765 loss_val: 88.2248\n",
      "Epoch: 265 loss_train: 97.9219 loss_val: 87.8582\n",
      "Epoch: 266 loss_train: 97.3739 loss_val: 87.2410\n",
      "Epoch: 267 loss_train: 96.8946 loss_val: 87.0094\n",
      "Epoch: 268 loss_train: 97.4496 loss_val: 87.7466\n",
      "Epoch: 269 loss_train: 97.0772 loss_val: 87.4985\n",
      "Epoch: 270 loss_train: 96.7735 loss_val: 86.9915\n",
      "Epoch: 271 loss_train: 96.8205 loss_val: 86.8730\n",
      "Epoch: 272 loss_train: 96.7242 loss_val: 87.6809\n",
      "Epoch: 273 loss_train: 96.6789 loss_val: 87.6405\n",
      "Epoch: 274 loss_train: 96.5523 loss_val: 86.7573\n",
      "Epoch: 275 loss_train: 95.6950 loss_val: 86.4831\n",
      "Epoch: 276 loss_train: 96.2485 loss_val: 86.9687\n",
      "Epoch: 277 loss_train: 96.2700 loss_val: 86.7382\n",
      "Epoch: 278 loss_train: 96.0233 loss_val: 86.4179\n",
      "Epoch: 279 loss_train: 96.6803 loss_val: 86.7765\n",
      "Epoch: 280 loss_train: 96.2273 loss_val: 86.7504\n",
      "Epoch: 281 loss_train: 95.7828 loss_val: 86.3424\n",
      "Epoch: 282 loss_train: 95.5193 loss_val: 86.2228\n",
      "Epoch: 283 loss_train: 95.4422 loss_val: 86.6091\n",
      "Epoch: 284 loss_train: 95.4578 loss_val: 85.9900\n",
      "Epoch: 285 loss_train: 96.3219 loss_val: 86.2639\n",
      "Epoch: 286 loss_train: 96.0598 loss_val: 86.8340\n",
      "Epoch: 287 loss_train: 96.0820 loss_val: 86.1239\n",
      "Epoch: 288 loss_train: 95.3274 loss_val: 85.8343\n",
      "Epoch: 289 loss_train: 95.1571 loss_val: 86.3195\n",
      "Epoch: 290 loss_train: 95.2632 loss_val: 86.5678\n",
      "Epoch: 291 loss_train: 95.1777 loss_val: 85.8120\n",
      "Epoch: 292 loss_train: 94.4011 loss_val: 85.3521\n",
      "Epoch: 293 loss_train: 95.8140 loss_val: 86.8215\n",
      "Epoch: 294 loss_train: 94.9790 loss_val: 86.1319\n",
      "Epoch: 295 loss_train: 94.8716 loss_val: 85.0770\n",
      "Epoch: 296 loss_train: 95.9923 loss_val: 86.8970\n",
      "Epoch: 297 loss_train: 95.7205 loss_val: 87.1750\n",
      "Epoch: 298 loss_train: 94.9512 loss_val: 85.0608\n",
      "Epoch: 299 loss_train: 94.8214 loss_val: 85.3064\n",
      "Epoch: 300 loss_train: 95.0683 loss_val: 86.8767\n",
      "Epoch: 301 loss_train: 94.2208 loss_val: 85.8240\n",
      "Epoch: 302 loss_train: 94.9724 loss_val: 84.7751\n",
      "Epoch: 303 loss_train: 94.5923 loss_val: 85.8551\n",
      "Epoch: 304 loss_train: 94.0320 loss_val: 86.4709\n",
      "Epoch: 305 loss_train: 94.7164 loss_val: 84.7488\n",
      "Epoch: 306 loss_train: 93.3651 loss_val: 84.5108\n",
      "Epoch: 307 loss_train: 94.2900 loss_val: 85.8853\n",
      "Epoch: 308 loss_train: 94.0118 loss_val: 85.9155\n",
      "Epoch: 309 loss_train: 94.0145 loss_val: 84.6540\n",
      "Epoch: 310 loss_train: 94.1735 loss_val: 85.0155\n",
      "Epoch: 311 loss_train: 93.3010 loss_val: 85.1971\n",
      "Epoch: 312 loss_train: 93.7161 loss_val: 85.0232\n",
      "Epoch: 313 loss_train: 93.6584 loss_val: 84.9458\n",
      "Epoch: 314 loss_train: 94.1281 loss_val: 85.6875\n",
      "Epoch: 315 loss_train: 93.5603 loss_val: 84.8375\n",
      "Epoch: 316 loss_train: 92.8284 loss_val: 84.7700\n",
      "Epoch: 317 loss_train: 94.0669 loss_val: 85.6140\n",
      "Epoch: 318 loss_train: 93.5364 loss_val: 85.0748\n",
      "Epoch: 319 loss_train: 92.9845 loss_val: 84.1275\n",
      "Epoch: 320 loss_train: 93.0013 loss_val: 85.0195\n",
      "Epoch: 321 loss_train: 93.0486 loss_val: 85.3262\n",
      "Epoch: 322 loss_train: 92.6514 loss_val: 84.2686\n",
      "Epoch: 323 loss_train: 93.4648 loss_val: 84.8586\n",
      "Epoch: 324 loss_train: 93.1514 loss_val: 85.2621\n",
      "Epoch: 325 loss_train: 92.5608 loss_val: 84.2598\n",
      "Epoch: 326 loss_train: 93.0277 loss_val: 84.4161\n",
      "Epoch: 327 loss_train: 92.9484 loss_val: 85.3538\n",
      "Epoch: 328 loss_train: 92.6712 loss_val: 84.0320\n",
      "Epoch: 329 loss_train: 92.4801 loss_val: 84.6321\n",
      "Epoch: 330 loss_train: 92.9252 loss_val: 85.6075\n",
      "Epoch: 331 loss_train: 92.6681 loss_val: 83.9595\n",
      "Epoch: 332 loss_train: 92.2950 loss_val: 83.8811\n",
      "Epoch: 333 loss_train: 92.5187 loss_val: 85.3508\n",
      "Epoch: 334 loss_train: 91.9652 loss_val: 84.0027\n",
      "Epoch: 335 loss_train: 91.9698 loss_val: 83.0973\n",
      "Epoch: 336 loss_train: 92.5269 loss_val: 86.0106\n",
      "Epoch: 337 loss_train: 92.1888 loss_val: 85.0940\n",
      "Epoch: 338 loss_train: 92.1933 loss_val: 82.9577\n",
      "Epoch: 339 loss_train: 91.9163 loss_val: 84.7471\n",
      "Epoch: 340 loss_train: 92.2239 loss_val: 84.6633\n",
      "Epoch: 341 loss_train: 91.7489 loss_val: 83.4127\n",
      "Epoch: 342 loss_train: 92.8426 loss_val: 85.1760\n",
      "Epoch: 343 loss_train: 91.8006 loss_val: 84.4999\n",
      "Epoch: 344 loss_train: 91.5898 loss_val: 83.4441\n",
      "Epoch: 345 loss_train: 93.2197 loss_val: 86.9172\n",
      "Epoch: 346 loss_train: 93.0985 loss_val: 85.4019\n",
      "Epoch: 347 loss_train: 92.3727 loss_val: 81.9702\n",
      "Epoch: 348 loss_train: 93.7351 loss_val: 86.0629\n",
      "Epoch: 349 loss_train: 92.1718 loss_val: 85.4455\n",
      "Epoch: 350 loss_train: 91.5826 loss_val: 83.4511\n",
      "Epoch: 351 loss_train: 92.6762 loss_val: 86.1045\n",
      "Epoch: 352 loss_train: 91.3941 loss_val: 84.6317\n",
      "Epoch: 353 loss_train: 90.8448 loss_val: 82.9583\n",
      "Epoch: 354 loss_train: 92.6567 loss_val: 84.5656\n",
      "Epoch: 355 loss_train: 91.3859 loss_val: 86.0303\n",
      "Epoch: 356 loss_train: 91.3619 loss_val: 82.8234\n",
      "Epoch: 357 loss_train: 92.4657 loss_val: 83.9333\n",
      "Epoch: 358 loss_train: 90.2695 loss_val: 84.4797\n",
      "Epoch: 359 loss_train: 90.9060 loss_val: 82.6443\n",
      "Epoch: 360 loss_train: 91.3805 loss_val: 84.3242\n",
      "Epoch: 361 loss_train: 91.4406 loss_val: 84.0599\n",
      "Epoch: 362 loss_train: 90.9682 loss_val: 82.5577\n",
      "Epoch: 363 loss_train: 90.5338 loss_val: 82.4756\n",
      "Epoch: 364 loss_train: 90.0613 loss_val: 83.9935\n",
      "Epoch: 365 loss_train: 91.1462 loss_val: 83.9393\n",
      "Epoch: 366 loss_train: 90.8355 loss_val: 82.6545\n",
      "Epoch: 367 loss_train: 90.0132 loss_val: 84.0362\n",
      "Epoch: 368 loss_train: 90.3237 loss_val: 82.9297\n",
      "Epoch: 369 loss_train: 90.0182 loss_val: 82.1400\n",
      "Epoch: 370 loss_train: 89.1133 loss_val: 82.9553\n",
      "Epoch: 371 loss_train: 89.4286 loss_val: 83.5003\n",
      "Epoch: 372 loss_train: 89.9122 loss_val: 82.1612\n",
      "Epoch: 373 loss_train: 89.5016 loss_val: 83.2039\n",
      "Epoch: 374 loss_train: 89.4461 loss_val: 82.9832\n",
      "Epoch: 375 loss_train: 89.6811 loss_val: 82.3986\n",
      "Epoch: 376 loss_train: 90.3622 loss_val: 83.0403\n",
      "Epoch: 377 loss_train: 89.3813 loss_val: 82.6894\n",
      "Epoch: 378 loss_train: 89.4584 loss_val: 82.4591\n",
      "Epoch: 379 loss_train: 89.8789 loss_val: 83.6248\n",
      "Epoch: 380 loss_train: 89.3744 loss_val: 81.9515\n",
      "Epoch: 381 loss_train: 88.9060 loss_val: 82.6458\n",
      "Epoch: 382 loss_train: 89.1258 loss_val: 82.5984\n",
      "Epoch: 383 loss_train: 89.1164 loss_val: 81.5000\n",
      "Epoch: 384 loss_train: 89.3194 loss_val: 82.5329\n",
      "Epoch: 385 loss_train: 88.6019 loss_val: 82.8696\n",
      "Epoch: 386 loss_train: 89.9060 loss_val: 82.4039\n",
      "Epoch: 387 loss_train: 88.9935 loss_val: 82.3484\n",
      "Epoch: 388 loss_train: 88.7124 loss_val: 83.4606\n",
      "Epoch: 389 loss_train: 88.5582 loss_val: 82.2438\n",
      "Epoch: 390 loss_train: 89.3909 loss_val: 82.4880\n",
      "Epoch: 391 loss_train: 88.4791 loss_val: 82.1663\n",
      "Epoch: 392 loss_train: 88.8874 loss_val: 81.9459\n",
      "Epoch: 393 loss_train: 89.1585 loss_val: 83.4490\n",
      "Epoch: 394 loss_train: 88.8814 loss_val: 81.4098\n",
      "Epoch: 395 loss_train: 88.3104 loss_val: 81.4066\n",
      "Epoch: 396 loss_train: 88.2926 loss_val: 84.1757\n",
      "Epoch: 397 loss_train: 88.6670 loss_val: 81.1371\n",
      "Epoch: 398 loss_train: 88.3569 loss_val: 81.9532\n",
      "Epoch: 399 loss_train: 88.2193 loss_val: 82.4064\n",
      "Epoch: 400 loss_train: 88.3124 loss_val: 81.5035\n",
      "Epoch: 401 loss_train: 87.7664 loss_val: 82.3211\n",
      "Epoch: 402 loss_train: 87.7114 loss_val: 82.3579\n",
      "Epoch: 403 loss_train: 87.8128 loss_val: 81.6852\n",
      "Epoch: 404 loss_train: 88.3917 loss_val: 82.1919\n",
      "Epoch: 405 loss_train: 87.3524 loss_val: 80.9897\n",
      "Epoch: 406 loss_train: 88.9157 loss_val: 82.8419\n",
      "Epoch: 407 loss_train: 88.0633 loss_val: 80.5750\n",
      "Epoch: 408 loss_train: 87.9961 loss_val: 83.4690\n",
      "Epoch: 409 loss_train: 88.6349 loss_val: 82.2014\n",
      "Epoch: 410 loss_train: 87.4323 loss_val: 80.2626\n",
      "Epoch: 411 loss_train: 88.6134 loss_val: 85.4232\n",
      "Epoch: 412 loss_train: 88.7261 loss_val: 82.8945\n",
      "Epoch: 413 loss_train: 87.3939 loss_val: 79.4853\n",
      "Epoch: 414 loss_train: 89.5844 loss_val: 90.2231\n",
      "Epoch: 415 loss_train: 92.1255 loss_val: 83.5010\n",
      "Epoch: 416 loss_train: 88.3548 loss_val: 81.2847\n",
      "Epoch: 417 loss_train: 93.1441 loss_val: 98.5370\n",
      "Epoch: 418 loss_train: 98.9394 loss_val: 93.4429\n",
      "Epoch: 419 loss_train: 95.6496 loss_val: 84.3052\n",
      "Epoch: 420 loss_train: 95.4491 loss_val: 85.0154\n",
      "Epoch: 421 loss_train: 96.9469 loss_val: 87.9281\n",
      "Epoch: 422 loss_train: 91.3388 loss_val: 91.6925\n",
      "Epoch: 423 loss_train: 93.3053 loss_val: 82.9730\n",
      "Epoch: 424 loss_train: 89.9960 loss_val: 79.7580\n",
      "Epoch: 425 loss_train: 90.0236 loss_val: 90.5789\n",
      "Epoch: 426 loss_train: 93.9851 loss_val: 81.8256\n",
      "Epoch: 427 loss_train: 88.6894 loss_val: 83.3135\n",
      "Epoch: 428 loss_train: 92.1900 loss_val: 88.1533\n",
      "Epoch: 429 loss_train: 92.1611 loss_val: 89.7998\n",
      "Epoch: 430 loss_train: 93.3046 loss_val: 84.3082\n",
      "Epoch: 431 loss_train: 92.3640 loss_val: 85.1488\n",
      "Epoch: 432 loss_train: 94.1396 loss_val: 88.5903\n",
      "Epoch: 433 loss_train: 92.2181 loss_val: 87.5677\n",
      "Epoch: 434 loss_train: 91.4088 loss_val: 82.2083\n",
      "Epoch: 435 loss_train: 90.1130 loss_val: 80.3404\n",
      "Epoch: 436 loss_train: 89.4435 loss_val: 88.7766\n",
      "Epoch: 437 loss_train: 91.8427 loss_val: 89.4662\n",
      "Epoch: 438 loss_train: 91.4413 loss_val: 80.9793\n",
      "Epoch: 439 loss_train: 89.2545 loss_val: 79.7675\n",
      "Epoch: 440 loss_train: 91.2100 loss_val: 85.6011\n",
      "Epoch: 441 loss_train: 89.8422 loss_val: 84.9038\n",
      "Epoch: 442 loss_train: 88.3186 loss_val: 84.9030\n",
      "Epoch: 443 loss_train: 92.6101 loss_val: 82.6836\n",
      "Epoch: 444 loss_train: 88.0627 loss_val: 84.5110\n",
      "Epoch: 445 loss_train: 90.6411 loss_val: 83.7623\n",
      "Epoch: 446 loss_train: 89.8075 loss_val: 85.6644\n",
      "Epoch: 447 loss_train: 90.9433 loss_val: 87.0629\n",
      "Epoch: 448 loss_train: 91.1354 loss_val: 86.0495\n",
      "Epoch: 449 loss_train: 89.7963 loss_val: 82.0944\n",
      "Epoch: 450 loss_train: 89.4031 loss_val: 82.1225\n",
      "Epoch: 451 loss_train: 87.5206 loss_val: 84.4154\n",
      "Epoch: 452 loss_train: 87.4859 loss_val: 82.4152\n",
      "Epoch: 453 loss_train: 87.5066 loss_val: 79.1420\n",
      "Epoch: 454 loss_train: 87.0375 loss_val: 80.2629\n",
      "Epoch: 455 loss_train: 86.2419 loss_val: 80.8326\n",
      "Epoch: 456 loss_train: 87.0481 loss_val: 81.1335\n",
      "Epoch: 457 loss_train: 86.6874 loss_val: 80.7425\n",
      "Epoch: 458 loss_train: 86.6769 loss_val: 81.0535\n",
      "Epoch: 459 loss_train: 86.5935 loss_val: 82.3986\n",
      "Epoch: 460 loss_train: 86.1390 loss_val: 82.2991\n",
      "Epoch: 461 loss_train: 86.8721 loss_val: 80.9344\n",
      "Epoch: 462 loss_train: 86.0634 loss_val: 81.0314\n",
      "Epoch: 463 loss_train: 86.8455 loss_val: 81.0671\n",
      "Epoch: 464 loss_train: 86.7929 loss_val: 81.5494\n",
      "Epoch: 465 loss_train: 85.8108 loss_val: 80.4137\n",
      "Epoch: 466 loss_train: 85.7049 loss_val: 79.3583\n",
      "Epoch: 467 loss_train: 86.1395 loss_val: 80.6553\n",
      "Epoch: 468 loss_train: 85.6189 loss_val: 80.7067\n",
      "Epoch: 469 loss_train: 85.1470 loss_val: 79.2476\n",
      "Epoch: 470 loss_train: 85.8233 loss_val: 79.7922\n",
      "Epoch: 471 loss_train: 85.3147 loss_val: 80.3118\n",
      "Epoch: 472 loss_train: 85.2510 loss_val: 80.8400\n",
      "Epoch: 473 loss_train: 84.8197 loss_val: 80.3462\n",
      "Epoch: 474 loss_train: 85.1571 loss_val: 79.6973\n",
      "Epoch: 475 loss_train: 84.4675 loss_val: 80.1161\n",
      "Epoch: 476 loss_train: 84.6806 loss_val: 80.1077\n",
      "Epoch: 477 loss_train: 85.2269 loss_val: 80.2821\n",
      "Epoch: 478 loss_train: 84.7550 loss_val: 78.7354\n",
      "Epoch: 479 loss_train: 84.8865 loss_val: 80.9951\n",
      "Epoch: 480 loss_train: 84.6348 loss_val: 80.1772\n",
      "Epoch: 481 loss_train: 84.2536 loss_val: 78.3218\n",
      "Epoch: 482 loss_train: 85.0067 loss_val: 82.7599\n",
      "Epoch: 483 loss_train: 85.7678 loss_val: 80.0259\n",
      "Epoch: 484 loss_train: 84.7662 loss_val: 78.3926\n",
      "Epoch: 485 loss_train: 85.5638 loss_val: 83.2075\n",
      "Epoch: 486 loss_train: 86.0657 loss_val: 79.6518\n",
      "Epoch: 487 loss_train: 84.0275 loss_val: 79.5575\n",
      "Epoch: 488 loss_train: 85.6154 loss_val: 88.1540\n",
      "Epoch: 489 loss_train: 88.4953 loss_val: 80.3523\n",
      "Epoch: 490 loss_train: 84.8232 loss_val: 81.7141\n",
      "Epoch: 491 loss_train: 92.7815 loss_val: 97.4079\n",
      "Epoch: 492 loss_train: 96.0321 loss_val: 93.6162\n",
      "Epoch: 493 loss_train: 93.8764 loss_val: 82.5312\n",
      "Epoch: 494 loss_train: 91.4066 loss_val: 84.9181\n",
      "Epoch: 495 loss_train: 93.1186 loss_val: 90.1153\n",
      "Epoch: 496 loss_train: 91.7033 loss_val: 93.5407\n",
      "Epoch: 497 loss_train: 93.6165 loss_val: 82.0539\n",
      "Epoch: 498 loss_train: 86.8905 loss_val: 81.1652\n",
      "Epoch: 499 loss_train: 96.1928 loss_val: 89.9466\n",
      "Epoch: 500 loss_train: 90.2787 loss_val: 91.3004\n",
      "Epoch: 501 loss_train: 90.7557 loss_val: 85.9446\n",
      "Epoch: 502 loss_train: 96.2391 loss_val: 94.9762\n",
      "Epoch: 503 loss_train: 96.3506 loss_val: 93.7325\n",
      "Epoch: 504 loss_train: 98.2990 loss_val: 84.5555\n",
      "Epoch: 505 loss_train: 94.6643 loss_val: 86.6914\n",
      "Epoch: 506 loss_train: 95.5245 loss_val: 91.2733\n",
      "Epoch: 507 loss_train: 95.5986 loss_val: 95.2371\n",
      "Epoch: 508 loss_train: 97.3511 loss_val: 92.5329\n",
      "Epoch: 509 loss_train: 95.3340 loss_val: 86.5268\n",
      "Epoch: 510 loss_train: 92.7368 loss_val: 83.8315\n",
      "Epoch: 511 loss_train: 95.3296 loss_val: 83.0290\n",
      "Epoch: 512 loss_train: 90.7510 loss_val: 88.4439\n",
      "Epoch: 513 loss_train: 90.7261 loss_val: 90.8855\n",
      "Epoch: 514 loss_train: 92.9943 loss_val: 85.6800\n",
      "Epoch: 515 loss_train: 91.6643 loss_val: 89.0882\n",
      "Epoch: 516 loss_train: 91.7655 loss_val: 81.5009\n",
      "Epoch: 517 loss_train: 89.7925 loss_val: 81.4089\n",
      "Epoch: 518 loss_train: 94.6832 loss_val: 87.5785\n",
      "Epoch: 519 loss_train: 91.0848 loss_val: 93.8251\n",
      "Epoch: 520 loss_train: 94.2322 loss_val: 89.7521\n",
      "Epoch: 521 loss_train: 92.6129 loss_val: 86.5810\n",
      "Epoch: 522 loss_train: 92.8709 loss_val: 82.3646\n",
      "Epoch: 523 loss_train: 90.2791 loss_val: 83.8747\n",
      "Epoch: 524 loss_train: 90.8379 loss_val: 84.8257\n",
      "Epoch: 525 loss_train: 89.3560 loss_val: 82.7672\n",
      "Epoch: 526 loss_train: 86.8975 loss_val: 84.9029\n",
      "Epoch: 527 loss_train: 89.4671 loss_val: 90.9342\n",
      "Epoch: 528 loss_train: 90.6483 loss_val: 85.9341\n",
      "Epoch: 529 loss_train: 87.7996 loss_val: 80.7950\n",
      "Epoch: 530 loss_train: 92.0539 loss_val: 84.2408\n",
      "Epoch: 531 loss_train: 88.5504 loss_val: 86.5543\n",
      "Epoch: 532 loss_train: 88.6092 loss_val: 82.7831\n",
      "Epoch: 533 loss_train: 86.3702 loss_val: 82.7094\n",
      "Epoch: 534 loss_train: 88.2620 loss_val: 82.5372\n",
      "Epoch: 535 loss_train: 85.9936 loss_val: 83.4744\n",
      "Epoch: 536 loss_train: 88.1958 loss_val: 81.5964\n",
      "Epoch: 537 loss_train: 86.8435 loss_val: 80.8458\n",
      "Epoch: 538 loss_train: 86.3691 loss_val: 82.6920\n",
      "Epoch: 539 loss_train: 86.0358 loss_val: 84.3689\n",
      "Epoch: 540 loss_train: 85.5809 loss_val: 81.2894\n",
      "Epoch: 541 loss_train: 84.3034 loss_val: 78.1156\n",
      "Epoch: 542 loss_train: 87.2681 loss_val: 82.4619\n",
      "Epoch: 543 loss_train: 85.0933 loss_val: 83.3477\n",
      "Epoch: 544 loss_train: 85.3422 loss_val: 80.3559\n",
      "Epoch: 545 loss_train: 85.6189 loss_val: 80.8726\n",
      "Epoch: 546 loss_train: 85.3758 loss_val: 81.3631\n",
      "Epoch: 547 loss_train: 85.4489 loss_val: 80.2641\n",
      "Epoch: 548 loss_train: 84.5182 loss_val: 81.8915\n",
      "Epoch: 549 loss_train: 85.1108 loss_val: 82.4017\n",
      "Epoch: 550 loss_train: 85.4466 loss_val: 81.6563\n",
      "Epoch: 551 loss_train: 83.1661 loss_val: 79.0208\n",
      "Epoch: 552 loss_train: 84.4291 loss_val: 78.4343\n",
      "Epoch: 553 loss_train: 83.6286 loss_val: 80.8413\n",
      "Epoch: 554 loss_train: 84.9620 loss_val: 84.1464\n",
      "Epoch: 555 loss_train: 85.3372 loss_val: 79.3991\n",
      "Epoch: 556 loss_train: 83.2789 loss_val: 78.0677\n",
      "Epoch: 557 loss_train: 83.6211 loss_val: 81.7878\n",
      "Epoch: 558 loss_train: 84.2676 loss_val: 81.0816\n",
      "Epoch: 559 loss_train: 82.8394 loss_val: 80.4348\n",
      "Epoch: 560 loss_train: 84.2431 loss_val: 81.3481\n",
      "Epoch: 561 loss_train: 83.7230 loss_val: 81.6614\n",
      "Epoch: 562 loss_train: 83.8766 loss_val: 78.9712\n",
      "Epoch: 563 loss_train: 83.0536 loss_val: 79.9792\n",
      "Epoch: 564 loss_train: 82.9725 loss_val: 81.8116\n",
      "Epoch: 565 loss_train: 82.8108 loss_val: 79.5363\n",
      "Epoch: 566 loss_train: 82.4805 loss_val: 78.2846\n",
      "Epoch: 567 loss_train: 84.4439 loss_val: 85.0659\n",
      "Epoch: 568 loss_train: 84.8907 loss_val: 79.0522\n",
      "Epoch: 569 loss_train: 82.8756 loss_val: 80.6005\n",
      "Epoch: 570 loss_train: 85.4390 loss_val: 87.8590\n",
      "Epoch: 571 loss_train: 87.8295 loss_val: 81.6967\n",
      "Epoch: 572 loss_train: 84.2190 loss_val: 80.3563\n",
      "Epoch: 573 loss_train: 87.5245 loss_val: 82.4038\n",
      "Epoch: 574 loss_train: 83.8586 loss_val: 86.3458\n",
      "Epoch: 575 loss_train: 86.0227 loss_val: 80.2945\n",
      "Epoch: 576 loss_train: 83.5888 loss_val: 78.0317\n",
      "Epoch: 577 loss_train: 86.0208 loss_val: 81.7835\n",
      "Epoch: 578 loss_train: 83.4747 loss_val: 83.5592\n",
      "Epoch: 579 loss_train: 84.1056 loss_val: 79.4503\n",
      "Epoch: 580 loss_train: 84.0144 loss_val: 78.3362\n",
      "Epoch: 581 loss_train: 82.8399 loss_val: 81.5408\n",
      "Epoch: 582 loss_train: 83.8180 loss_val: 78.9065\n",
      "Epoch: 583 loss_train: 82.7129 loss_val: 79.0869\n",
      "Epoch: 584 loss_train: 82.8356 loss_val: 81.2094\n",
      "Epoch: 585 loss_train: 82.9620 loss_val: 77.8368\n",
      "Epoch: 586 loss_train: 82.8379 loss_val: 77.2865\n",
      "Epoch: 587 loss_train: 83.5412 loss_val: 82.3183\n",
      "Epoch: 588 loss_train: 83.1619 loss_val: 81.1811\n",
      "Epoch: 589 loss_train: 82.8141 loss_val: 78.3531\n",
      "Epoch: 590 loss_train: 83.7604 loss_val: 78.4974\n",
      "Epoch: 591 loss_train: 82.2686 loss_val: 81.4058\n",
      "Epoch: 592 loss_train: 82.7622 loss_val: 78.8006\n",
      "Epoch: 593 loss_train: 81.2788 loss_val: 78.4421\n",
      "Epoch: 594 loss_train: 82.2614 loss_val: 81.3511\n",
      "Epoch: 595 loss_train: 82.4604 loss_val: 77.9829\n",
      "Epoch: 596 loss_train: 81.0286 loss_val: 77.8096\n",
      "Epoch: 597 loss_train: 82.1014 loss_val: 82.7553\n",
      "Epoch: 598 loss_train: 82.1574 loss_val: 79.0820\n",
      "Epoch: 599 loss_train: 80.8122 loss_val: 76.7108\n",
      "Epoch: 600 loss_train: 82.7836 loss_val: 82.3809\n",
      "Epoch: 601 loss_train: 82.8899 loss_val: 80.2248\n",
      "Epoch: 602 loss_train: 80.9183 loss_val: 79.8813\n",
      "Epoch: 603 loss_train: 84.6038 loss_val: 82.7875\n",
      "Epoch: 604 loss_train: 83.1588 loss_val: 82.6061\n",
      "Epoch: 605 loss_train: 83.9113 loss_val: 78.4144\n",
      "Epoch: 606 loss_train: 83.0265 loss_val: 80.2944\n",
      "Epoch: 607 loss_train: 82.9941 loss_val: 81.8648\n",
      "Epoch: 608 loss_train: 82.1123 loss_val: 80.8796\n",
      "Epoch: 609 loss_train: 81.7710 loss_val: 77.0426\n",
      "Epoch: 610 loss_train: 81.4993 loss_val: 77.7504\n",
      "Epoch: 611 loss_train: 81.2808 loss_val: 80.6491\n",
      "Epoch: 612 loss_train: 81.3031 loss_val: 77.0018\n",
      "Epoch: 613 loss_train: 81.9993 loss_val: 78.7303\n",
      "Epoch: 614 loss_train: 82.0375 loss_val: 77.2537\n",
      "Epoch: 615 loss_train: 80.6576 loss_val: 77.4188\n",
      "Epoch: 616 loss_train: 80.6761 loss_val: 80.0004\n",
      "Epoch: 617 loss_train: 81.1983 loss_val: 77.8783\n",
      "Epoch: 618 loss_train: 80.4500 loss_val: 77.2230\n",
      "Epoch: 619 loss_train: 82.5470 loss_val: 79.9613\n",
      "Epoch: 620 loss_train: 81.2837 loss_val: 79.7563\n",
      "Epoch: 621 loss_train: 80.6292 loss_val: 77.0888\n",
      "Epoch: 622 loss_train: 80.8338 loss_val: 78.1013\n",
      "Epoch: 623 loss_train: 79.9680 loss_val: 80.0163\n",
      "Epoch: 624 loss_train: 80.4936 loss_val: 77.2632\n",
      "Epoch: 625 loss_train: 80.2408 loss_val: 77.4477\n",
      "Epoch: 626 loss_train: 79.8785 loss_val: 78.3589\n",
      "Epoch: 627 loss_train: 80.0919 loss_val: 76.7820\n",
      "Epoch: 628 loss_train: 79.7481 loss_val: 76.9279\n",
      "Epoch: 629 loss_train: 81.5285 loss_val: 80.7392\n",
      "Epoch: 630 loss_train: 81.4031 loss_val: 77.4045\n",
      "Epoch: 631 loss_train: 81.2170 loss_val: 79.2907\n",
      "Epoch: 632 loss_train: 80.7916 loss_val: 80.0214\n",
      "Epoch: 633 loss_train: 81.5736 loss_val: 79.6946\n",
      "Epoch: 634 loss_train: 81.0668 loss_val: 77.1666\n",
      "Epoch: 635 loss_train: 80.7283 loss_val: 77.7047\n",
      "Epoch: 636 loss_train: 80.6424 loss_val: 78.3563\n",
      "Epoch: 637 loss_train: 80.5578 loss_val: 76.2790\n",
      "Epoch: 638 loss_train: 81.6652 loss_val: 83.7374\n",
      "Epoch: 639 loss_train: 83.0878 loss_val: 77.9557\n",
      "Epoch: 640 loss_train: 80.8070 loss_val: 75.2212\n",
      "Epoch: 641 loss_train: 82.7114 loss_val: 80.8035\n",
      "Epoch: 642 loss_train: 82.6361 loss_val: 80.6996\n",
      "Epoch: 643 loss_train: 80.9900 loss_val: 78.3594\n",
      "Epoch: 644 loss_train: 84.3802 loss_val: 82.9280\n",
      "Epoch: 645 loss_train: 82.8898 loss_val: 81.3179\n",
      "Epoch: 646 loss_train: 83.5760 loss_val: 77.1689\n",
      "Epoch: 647 loss_train: 82.1440 loss_val: 79.3788\n",
      "Epoch: 648 loss_train: 83.0577 loss_val: 82.5797\n",
      "Epoch: 649 loss_train: 81.8548 loss_val: 79.7554\n",
      "Epoch: 650 loss_train: 81.3169 loss_val: 77.2485\n",
      "Epoch: 651 loss_train: 84.2559 loss_val: 83.3155\n",
      "Epoch: 652 loss_train: 82.7184 loss_val: 82.6184\n",
      "Epoch: 653 loss_train: 81.8886 loss_val: 77.9524\n",
      "Epoch: 654 loss_train: 84.4478 loss_val: 79.3157\n",
      "Epoch: 655 loss_train: 81.1253 loss_val: 80.4062\n",
      "Epoch: 656 loss_train: 81.0711 loss_val: 76.8718\n",
      "Epoch: 657 loss_train: 81.9108 loss_val: 78.9968\n",
      "Epoch: 658 loss_train: 80.2418 loss_val: 78.4877\n",
      "Epoch: 659 loss_train: 80.0895 loss_val: 75.7594\n",
      "Epoch: 660 loss_train: 80.3449 loss_val: 79.0922\n",
      "Epoch: 661 loss_train: 80.4363 loss_val: 79.6466\n",
      "Epoch: 662 loss_train: 80.0089 loss_val: 76.7365\n",
      "Epoch: 663 loss_train: 80.0487 loss_val: 76.8639\n",
      "Epoch: 664 loss_train: 80.0751 loss_val: 79.4113\n",
      "Epoch: 665 loss_train: 79.8402 loss_val: 77.4275\n",
      "Epoch: 666 loss_train: 79.5706 loss_val: 75.6404\n",
      "Epoch: 667 loss_train: 79.4141 loss_val: 78.7981\n",
      "Epoch: 668 loss_train: 79.3261 loss_val: 77.1492\n",
      "Epoch: 669 loss_train: 78.7529 loss_val: 76.0137\n",
      "Epoch: 670 loss_train: 79.1500 loss_val: 79.2084\n",
      "Epoch: 671 loss_train: 79.8995 loss_val: 77.2489\n",
      "Epoch: 672 loss_train: 79.3663 loss_val: 76.7588\n",
      "Epoch: 673 loss_train: 80.2295 loss_val: 79.9455\n",
      "Epoch: 674 loss_train: 79.8996 loss_val: 78.0429\n",
      "Epoch: 675 loss_train: 79.3800 loss_val: 75.4999\n",
      "Epoch: 676 loss_train: 79.8553 loss_val: 77.9260\n",
      "Epoch: 677 loss_train: 78.6291 loss_val: 79.0951\n",
      "Epoch: 678 loss_train: 78.8829 loss_val: 75.9068\n",
      "Epoch: 679 loss_train: 78.7397 loss_val: 76.0291\n",
      "Epoch: 680 loss_train: 79.1209 loss_val: 78.9419\n",
      "Epoch: 681 loss_train: 79.0581 loss_val: 76.8398\n",
      "Epoch: 682 loss_train: 78.8738 loss_val: 75.7348\n",
      "Epoch: 683 loss_train: 78.4274 loss_val: 77.5927\n",
      "Epoch: 684 loss_train: 78.7515 loss_val: 76.2020\n",
      "Epoch: 685 loss_train: 78.6938 loss_val: 76.8414\n",
      "Epoch: 686 loss_train: 77.7002 loss_val: 77.2193\n",
      "Epoch: 687 loss_train: 77.9855 loss_val: 76.3756\n",
      "Epoch: 688 loss_train: 78.1072 loss_val: 76.9037\n",
      "Epoch: 689 loss_train: 78.0245 loss_val: 76.9559\n",
      "Epoch: 690 loss_train: 78.7189 loss_val: 76.4022\n",
      "Epoch: 691 loss_train: 77.9024 loss_val: 76.1719\n",
      "Epoch: 692 loss_train: 78.5113 loss_val: 77.9008\n",
      "Epoch: 693 loss_train: 78.0414 loss_val: 76.2628\n",
      "Epoch: 694 loss_train: 77.6596 loss_val: 76.4959\n",
      "Epoch: 695 loss_train: 77.9539 loss_val: 77.3924\n",
      "Epoch: 696 loss_train: 78.1559 loss_val: 76.0283\n",
      "Epoch: 697 loss_train: 77.2253 loss_val: 76.0619\n",
      "Epoch: 698 loss_train: 77.7677 loss_val: 77.3270\n",
      "Epoch: 699 loss_train: 77.2112 loss_val: 76.1490\n",
      "Epoch: 700 loss_train: 77.9228 loss_val: 75.5187\n",
      "Epoch: 701 loss_train: 77.5089 loss_val: 77.6284\n",
      "Epoch: 702 loss_train: 77.4217 loss_val: 76.2343\n",
      "Epoch: 703 loss_train: 78.1598 loss_val: 76.0156\n",
      "Epoch: 704 loss_train: 77.1310 loss_val: 76.1645\n",
      "Epoch: 705 loss_train: 77.9112 loss_val: 76.3399\n",
      "Epoch: 706 loss_train: 77.9289 loss_val: 77.3311\n",
      "Epoch: 707 loss_train: 77.8471 loss_val: 75.4115\n",
      "Epoch: 708 loss_train: 77.5389 loss_val: 76.5610\n",
      "Epoch: 709 loss_train: 77.3964 loss_val: 76.6555\n",
      "Epoch: 710 loss_train: 77.0439 loss_val: 75.6761\n",
      "Epoch: 711 loss_train: 77.4278 loss_val: 76.8098\n",
      "Epoch: 712 loss_train: 77.8557 loss_val: 76.5420\n",
      "Epoch: 713 loss_train: 77.2333 loss_val: 75.7041\n",
      "Epoch: 714 loss_train: 76.9294 loss_val: 77.4756\n",
      "Epoch: 715 loss_train: 76.7728 loss_val: 75.6781\n",
      "Epoch: 716 loss_train: 77.0935 loss_val: 75.3431\n",
      "Epoch: 717 loss_train: 76.9914 loss_val: 77.0571\n",
      "Epoch: 718 loss_train: 77.5649 loss_val: 75.7820\n",
      "Epoch: 719 loss_train: 76.7160 loss_val: 76.4379\n",
      "Epoch: 720 loss_train: 76.8171 loss_val: 76.1886\n",
      "Epoch: 721 loss_train: 77.2113 loss_val: 75.8661\n",
      "Epoch: 722 loss_train: 76.7974 loss_val: 76.3431\n",
      "Epoch: 723 loss_train: 76.3457 loss_val: 75.5437\n",
      "Epoch: 724 loss_train: 77.2794 loss_val: 76.2478\n",
      "Epoch: 725 loss_train: 76.6920 loss_val: 76.1344\n",
      "Epoch: 726 loss_train: 76.8875 loss_val: 75.9282\n",
      "Epoch: 727 loss_train: 76.8383 loss_val: 75.5959\n",
      "Epoch: 728 loss_train: 76.8581 loss_val: 77.0931\n",
      "Epoch: 729 loss_train: 76.6065 loss_val: 75.6196\n",
      "Epoch: 730 loss_train: 76.5868 loss_val: 75.2264\n",
      "Epoch: 731 loss_train: 77.2120 loss_val: 76.3240\n",
      "Epoch: 732 loss_train: 76.7368 loss_val: 75.8487\n",
      "Epoch: 733 loss_train: 76.2931 loss_val: 76.8210\n",
      "Epoch: 734 loss_train: 76.2832 loss_val: 74.8512\n",
      "Epoch: 735 loss_train: 76.2760 loss_val: 76.5132\n",
      "Epoch: 736 loss_train: 76.3513 loss_val: 76.0288\n",
      "Epoch: 737 loss_train: 76.4855 loss_val: 74.9661\n",
      "Epoch: 738 loss_train: 75.9506 loss_val: 76.3074\n",
      "Epoch: 739 loss_train: 76.8586 loss_val: 76.0256\n",
      "Epoch: 740 loss_train: 76.7972 loss_val: 75.5303\n",
      "Epoch: 741 loss_train: 76.6170 loss_val: 75.9793\n",
      "Epoch: 742 loss_train: 75.9609 loss_val: 75.2533\n",
      "Epoch: 743 loss_train: 75.9846 loss_val: 76.0916\n",
      "Epoch: 744 loss_train: 76.1318 loss_val: 74.9283\n",
      "Epoch: 745 loss_train: 76.5664 loss_val: 77.3195\n",
      "Epoch: 746 loss_train: 76.4470 loss_val: 75.6904\n",
      "Epoch: 747 loss_train: 76.3093 loss_val: 74.8676\n",
      "Epoch: 748 loss_train: 75.6937 loss_val: 76.0960\n",
      "Epoch: 749 loss_train: 76.1310 loss_val: 75.4911\n",
      "Epoch: 750 loss_train: 76.1959 loss_val: 77.5028\n",
      "Epoch: 751 loss_train: 76.8158 loss_val: 74.2102\n",
      "Epoch: 752 loss_train: 76.1702 loss_val: 77.9079\n",
      "Epoch: 753 loss_train: 77.0563 loss_val: 75.6383\n",
      "Epoch: 754 loss_train: 76.6466 loss_val: 74.3828\n",
      "Epoch: 755 loss_train: 75.6841 loss_val: 77.0563\n",
      "Epoch: 756 loss_train: 76.4345 loss_val: 75.3256\n",
      "Epoch: 757 loss_train: 75.8268 loss_val: 75.9432\n",
      "Epoch: 758 loss_train: 76.0572 loss_val: 75.2596\n",
      "Epoch: 759 loss_train: 75.9051 loss_val: 75.5739\n",
      "Epoch: 760 loss_train: 75.6936 loss_val: 76.8184\n",
      "Epoch: 761 loss_train: 75.8507 loss_val: 74.3328\n",
      "Epoch: 762 loss_train: 75.6101 loss_val: 76.3435\n",
      "Epoch: 763 loss_train: 75.8485 loss_val: 75.4453\n",
      "Epoch: 764 loss_train: 76.0630 loss_val: 76.1524\n",
      "Epoch: 765 loss_train: 76.1300 loss_val: 74.5423\n",
      "Epoch: 766 loss_train: 75.9472 loss_val: 77.4440\n",
      "Epoch: 767 loss_train: 76.2394 loss_val: 74.9057\n",
      "Epoch: 768 loss_train: 75.6187 loss_val: 75.1852\n",
      "Epoch: 769 loss_train: 75.5278 loss_val: 76.1740\n",
      "Epoch: 770 loss_train: 75.7148 loss_val: 74.6038\n",
      "Epoch: 771 loss_train: 75.0554 loss_val: 76.5424\n",
      "Epoch: 772 loss_train: 75.7649 loss_val: 73.8884\n",
      "Epoch: 773 loss_train: 75.6407 loss_val: 77.2408\n",
      "Epoch: 774 loss_train: 75.6164 loss_val: 75.0676\n",
      "Epoch: 775 loss_train: 75.4480 loss_val: 74.8739\n",
      "Epoch: 776 loss_train: 75.5787 loss_val: 76.2794\n",
      "Epoch: 777 loss_train: 75.5840 loss_val: 74.6853\n",
      "Epoch: 778 loss_train: 75.0696 loss_val: 76.3481\n",
      "Epoch: 779 loss_train: 75.4807 loss_val: 73.8986\n",
      "Epoch: 780 loss_train: 75.8592 loss_val: 78.6741\n",
      "Epoch: 781 loss_train: 76.4158 loss_val: 74.6319\n",
      "Epoch: 782 loss_train: 75.2802 loss_val: 74.4630\n",
      "Epoch: 783 loss_train: 75.1641 loss_val: 76.6392\n",
      "Epoch: 784 loss_train: 75.2992 loss_val: 75.3837\n",
      "Epoch: 785 loss_train: 75.1997 loss_val: 76.9844\n",
      "Epoch: 786 loss_train: 75.9143 loss_val: 73.7525\n",
      "Epoch: 787 loss_train: 75.4424 loss_val: 76.6625\n",
      "Epoch: 788 loss_train: 76.1153 loss_val: 77.5182\n",
      "Epoch: 789 loss_train: 75.8053 loss_val: 72.2604\n",
      "Epoch: 790 loss_train: 77.0506 loss_val: 82.4921\n",
      "Epoch: 791 loss_train: 78.2887 loss_val: 78.9569\n",
      "Epoch: 792 loss_train: 77.0169 loss_val: 73.6616\n",
      "Epoch: 793 loss_train: 75.0648 loss_val: 73.6405\n",
      "Epoch: 794 loss_train: 76.3621 loss_val: 78.4749\n",
      "Epoch: 795 loss_train: 76.0531 loss_val: 78.6275\n",
      "Epoch: 796 loss_train: 77.9791 loss_val: 78.4554\n",
      "Epoch: 797 loss_train: 76.5626 loss_val: 74.6346\n",
      "Epoch: 798 loss_train: 77.3583 loss_val: 73.5503\n",
      "Epoch: 799 loss_train: 75.9693 loss_val: 78.3167\n",
      "Epoch: 800 loss_train: 75.7160 loss_val: 76.8764\n",
      "Epoch: 801 loss_train: 75.8071 loss_val: 75.5576\n",
      "Epoch: 802 loss_train: 77.5316 loss_val: 79.3770\n",
      "Epoch: 803 loss_train: 76.3463 loss_val: 76.1214\n",
      "Epoch: 804 loss_train: 77.3485 loss_val: 78.0343\n",
      "Epoch: 805 loss_train: 78.9825 loss_val: 75.2460\n",
      "Epoch: 806 loss_train: 75.7363 loss_val: 79.8412\n",
      "Epoch: 807 loss_train: 80.4203 loss_val: 87.3082\n",
      "Epoch: 808 loss_train: 83.1384 loss_val: 81.7215\n",
      "Epoch: 809 loss_train: 82.4081 loss_val: 76.5160\n",
      "Epoch: 810 loss_train: 79.5486 loss_val: 84.9293\n",
      "Epoch: 811 loss_train: 83.8222 loss_val: 86.0893\n",
      "Epoch: 812 loss_train: 81.9186 loss_val: 86.3309\n",
      "Epoch: 813 loss_train: 82.7183 loss_val: 80.9038\n",
      "Epoch: 814 loss_train: 85.5913 loss_val: 86.3438\n",
      "Epoch: 815 loss_train: 81.9140 loss_val: 83.7536\n",
      "Epoch: 816 loss_train: 82.3932 loss_val: 82.8664\n",
      "Epoch: 817 loss_train: 85.0432 loss_val: 81.0932\n",
      "Epoch: 818 loss_train: 82.0483 loss_val: 82.7620\n",
      "Epoch: 819 loss_train: 86.3015 loss_val: 78.7553\n",
      "Epoch: 820 loss_train: 84.2067 loss_val: 81.7555\n",
      "Epoch: 821 loss_train: 82.6338 loss_val: 82.8652\n",
      "Epoch: 822 loss_train: 81.5298 loss_val: 78.9031\n",
      "Epoch: 823 loss_train: 79.7431 loss_val: 75.5432\n",
      "Epoch: 824 loss_train: 79.6105 loss_val: 75.6789\n",
      "Epoch: 825 loss_train: 78.4970 loss_val: 80.3341\n",
      "Epoch: 826 loss_train: 79.5524 loss_val: 82.6672\n",
      "Epoch: 827 loss_train: 80.0979 loss_val: 80.2835\n",
      "Epoch: 828 loss_train: 80.4801 loss_val: 79.3607\n",
      "Epoch: 829 loss_train: 79.5410 loss_val: 77.2412\n",
      "Epoch: 830 loss_train: 80.5143 loss_val: 77.7797\n",
      "Epoch: 831 loss_train: 80.9703 loss_val: 82.1186\n",
      "Epoch: 832 loss_train: 79.6765 loss_val: 79.5950\n",
      "Epoch: 833 loss_train: 78.8896 loss_val: 78.0749\n",
      "Epoch: 834 loss_train: 81.8676 loss_val: 81.5952\n",
      "Epoch: 835 loss_train: 79.7183 loss_val: 78.3150\n",
      "Epoch: 836 loss_train: 77.9619 loss_val: 73.4113\n",
      "Epoch: 837 loss_train: 79.4953 loss_val: 76.0112\n",
      "Epoch: 838 loss_train: 76.8942 loss_val: 79.9959\n",
      "Epoch: 839 loss_train: 77.7909 loss_val: 78.6176\n",
      "Epoch: 840 loss_train: 77.0786 loss_val: 78.5922\n",
      "Epoch: 841 loss_train: 77.5776 loss_val: 79.6095\n",
      "Epoch: 842 loss_train: 76.8643 loss_val: 76.6294\n",
      "Epoch: 843 loss_train: 77.1428 loss_val: 74.8745\n",
      "Epoch: 844 loss_train: 76.2841 loss_val: 76.9113\n",
      "Epoch: 845 loss_train: 75.9836 loss_val: 77.0412\n",
      "Epoch: 846 loss_train: 75.4720 loss_val: 74.5839\n",
      "Epoch: 847 loss_train: 77.0943 loss_val: 79.9822\n",
      "Epoch: 848 loss_train: 76.7681 loss_val: 76.8822\n",
      "Epoch: 849 loss_train: 76.5969 loss_val: 73.2212\n",
      "Epoch: 850 loss_train: 76.1976 loss_val: 76.8131\n",
      "Epoch: 851 loss_train: 75.7442 loss_val: 76.8722\n",
      "Epoch: 852 loss_train: 74.9430 loss_val: 75.9812\n",
      "Epoch: 853 loss_train: 75.8965 loss_val: 78.8261\n",
      "Epoch: 854 loss_train: 75.9401 loss_val: 74.2537\n",
      "Epoch: 855 loss_train: 74.4298 loss_val: 74.3266\n",
      "Epoch: 856 loss_train: 75.1573 loss_val: 77.4304\n",
      "Epoch: 857 loss_train: 75.4711 loss_val: 72.8915\n",
      "Epoch: 858 loss_train: 75.9121 loss_val: 77.2349\n",
      "Epoch: 859 loss_train: 75.8382 loss_val: 77.7351\n",
      "Epoch: 860 loss_train: 74.8165 loss_val: 73.0564\n",
      "Epoch: 861 loss_train: 76.2085 loss_val: 79.7808\n",
      "Epoch: 862 loss_train: 75.5881 loss_val: 77.6136\n",
      "Epoch: 863 loss_train: 76.4054 loss_val: 73.7202\n",
      "Epoch: 864 loss_train: 74.1602 loss_val: 73.2261\n",
      "Epoch: 865 loss_train: 76.8294 loss_val: 80.8470\n",
      "Epoch: 866 loss_train: 76.5039 loss_val: 82.2111\n",
      "Epoch: 867 loss_train: 78.7070 loss_val: 80.1907\n",
      "Epoch: 868 loss_train: 76.5743 loss_val: 72.2040\n",
      "Epoch: 869 loss_train: 76.2049 loss_val: 72.8216\n",
      "Epoch: 870 loss_train: 74.7668 loss_val: 77.4244\n",
      "Epoch: 871 loss_train: 75.1464 loss_val: 78.2097\n",
      "Epoch: 872 loss_train: 76.4312 loss_val: 79.6761\n",
      "Epoch: 873 loss_train: 76.8136 loss_val: 73.3567\n",
      "Epoch: 874 loss_train: 77.9039 loss_val: 80.5726\n",
      "Epoch: 875 loss_train: 76.9065 loss_val: 83.6278\n",
      "Epoch: 876 loss_train: 79.0367 loss_val: 80.2814\n",
      "Epoch: 877 loss_train: 78.1184 loss_val: 77.3148\n",
      "Epoch: 878 loss_train: 78.5718 loss_val: 77.7524\n",
      "Epoch: 879 loss_train: 76.9671 loss_val: 77.1316\n",
      "Epoch: 880 loss_train: 77.1834 loss_val: 76.2166\n",
      "Epoch: 881 loss_train: 76.8968 loss_val: 76.4069\n",
      "Epoch: 882 loss_train: 76.1285 loss_val: 76.6873\n",
      "Epoch: 883 loss_train: 76.5550 loss_val: 76.1094\n",
      "Epoch: 884 loss_train: 74.5899 loss_val: 77.5885\n",
      "Epoch: 885 loss_train: 75.5462 loss_val: 79.2481\n",
      "Epoch: 886 loss_train: 76.4051 loss_val: 74.7717\n",
      "Epoch: 887 loss_train: 76.4466 loss_val: 77.0705\n",
      "Epoch: 888 loss_train: 75.0724 loss_val: 77.4720\n",
      "Epoch: 889 loss_train: 74.2338 loss_val: 77.8692\n",
      "Epoch: 890 loss_train: 75.6073 loss_val: 78.9676\n",
      "Epoch: 891 loss_train: 75.9088 loss_val: 74.6716\n",
      "Epoch: 892 loss_train: 75.3778 loss_val: 75.0874\n",
      "Epoch: 893 loss_train: 77.6169 loss_val: 81.8444\n",
      "Epoch: 894 loss_train: 78.3317 loss_val: 79.4866\n",
      "Epoch: 895 loss_train: 76.0455 loss_val: 76.8456\n",
      "Epoch: 896 loss_train: 80.1423 loss_val: 81.5752\n",
      "Epoch: 897 loss_train: 77.0399 loss_val: 82.3005\n",
      "Epoch: 898 loss_train: 77.4584 loss_val: 74.6922\n",
      "Epoch: 899 loss_train: 75.3905 loss_val: 76.3460\n",
      "Epoch: 900 loss_train: 76.5978 loss_val: 79.7672\n",
      "Epoch: 901 loss_train: 75.5629 loss_val: 79.3264\n",
      "Epoch: 902 loss_train: 75.4123 loss_val: 73.9720\n",
      "Epoch: 903 loss_train: 76.3524 loss_val: 74.9784\n",
      "Epoch: 904 loss_train: 75.0108 loss_val: 77.9287\n",
      "Epoch: 905 loss_train: 74.7798 loss_val: 75.7425\n",
      "Epoch: 906 loss_train: 75.1110 loss_val: 77.2617\n",
      "Epoch: 907 loss_train: 76.0443 loss_val: 73.6832\n",
      "Epoch: 908 loss_train: 74.6804 loss_val: 75.1402\n",
      "Epoch: 909 loss_train: 75.6472 loss_val: 81.4458\n",
      "Epoch: 910 loss_train: 75.8586 loss_val: 77.2179\n",
      "Epoch: 911 loss_train: 74.2774 loss_val: 74.6814\n",
      "Epoch: 912 loss_train: 77.1053 loss_val: 77.6469\n",
      "Epoch: 913 loss_train: 74.4798 loss_val: 79.5395\n",
      "Epoch: 914 loss_train: 75.0063 loss_val: 77.7490\n",
      "Epoch: 915 loss_train: 75.4717 loss_val: 75.2275\n",
      "Epoch: 916 loss_train: 74.6688 loss_val: 76.9396\n",
      "Epoch: 917 loss_train: 74.9149 loss_val: 75.4351\n",
      "Epoch: 918 loss_train: 73.3681 loss_val: 76.6781\n",
      "Epoch: 919 loss_train: 75.0997 loss_val: 83.2971\n"
     ]
    }
   ],
   "source": [
    "n_dim=X_train_x.shape[1]\n",
    "model=MLP(n_dim,256,64,0.3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "loss_vals=[]\n",
    "loss_trains=[]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_x)\n",
    "    loss_train = loss(output.reshape(-1), hindex_train_x)\n",
    "    loss_trains.append(loss_train.item())\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output= model(X_val_x)\n",
    "\n",
    "    loss_val = loss(output.reshape(-1), hindex_val_x)\n",
    "    loss_vals.append(loss_val.item())\n",
    "    print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "            'loss_val: {:.4f}'.format(loss_val.item()))\n",
    "    if (epoch>100 and loss_val.item()>loss_train.item()*1.1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f142cb8ae80>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_vals[20:])\n",
    "plt.plot(loss_trains[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "571ac4587ae4eeb6b02353bd76aeaaf0ceca15cf49d684242a7eb1fb5d42efb7"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('myEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
