{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Global embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory : /users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project/Global, Project directory : /users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model as LinearModels\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "from scipy import stats\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "NathanPath=\"d:\\Documents\\Info\\INF554\\INF554_Kaggle_Project\"\n",
    "NathanPath=\"/users/eleves-a/2019/nathan.peluso/INF554/INF554_Kaggle_Project\"\n",
    "\n",
    "project_path = str(Path(os.getcwd()).parent.absolute())\n",
    "print(\"Current directory : \" + os.getcwd() + \", Project directory : \" + project_path)\n",
    "\n",
    "os.chdir(project_path)\n",
    "os.chdir(NathanPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=np.load(\"Global/paper_vectors.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_abstracts=vectors[:,0].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624181\n",
      "2908511069\n",
      "3603\n"
     ]
    }
   ],
   "source": [
    "print(len(id_abstracts))\n",
    "print(np.max(id_abstracts))\n",
    "print(np.min(id_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_id_abstracts=dict([(a,b) for a,b in enumerate(id_abstracts)])\n",
    "id_abstracts_num=dict([(b,a) for a,b in enumerate(id_abstracts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624181"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_abstracts_num.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/author_papers.txt\") as f:\n",
    "    authors_papers=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=vectors.shape[1]-1\n",
    "authors_vectors=np.zeros((len(authors_papers), n_dim+1), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1510273386', '1827736641', '1588673897', '2252711322', '2123653597']\n",
      "1510273386\n",
      "58046\n"
     ]
    }
   ],
   "source": [
    "papers=authors_papers[0].split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "print(papers)\n",
    "p=papers[0]\n",
    "print(int(p))\n",
    "print(id_abstracts_num.get(int(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880\n"
     ]
    }
   ],
   "source": [
    "s=0\n",
    "for i,author in enumerate(authors_papers):\n",
    "    papers=author.split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "    vector=np.zeros(n_dim)\n",
    "    no_fail=False\n",
    "    for p in papers:\n",
    "        try:\n",
    "            vector+=vectors[id_abstracts_num[int(p)], 1:]\n",
    "            no_fail=True\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if (not no_fail):\n",
    "        s+=1\n",
    "    authors_vectors[i][0]=int(author.split(\":\")[0])\n",
    "    if (np.linalg.norm(vector)>0):\n",
    "        vector=vector/np.linalg.norm(vector)\n",
    "    authors_vectors[i][1:]=vector.copy()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Global/authors_vectors.npy\", authors_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_improved=np.load(\"DeepWalk/embeddings_improved.npy\")\n",
    "authors_vectors=np.load(\"Global/authors_vectors.npy\")\n",
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217801, 135)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_improved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217801, 135)\n",
      "(217801, 151)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_improved.shape)\n",
    "print(authors_vectors.shape)\n",
    "\n",
    "auth_vec_num_id_author=dict([(a,b) for a,b in enumerate(authors_vectors[:,0])])\n",
    "id_author_auth_vec_num=dict([(b,a) for a,b in enumerate(authors_vectors[:,0])])\n",
    "\n",
    "graph_num_id_author=dict([(a,b) for a,b in enumerate(G.nodes)])\n",
    "id_author_graph_num=dict([(b,a) for a,b in enumerate(G.nodes)])\n",
    "\n",
    "emb_num_id_author=dict([(a,int(b)) for a,b in enumerate(embeddings_improved[:,0])])\n",
    "id_author_emb_num=dict([(int(b),a) for a,b in enumerate(embeddings_improved[:,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=G.number_of_nodes()\n",
    "n_emb=embeddings_improved.shape[1]-1\n",
    "n_abs=authors_vectors.shape[1]-1\n",
    "n_dim_tot=1+n_emb+n_abs\n",
    "full_matrix=np.zeros((n_nodes, n_dim_tot), dtype=np.float64)\n",
    "for i in range(n_nodes):\n",
    "    node=graph_num_id_author[i]\n",
    "    full_matrix[i,0]=node\n",
    "    full_matrix[i,1:1+n_emb]=embeddings_improved[id_author_emb_num[node],1:].copy()\n",
    "    full_matrix[i,1+n_emb:]=authors_vectors[id_author_auth_vec_num[node],1:].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Global/full_embedding_matrix.npy\", full_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217801, 285)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP model\"\"\"\n",
    "    def __init__(self, n_feat, n_hidden_1, n_hidden_2, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z0 = self.relu(self.fc1(x))\n",
    "        z0 = self.dropout(z0)\n",
    "        z1 = self.relu(self.fc2(z0))\n",
    "        out = self.fc3(z1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X,Y):\n",
    "    if (len(X)!=len(Y)):\n",
    "        print(\"Sizes not identical\")\n",
    "        return -1\n",
    "    return (X-Y)@(X-Y) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('data/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'author': np.int64})\n",
    "full_embedding=np.load(\"Global/full_embedding_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_nodeID_Train=dict(df_train[\"author\"])\n",
    "nodeID_abs_Train=dict([(b,a) for a,b in abs_nodeID_Train.items()])\n",
    "\n",
    "abs_nodeID_Test=dict(df_test[\"author\"])\n",
    "nodeID_abs_Test=dict([(b,a) for a,b in abs_nodeID_Test.items()])\n",
    "\n",
    "abs_hindex_Train=dict(df_train[\"hindex\"])\n",
    "\n",
    "abs_nodeID_Graph=dict(enumerate(G.nodes))\n",
    "nodeID_abs_Graph=dict([(b,a) for a,b in enumerate(G.nodes)])\n",
    "\n",
    "n=G.number_of_nodes()\n",
    "n_train=abs_nodeID_Train.__len__()\n",
    "n_test=abs_nodeID_Test.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Careful, those indexes are related to the TRAIN set, not to the global graph indexing\n",
    "idx=np.random.permutation(n_train)\n",
    "idx_train=idx[:int(0.8*n_train)]\n",
    "idx_val=idx[int(0.8*n_train):]\n",
    "\n",
    "nodes_train=[abs_nodeID_Train[i] for i in idx_train]\n",
    "nodes_val=[abs_nodeID_Train[i] for i in idx_val]\n",
    "\n",
    "X_train_x = torch.tensor([full_embedding[nodeID_abs_Graph[node]][1:] for node in nodes_train], dtype=torch.float32)\n",
    "X_val_x = torch.tensor([full_embedding[nodeID_abs_Graph[node]][1:] for node in nodes_val], dtype=torch.float32)\n",
    "\n",
    "hindex_train_x=torch.tensor([abs_hindex_Train[i] for i in idx_train], dtype=torch.float32)\n",
    "hindex_val_x=torch.tensor([abs_hindex_Train[i] for i in idx_val], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Training on split set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=X_train_x.shape[1]\n",
    "model=MLP(n_dim,256,64,0.3)\n",
    "\n",
    "loss_vals=[]\n",
    "loss_trains=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 129.9395 loss_val: 75.9737\n",
      "Epoch: 002 loss_train: 84.2800 loss_val: 81.0137\n",
      "Epoch: 003 loss_train: 85.8534 loss_val: 83.6312\n",
      "Epoch: 004 loss_train: 86.0053 loss_val: 84.3926\n",
      "Epoch: 005 loss_train: 82.9849 loss_val: 86.9541\n",
      "Epoch: 006 loss_train: 82.0464 loss_val: 85.3234\n",
      "Epoch: 007 loss_train: 81.3070 loss_val: 80.0291\n",
      "Epoch: 008 loss_train: 77.5318 loss_val: 74.9453\n",
      "Epoch: 009 loss_train: 74.7539 loss_val: 72.6639\n",
      "Epoch: 010 loss_train: 75.8380 loss_val: 72.4747\n",
      "Epoch: 011 loss_train: 77.9883 loss_val: 71.6868\n",
      "Epoch: 012 loss_train: 78.7933 loss_val: 69.6541\n",
      "Epoch: 013 loss_train: 77.9094 loss_val: 69.6241\n",
      "Epoch: 014 loss_train: 74.9831 loss_val: 72.4308\n",
      "Epoch: 015 loss_train: 73.3754 loss_val: 76.1548\n",
      "Epoch: 016 loss_train: 75.8551 loss_val: 77.7035\n",
      "Epoch: 017 loss_train: 76.1453 loss_val: 76.0124\n",
      "Epoch: 018 loss_train: 74.7386 loss_val: 72.6891\n",
      "Epoch: 019 loss_train: 72.4096 loss_val: 70.0062\n",
      "Epoch: 020 loss_train: 72.1020 loss_val: 68.9886\n",
      "Epoch: 021 loss_train: 73.9758 loss_val: 68.6267\n",
      "Epoch: 022 loss_train: 72.8730 loss_val: 68.4436\n",
      "Epoch: 023 loss_train: 72.0095 loss_val: 69.2702\n",
      "Epoch: 024 loss_train: 71.0754 loss_val: 70.8595\n",
      "Epoch: 025 loss_train: 72.4844 loss_val: 71.8711\n",
      "Epoch: 026 loss_train: 72.5902 loss_val: 71.4790\n",
      "Epoch: 027 loss_train: 71.7650 loss_val: 70.2865\n",
      "Epoch: 028 loss_train: 71.3110 loss_val: 69.4186\n",
      "Epoch: 029 loss_train: 70.4763 loss_val: 69.1966\n",
      "Epoch: 030 loss_train: 71.6989 loss_val: 69.2238\n",
      "Epoch: 031 loss_train: 70.8493 loss_val: 69.3146\n",
      "Epoch: 032 loss_train: 70.2741 loss_val: 69.8270\n",
      "Epoch: 033 loss_train: 70.3253 loss_val: 70.3676\n",
      "Epoch: 034 loss_train: 70.8934 loss_val: 70.2413\n",
      "Epoch: 035 loss_train: 70.5592 loss_val: 69.4591\n",
      "Epoch: 036 loss_train: 70.1177 loss_val: 68.3564\n",
      "Epoch: 037 loss_train: 70.5156 loss_val: 67.8453\n",
      "Epoch: 038 loss_train: 69.9955 loss_val: 67.8589\n",
      "Epoch: 039 loss_train: 70.8246 loss_val: 68.4887\n",
      "Epoch: 040 loss_train: 69.8881 loss_val: 69.2142\n",
      "Epoch: 041 loss_train: 69.3609 loss_val: 69.7453\n",
      "Epoch: 042 loss_train: 69.3284 loss_val: 69.7563\n",
      "Epoch: 043 loss_train: 68.7750 loss_val: 69.2958\n",
      "Epoch: 044 loss_train: 69.6654 loss_val: 68.8482\n",
      "Epoch: 045 loss_train: 70.1452 loss_val: 68.6180\n",
      "Epoch: 046 loss_train: 68.8956 loss_val: 68.5670\n",
      "Epoch: 047 loss_train: 69.0082 loss_val: 68.7459\n",
      "Epoch: 048 loss_train: 69.1182 loss_val: 68.6190\n",
      "Epoch: 049 loss_train: 69.0442 loss_val: 68.3382\n",
      "Epoch: 050 loss_train: 68.8706 loss_val: 68.1938\n",
      "Epoch: 051 loss_train: 69.1560 loss_val: 68.3094\n",
      "Epoch: 052 loss_train: 68.8531 loss_val: 68.5757\n",
      "Epoch: 053 loss_train: 69.3222 loss_val: 68.8922\n",
      "Epoch: 054 loss_train: 68.7168 loss_val: 68.8607\n",
      "Epoch: 055 loss_train: 68.5580 loss_val: 68.6316\n",
      "Epoch: 056 loss_train: 68.2878 loss_val: 68.3231\n",
      "Epoch: 057 loss_train: 68.3736 loss_val: 68.2068\n",
      "Epoch: 058 loss_train: 69.1323 loss_val: 68.3345\n",
      "Epoch: 059 loss_train: 69.1887 loss_val: 68.5445\n",
      "Epoch: 060 loss_train: 68.6978 loss_val: 68.6180\n",
      "Epoch: 061 loss_train: 69.0009 loss_val: 68.3961\n",
      "Epoch: 062 loss_train: 68.7287 loss_val: 68.1678\n",
      "Epoch: 063 loss_train: 68.6418 loss_val: 68.1960\n",
      "Epoch: 064 loss_train: 68.0437 loss_val: 68.4883\n",
      "Epoch: 065 loss_train: 68.4260 loss_val: 68.7155\n",
      "Epoch: 066 loss_train: 67.8515 loss_val: 68.5113\n",
      "Epoch: 067 loss_train: 68.5647 loss_val: 68.2264\n",
      "Epoch: 068 loss_train: 68.3318 loss_val: 68.0677\n",
      "Epoch: 069 loss_train: 67.8516 loss_val: 68.1252\n",
      "Epoch: 070 loss_train: 67.9639 loss_val: 68.3754\n",
      "Epoch: 071 loss_train: 68.4554 loss_val: 68.5195\n",
      "Epoch: 072 loss_train: 67.5228 loss_val: 68.2768\n",
      "Epoch: 073 loss_train: 68.0328 loss_val: 67.9954\n",
      "Epoch: 074 loss_train: 68.2492 loss_val: 67.8251\n",
      "Epoch: 075 loss_train: 67.9543 loss_val: 67.9187\n",
      "Epoch: 076 loss_train: 68.2499 loss_val: 68.1334\n",
      "Epoch: 077 loss_train: 67.8450 loss_val: 68.1905\n",
      "Epoch: 078 loss_train: 68.1881 loss_val: 68.0167\n",
      "Epoch: 079 loss_train: 68.8261 loss_val: 67.9251\n",
      "Epoch: 080 loss_train: 68.2689 loss_val: 68.0864\n",
      "Epoch: 081 loss_train: 67.9676 loss_val: 68.3697\n",
      "Epoch: 082 loss_train: 67.4788 loss_val: 68.2532\n",
      "Epoch: 083 loss_train: 67.5131 loss_val: 67.8515\n",
      "Epoch: 084 loss_train: 67.6983 loss_val: 67.7926\n",
      "Epoch: 085 loss_train: 67.8465 loss_val: 68.1184\n",
      "Epoch: 086 loss_train: 67.7549 loss_val: 68.0773\n",
      "Epoch: 087 loss_train: 67.4348 loss_val: 67.8405\n",
      "Epoch: 088 loss_train: 67.4455 loss_val: 67.8108\n",
      "Epoch: 089 loss_train: 67.3432 loss_val: 68.0006\n",
      "Epoch: 090 loss_train: 67.8977 loss_val: 68.0643\n",
      "Epoch: 091 loss_train: 67.4047 loss_val: 67.8538\n",
      "Epoch: 092 loss_train: 67.3037 loss_val: 67.6213\n",
      "Epoch: 093 loss_train: 67.8538 loss_val: 67.3409\n",
      "Epoch: 094 loss_train: 67.3351 loss_val: 67.5021\n",
      "Epoch: 095 loss_train: 67.2883 loss_val: 67.8218\n",
      "Epoch: 096 loss_train: 67.1105 loss_val: 68.0090\n",
      "Epoch: 097 loss_train: 68.0606 loss_val: 67.9619\n",
      "Epoch: 098 loss_train: 67.3790 loss_val: 67.7970\n",
      "Epoch: 099 loss_train: 66.8187 loss_val: 67.8265\n",
      "Epoch: 100 loss_train: 67.0837 loss_val: 68.1377\n",
      "Epoch: 101 loss_train: 67.4712 loss_val: 67.8968\n",
      "Epoch: 102 loss_train: 67.5011 loss_val: 67.3966\n",
      "Epoch: 103 loss_train: 67.0661 loss_val: 67.2676\n",
      "Epoch: 104 loss_train: 67.3413 loss_val: 67.6221\n",
      "Epoch: 105 loss_train: 66.9249 loss_val: 68.0595\n",
      "Epoch: 106 loss_train: 67.3178 loss_val: 67.8937\n",
      "Epoch: 107 loss_train: 66.6962 loss_val: 67.3307\n",
      "Epoch: 108 loss_train: 67.0053 loss_val: 67.1796\n",
      "Epoch: 109 loss_train: 66.9256 loss_val: 67.5851\n",
      "Epoch: 110 loss_train: 66.9777 loss_val: 67.5187\n",
      "Epoch: 111 loss_train: 66.7155 loss_val: 67.4245\n",
      "Epoch: 112 loss_train: 66.8630 loss_val: 67.5220\n",
      "Epoch: 113 loss_train: 67.3123 loss_val: 67.8241\n",
      "Epoch: 114 loss_train: 66.9219 loss_val: 67.7403\n",
      "Epoch: 115 loss_train: 67.0203 loss_val: 67.4591\n",
      "Epoch: 116 loss_train: 67.0095 loss_val: 67.3728\n",
      "Epoch: 117 loss_train: 66.6042 loss_val: 67.4718\n",
      "Epoch: 118 loss_train: 66.2969 loss_val: 67.3558\n",
      "Epoch: 119 loss_train: 66.7787 loss_val: 67.1884\n",
      "Epoch: 120 loss_train: 66.5130 loss_val: 67.2996\n",
      "Epoch: 121 loss_train: 66.6993 loss_val: 67.5494\n",
      "Epoch: 122 loss_train: 66.4114 loss_val: 67.3232\n",
      "Epoch: 123 loss_train: 66.8665 loss_val: 67.1873\n",
      "Epoch: 124 loss_train: 66.1168 loss_val: 67.5026\n",
      "Epoch: 125 loss_train: 66.4436 loss_val: 67.1362\n",
      "Epoch: 126 loss_train: 66.8057 loss_val: 67.0338\n",
      "Epoch: 127 loss_train: 66.3270 loss_val: 67.4220\n",
      "Epoch: 128 loss_train: 66.0581 loss_val: 67.7024\n",
      "Epoch: 129 loss_train: 66.1853 loss_val: 67.4077\n",
      "Epoch: 130 loss_train: 66.2623 loss_val: 67.2251\n",
      "Epoch: 131 loss_train: 66.3172 loss_val: 67.1133\n",
      "Epoch: 132 loss_train: 66.0221 loss_val: 67.2832\n",
      "Epoch: 133 loss_train: 66.3842 loss_val: 67.2890\n",
      "Epoch: 134 loss_train: 65.8360 loss_val: 67.1506\n",
      "Epoch: 135 loss_train: 66.0414 loss_val: 67.0765\n",
      "Epoch: 136 loss_train: 65.9609 loss_val: 67.2761\n",
      "Epoch: 137 loss_train: 66.0673 loss_val: 67.6087\n",
      "Epoch: 138 loss_train: 66.1983 loss_val: 67.4530\n",
      "Epoch: 139 loss_train: 65.7793 loss_val: 67.0683\n",
      "Epoch: 140 loss_train: 66.4072 loss_val: 67.1093\n",
      "Epoch: 141 loss_train: 65.7835 loss_val: 67.4864\n",
      "Epoch: 142 loss_train: 66.1552 loss_val: 67.5061\n",
      "Epoch: 143 loss_train: 65.8889 loss_val: 67.1402\n",
      "Epoch: 144 loss_train: 65.8801 loss_val: 67.4832\n",
      "Epoch: 145 loss_train: 65.5743 loss_val: 67.3363\n",
      "Epoch: 146 loss_train: 65.9683 loss_val: 67.3033\n",
      "Epoch: 147 loss_train: 66.0512 loss_val: 67.1460\n",
      "Epoch: 148 loss_train: 65.7427 loss_val: 67.2162\n",
      "Epoch: 149 loss_train: 65.9352 loss_val: 67.4090\n",
      "Epoch: 150 loss_train: 65.7009 loss_val: 67.2944\n",
      "Epoch: 151 loss_train: 65.7650 loss_val: 67.1582\n",
      "Epoch: 152 loss_train: 65.6758 loss_val: 67.0816\n",
      "Epoch: 153 loss_train: 65.7765 loss_val: 67.2868\n",
      "Epoch: 154 loss_train: 65.9097 loss_val: 67.2517\n",
      "Epoch: 155 loss_train: 65.8216 loss_val: 67.5800\n",
      "Epoch: 156 loss_train: 65.6777 loss_val: 67.3569\n",
      "Epoch: 157 loss_train: 65.8946 loss_val: 67.3986\n",
      "Epoch: 158 loss_train: 65.8365 loss_val: 67.5859\n",
      "Epoch: 159 loss_train: 65.6067 loss_val: 67.4588\n",
      "Epoch: 160 loss_train: 65.5643 loss_val: 67.2490\n",
      "Epoch: 161 loss_train: 65.1883 loss_val: 67.2407\n",
      "Epoch: 162 loss_train: 65.3037 loss_val: 67.1772\n",
      "Epoch: 163 loss_train: 65.2752 loss_val: 67.1913\n",
      "Epoch: 164 loss_train: 65.7428 loss_val: 67.3819\n",
      "Epoch: 165 loss_train: 65.0653 loss_val: 67.1841\n",
      "Epoch: 166 loss_train: 65.0642 loss_val: 67.2980\n",
      "Epoch: 167 loss_train: 65.0586 loss_val: 67.6271\n",
      "Epoch: 168 loss_train: 65.4170 loss_val: 67.3958\n",
      "Epoch: 169 loss_train: 65.5737 loss_val: 67.2420\n",
      "Epoch: 170 loss_train: 65.4727 loss_val: 67.4680\n",
      "Epoch: 171 loss_train: 65.1447 loss_val: 67.5904\n",
      "Epoch: 172 loss_train: 66.0234 loss_val: 67.1293\n",
      "Epoch: 173 loss_train: 65.1634 loss_val: 67.0148\n",
      "Epoch: 174 loss_train: 65.2990 loss_val: 67.7211\n",
      "Epoch: 175 loss_train: 65.2653 loss_val: 67.3285\n",
      "Epoch: 176 loss_train: 65.4314 loss_val: 66.8280\n",
      "Epoch: 177 loss_train: 65.2176 loss_val: 67.1126\n",
      "Epoch: 178 loss_train: 64.9481 loss_val: 67.3894\n",
      "Epoch: 179 loss_train: 64.9960 loss_val: 67.3013\n",
      "Epoch: 180 loss_train: 64.8047 loss_val: 67.0686\n",
      "Epoch: 181 loss_train: 64.8519 loss_val: 66.9137\n",
      "Epoch: 182 loss_train: 64.8967 loss_val: 67.1885\n",
      "Epoch: 183 loss_train: 65.0560 loss_val: 67.3694\n",
      "Epoch: 184 loss_train: 65.0331 loss_val: 67.3934\n",
      "Epoch: 185 loss_train: 65.1922 loss_val: 67.0705\n",
      "Epoch: 186 loss_train: 64.7077 loss_val: 67.2256\n",
      "Epoch: 187 loss_train: 64.9799 loss_val: 67.3725\n",
      "Epoch: 188 loss_train: 64.9470 loss_val: 66.8601\n",
      "Epoch: 189 loss_train: 64.9692 loss_val: 66.8101\n",
      "Epoch: 190 loss_train: 65.0409 loss_val: 67.5614\n",
      "Epoch: 191 loss_train: 65.2961 loss_val: 67.0363\n",
      "Epoch: 192 loss_train: 64.4784 loss_val: 66.8451\n",
      "Epoch: 193 loss_train: 64.6258 loss_val: 67.4943\n",
      "Epoch: 194 loss_train: 64.8544 loss_val: 66.8595\n",
      "Epoch: 195 loss_train: 64.5975 loss_val: 66.9055\n",
      "Epoch: 196 loss_train: 64.3841 loss_val: 67.5483\n",
      "Epoch: 197 loss_train: 65.0049 loss_val: 67.8226\n",
      "Epoch: 198 loss_train: 64.2118 loss_val: 67.1485\n",
      "Epoch: 199 loss_train: 64.8540 loss_val: 66.7472\n",
      "Epoch: 200 loss_train: 64.6834 loss_val: 67.0999\n",
      "Epoch: 201 loss_train: 64.5321 loss_val: 66.9098\n",
      "Epoch: 202 loss_train: 64.1482 loss_val: 67.0074\n",
      "Epoch: 203 loss_train: 64.8773 loss_val: 67.7619\n",
      "Epoch: 204 loss_train: 64.8235 loss_val: 67.3428\n",
      "Epoch: 205 loss_train: 64.4336 loss_val: 67.1764\n",
      "Epoch: 206 loss_train: 64.0117 loss_val: 67.1804\n",
      "Epoch: 207 loss_train: 64.6718 loss_val: 66.9336\n",
      "Epoch: 208 loss_train: 64.7012 loss_val: 67.3449\n",
      "Epoch: 209 loss_train: 64.5042 loss_val: 67.7886\n",
      "Epoch: 210 loss_train: 64.6510 loss_val: 67.4670\n",
      "Epoch: 211 loss_train: 64.1883 loss_val: 66.8315\n",
      "Epoch: 212 loss_train: 64.6014 loss_val: 66.8931\n",
      "Epoch: 213 loss_train: 64.1213 loss_val: 67.4365\n",
      "Epoch: 214 loss_train: 64.2313 loss_val: 67.1698\n",
      "Epoch: 215 loss_train: 65.6116 loss_val: 68.0451\n",
      "Epoch: 216 loss_train: 64.9105 loss_val: 66.7967\n",
      "Epoch: 217 loss_train: 64.3243 loss_val: 66.8221\n",
      "Epoch: 218 loss_train: 64.3892 loss_val: 67.9434\n",
      "Epoch: 219 loss_train: 64.4428 loss_val: 67.6778\n",
      "Epoch: 220 loss_train: 63.7835 loss_val: 67.1807\n",
      "Epoch: 221 loss_train: 64.4543 loss_val: 67.1073\n",
      "Epoch: 222 loss_train: 63.5928 loss_val: 67.2934\n",
      "Epoch: 223 loss_train: 64.4209 loss_val: 66.7880\n",
      "Epoch: 224 loss_train: 63.9144 loss_val: 66.9856\n",
      "Epoch: 225 loss_train: 64.3523 loss_val: 67.4832\n",
      "Epoch: 226 loss_train: 64.1779 loss_val: 66.7601\n",
      "Epoch: 227 loss_train: 64.1241 loss_val: 66.7269\n",
      "Epoch: 228 loss_train: 63.9947 loss_val: 67.9048\n",
      "Epoch: 229 loss_train: 64.8488 loss_val: 66.5560\n",
      "Epoch: 230 loss_train: 64.2145 loss_val: 66.6394\n",
      "Epoch: 231 loss_train: 64.6114 loss_val: 67.7358\n",
      "Epoch: 232 loss_train: 64.1591 loss_val: 67.9330\n",
      "Epoch: 233 loss_train: 64.4072 loss_val: 67.3641\n",
      "Epoch: 234 loss_train: 64.4491 loss_val: 66.9748\n",
      "Epoch: 235 loss_train: 63.7188 loss_val: 67.4290\n",
      "Epoch: 236 loss_train: 63.7931 loss_val: 66.9412\n",
      "Epoch: 237 loss_train: 63.7785 loss_val: 67.4105\n",
      "Epoch: 238 loss_train: 63.7204 loss_val: 67.2014\n",
      "Epoch: 239 loss_train: 64.1375 loss_val: 66.4891\n",
      "Epoch: 240 loss_train: 63.8158 loss_val: 66.7877\n",
      "Epoch: 241 loss_train: 63.5728 loss_val: 66.9904\n",
      "Epoch: 242 loss_train: 63.6701 loss_val: 66.6904\n",
      "Epoch: 243 loss_train: 64.2151 loss_val: 67.2105\n",
      "Epoch: 244 loss_train: 63.7268 loss_val: 67.2300\n",
      "Epoch: 245 loss_train: 63.3340 loss_val: 66.6282\n",
      "Epoch: 246 loss_train: 63.4775 loss_val: 66.9392\n",
      "Epoch: 247 loss_train: 64.1288 loss_val: 67.2169\n",
      "Epoch: 248 loss_train: 64.0266 loss_val: 67.1116\n",
      "Epoch: 249 loss_train: 63.7159 loss_val: 67.2072\n",
      "Epoch: 250 loss_train: 63.7040 loss_val: 66.7157\n",
      "Epoch: 251 loss_train: 63.5548 loss_val: 66.6758\n",
      "Epoch: 252 loss_train: 63.4718 loss_val: 67.1150\n",
      "Epoch: 253 loss_train: 63.5043 loss_val: 67.1813\n",
      "Epoch: 254 loss_train: 63.6214 loss_val: 66.9615\n",
      "Epoch: 255 loss_train: 63.3335 loss_val: 66.4510\n",
      "Epoch: 256 loss_train: 63.3315 loss_val: 66.1218\n",
      "Epoch: 257 loss_train: 63.7836 loss_val: 67.7861\n",
      "Epoch: 258 loss_train: 63.3147 loss_val: 67.4409\n",
      "Epoch: 259 loss_train: 63.2876 loss_val: 66.8146\n",
      "Epoch: 260 loss_train: 63.8608 loss_val: 67.2668\n",
      "Epoch: 261 loss_train: 63.4965 loss_val: 67.3009\n",
      "Epoch: 262 loss_train: 63.3252 loss_val: 66.5179\n",
      "Epoch: 263 loss_train: 62.9654 loss_val: 66.5207\n",
      "Epoch: 264 loss_train: 63.1812 loss_val: 67.6892\n",
      "Epoch: 265 loss_train: 63.6616 loss_val: 66.8695\n",
      "Epoch: 266 loss_train: 63.0358 loss_val: 66.4590\n",
      "Epoch: 267 loss_train: 63.0507 loss_val: 67.0528\n",
      "Epoch: 268 loss_train: 63.0634 loss_val: 67.3060\n",
      "Epoch: 269 loss_train: 63.3679 loss_val: 66.3548\n",
      "Epoch: 270 loss_train: 62.9481 loss_val: 66.4172\n",
      "Epoch: 271 loss_train: 62.8507 loss_val: 66.6701\n",
      "Epoch: 272 loss_train: 62.6693 loss_val: 67.0826\n",
      "Epoch: 273 loss_train: 63.1458 loss_val: 66.9728\n",
      "Epoch: 274 loss_train: 63.1435 loss_val: 66.6824\n",
      "Epoch: 275 loss_train: 63.0031 loss_val: 67.0757\n",
      "Epoch: 276 loss_train: 63.1258 loss_val: 66.6396\n",
      "Epoch: 277 loss_train: 63.0547 loss_val: 66.3740\n",
      "Epoch: 278 loss_train: 62.4215 loss_val: 66.9867\n",
      "Epoch: 279 loss_train: 63.0581 loss_val: 67.2995\n",
      "Epoch: 280 loss_train: 62.9929 loss_val: 66.4896\n",
      "Epoch: 281 loss_train: 62.8767 loss_val: 66.7814\n",
      "Epoch: 282 loss_train: 62.9439 loss_val: 66.7545\n",
      "Epoch: 283 loss_train: 62.5421 loss_val: 66.5501\n",
      "Epoch: 284 loss_train: 62.7544 loss_val: 67.4518\n",
      "Epoch: 285 loss_train: 63.0936 loss_val: 66.8496\n",
      "Epoch: 286 loss_train: 62.5900 loss_val: 66.8712\n",
      "Epoch: 287 loss_train: 62.9967 loss_val: 66.3722\n",
      "Epoch: 288 loss_train: 62.5540 loss_val: 66.5851\n",
      "Epoch: 289 loss_train: 62.9738 loss_val: 67.6380\n",
      "Epoch: 290 loss_train: 62.5949 loss_val: 67.2102\n",
      "Epoch: 291 loss_train: 62.4444 loss_val: 66.2024\n",
      "Epoch: 292 loss_train: 62.5427 loss_val: 67.1011\n",
      "Epoch: 293 loss_train: 62.6604 loss_val: 66.6428\n",
      "Epoch: 294 loss_train: 62.4494 loss_val: 66.4585\n",
      "Epoch: 295 loss_train: 62.5938 loss_val: 67.4433\n",
      "Epoch: 296 loss_train: 62.6814 loss_val: 67.4696\n",
      "Epoch: 297 loss_train: 62.3397 loss_val: 66.0150\n",
      "Epoch: 298 loss_train: 63.2804 loss_val: 66.5437\n",
      "Epoch: 299 loss_train: 62.9872 loss_val: 66.7389\n",
      "Epoch: 300 loss_train: 62.5226 loss_val: 67.0804\n",
      "Epoch: 301 loss_train: 62.3750 loss_val: 67.3811\n",
      "Epoch: 302 loss_train: 62.6444 loss_val: 67.0174\n",
      "Epoch: 303 loss_train: 62.5949 loss_val: 66.0196\n",
      "Epoch: 304 loss_train: 62.4453 loss_val: 66.0494\n",
      "Epoch: 305 loss_train: 62.3679 loss_val: 68.0891\n",
      "Epoch: 306 loss_train: 63.1878 loss_val: 66.5772\n",
      "Epoch: 307 loss_train: 62.4004 loss_val: 66.3887\n",
      "Epoch: 308 loss_train: 62.3001 loss_val: 67.3876\n",
      "Epoch: 309 loss_train: 63.0270 loss_val: 68.0475\n",
      "Epoch: 310 loss_train: 63.0921 loss_val: 65.8898\n",
      "Epoch: 311 loss_train: 62.4548 loss_val: 66.2783\n",
      "Epoch: 312 loss_train: 62.3132 loss_val: 67.8964\n",
      "Epoch: 313 loss_train: 62.6918 loss_val: 67.2053\n",
      "Epoch: 314 loss_train: 62.3254 loss_val: 66.0098\n",
      "Epoch: 315 loss_train: 62.6322 loss_val: 66.9762\n",
      "Epoch: 316 loss_train: 62.3554 loss_val: 67.4928\n",
      "Epoch: 317 loss_train: 62.6085 loss_val: 66.2574\n",
      "Epoch: 318 loss_train: 62.1953 loss_val: 66.0679\n",
      "Epoch: 319 loss_train: 62.1943 loss_val: 67.2645\n",
      "Epoch: 320 loss_train: 61.9410 loss_val: 67.5691\n",
      "Epoch: 321 loss_train: 61.9098 loss_val: 66.0705\n",
      "Epoch: 322 loss_train: 62.2480 loss_val: 66.2481\n",
      "Epoch: 323 loss_train: 61.9545 loss_val: 67.0959\n",
      "Epoch: 324 loss_train: 61.8616 loss_val: 66.6660\n",
      "Epoch: 325 loss_train: 62.3607 loss_val: 66.3889\n",
      "Epoch: 326 loss_train: 62.2356 loss_val: 66.6267\n",
      "Epoch: 327 loss_train: 61.7126 loss_val: 67.1693\n",
      "Epoch: 328 loss_train: 62.1327 loss_val: 66.4539\n",
      "Epoch: 329 loss_train: 61.9827 loss_val: 66.3993\n",
      "Epoch: 330 loss_train: 61.8450 loss_val: 66.9873\n",
      "Epoch: 331 loss_train: 62.0245 loss_val: 66.8372\n",
      "Epoch: 332 loss_train: 61.8183 loss_val: 66.2899\n",
      "Epoch: 333 loss_train: 61.9184 loss_val: 66.5757\n",
      "Epoch: 334 loss_train: 61.9583 loss_val: 66.7044\n",
      "Epoch: 335 loss_train: 61.6444 loss_val: 66.3734\n",
      "Epoch: 336 loss_train: 61.9014 loss_val: 66.8720\n",
      "Epoch: 337 loss_train: 61.6360 loss_val: 66.8808\n",
      "Epoch: 338 loss_train: 61.9793 loss_val: 67.0114\n",
      "Epoch: 339 loss_train: 62.0461 loss_val: 66.8630\n",
      "Epoch: 340 loss_train: 61.8650 loss_val: 66.6363\n",
      "Epoch: 341 loss_train: 61.6019 loss_val: 66.9977\n",
      "Epoch: 342 loss_train: 61.5953 loss_val: 66.1876\n",
      "Epoch: 343 loss_train: 61.2480 loss_val: 66.3080\n",
      "Epoch: 344 loss_train: 61.7955 loss_val: 67.1734\n",
      "Epoch: 345 loss_train: 61.7322 loss_val: 66.6713\n",
      "Epoch: 346 loss_train: 61.2360 loss_val: 66.4322\n",
      "Epoch: 347 loss_train: 61.6349 loss_val: 66.9229\n",
      "Epoch: 348 loss_train: 60.9997 loss_val: 66.8299\n",
      "Epoch: 349 loss_train: 61.1800 loss_val: 67.6118\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_x)\n",
    "    loss_train = loss(output.reshape(-1), hindex_train_x)\n",
    "    loss_trains.append(loss_train.item())\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output= model(X_val_x)\n",
    "\n",
    "    loss_val = loss(output.reshape(-1), hindex_val_x)\n",
    "    loss_vals.append(loss_val.item())\n",
    "    print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "            'loss_val: {:.4f}'.format(loss_val.item()))\n",
    "    if (epoch>100 and loss_val.item()>loss_train.item()*1.1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f142cb8ae80>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_vals[20:])\n",
    "plt.plot(loss_trains[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Global/256_64_1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_embedding_glob=np.load(\"Global/full_embedding_matrix.npy\")\n",
    "X_train_glob = torch.tensor([full_embedding_glob[nodeID_abs_Graph[node]][1:]for node in nodeID_abs_Train.keys()], dtype=torch.float32)\n",
    "hindex_train_glob=torch.tensor([abs_hindex_Train[nodeID_abs_Train[node]] for node in nodeID_abs_Train.keys()], dtype=torch.float32)\n",
    "X_test=torch.tensor([full_embedding_glob[nodeID_abs_Graph[node]][1:] for node in nodeID_abs_Test.keys()], dtype=torch.float32)\n",
    "\n",
    "\n",
    "idx=range(n_train)\n",
    "nodes_train=[abs_nodeID_Train[i] for i in idx]\n",
    "X_train_glob = torch.tensor([full_embedding_glob[nodeID_abs_Graph[node]][1:] for node in nodes_train], dtype=torch.float32)\n",
    "hindex_train_glob=torch.tensor([abs_hindex_Train[i] for i in idx], dtype=torch.float32)\n",
    "\n",
    "X_test_glob = torch.tensor([full_embedding_glob[nodeID_abs_Graph[node]][1:] for node in nodeID_abs_Test.keys()], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 24992.8770\n",
      "Epoch: 002 loss_train: 4594351104.0000\n",
      "Epoch: 003 loss_train: 1537301120.0000\n",
      "Epoch: 004 loss_train: 10861291.0000\n",
      "Epoch: 005 loss_train: 18185912.0000\n",
      "Epoch: 006 loss_train: 6824299.0000\n",
      "Epoch: 007 loss_train: 141065.9219\n",
      "Epoch: 008 loss_train: 2137109.2500\n",
      "Epoch: 009 loss_train: 3920088.2500\n",
      "Epoch: 010 loss_train: 3980178.5000\n",
      "Epoch: 011 loss_train: 3192298.7500\n",
      "Epoch: 012 loss_train: 2431263.2500\n",
      "Epoch: 013 loss_train: 1826765.0000\n",
      "Epoch: 014 loss_train: 1012823.1250\n",
      "Epoch: 015 loss_train: 194875.3125\n",
      "Epoch: 016 loss_train: 939968.6250\n",
      "Epoch: 017 loss_train: 338878.2500\n",
      "Epoch: 018 loss_train: 77190.0234\n",
      "Epoch: 019 loss_train: 44419.0820\n",
      "Epoch: 020 loss_train: 10778.8438\n",
      "Epoch: 021 loss_train: 508.9709\n",
      "Epoch: 022 loss_train: 9601.0850\n",
      "Epoch: 023 loss_train: 25949.4609\n",
      "Epoch: 024 loss_train: 35835.5195\n",
      "Epoch: 025 loss_train: 43942.3984\n",
      "Epoch: 026 loss_train: 45779.9141\n",
      "Epoch: 027 loss_train: 44790.4258\n",
      "Epoch: 028 loss_train: 43212.3984\n",
      "Epoch: 029 loss_train: 37119.5469\n",
      "Epoch: 030 loss_train: 32131.3496\n",
      "Epoch: 031 loss_train: 22824.1992\n",
      "Epoch: 032 loss_train: 16251.3535\n",
      "Epoch: 033 loss_train: 10532.4551\n",
      "Epoch: 034 loss_train: 6808.9624\n",
      "Epoch: 035 loss_train: 3595.4465\n",
      "Epoch: 036 loss_train: 1998.5046\n",
      "Epoch: 037 loss_train: 1130.2324\n",
      "Epoch: 038 loss_train: 794.0204\n",
      "Epoch: 039 loss_train: 1059.2740\n",
      "Epoch: 040 loss_train: 1597.3090\n",
      "Epoch: 041 loss_train: 2283.6606\n",
      "Epoch: 042 loss_train: 3107.1309\n",
      "Epoch: 043 loss_train: 3644.7507\n",
      "Epoch: 044 loss_train: 4008.6204\n",
      "Epoch: 045 loss_train: 3882.5793\n",
      "Epoch: 046 loss_train: 3783.9927\n",
      "Epoch: 047 loss_train: 3326.8049\n",
      "Epoch: 048 loss_train: 3111.2712\n",
      "Epoch: 049 loss_train: 2959.5386\n",
      "Epoch: 050 loss_train: 2796.3257\n",
      "Epoch: 051 loss_train: 2408.5945\n",
      "Epoch: 052 loss_train: 2203.6597\n",
      "Epoch: 053 loss_train: 1994.5686\n",
      "Epoch: 054 loss_train: 1755.2710\n",
      "Epoch: 055 loss_train: 1610.5748\n",
      "Epoch: 056 loss_train: 1402.5598\n",
      "Epoch: 057 loss_train: 1214.0157\n",
      "Epoch: 058 loss_train: 1483.5201\n",
      "Epoch: 059 loss_train: 920.1813\n",
      "Epoch: 060 loss_train: 835.0389\n",
      "Epoch: 061 loss_train: 140723.0781\n",
      "Epoch: 062 loss_train: 576.1086\n",
      "Epoch: 063 loss_train: 5945.7495\n",
      "Epoch: 064 loss_train: 464.6037\n",
      "Epoch: 065 loss_train: 420.0714\n",
      "Epoch: 066 loss_train: 380.6241\n",
      "Epoch: 067 loss_train: 347.5525\n",
      "Epoch: 068 loss_train: 318.2919\n",
      "Epoch: 069 loss_train: 295.8489\n",
      "Epoch: 070 loss_train: 288.7219\n",
      "Epoch: 071 loss_train: 275.8906\n",
      "Epoch: 072 loss_train: 286.7980\n",
      "Epoch: 073 loss_train: 277.6284\n",
      "Epoch: 074 loss_train: 274.2238\n",
      "Epoch: 075 loss_train: 281.0911\n",
      "Epoch: 076 loss_train: 277.1091\n",
      "Epoch: 077 loss_train: 277.1216\n",
      "Epoch: 078 loss_train: 276.4137\n",
      "Epoch: 079 loss_train: 271.8656\n",
      "Epoch: 080 loss_train: 279.6321\n",
      "Epoch: 081 loss_train: 272.2558\n",
      "Epoch: 082 loss_train: 274.9255\n",
      "Epoch: 083 loss_train: 274.6515\n",
      "Epoch: 084 loss_train: 269.5678\n",
      "Epoch: 085 loss_train: 265.5652\n",
      "Epoch: 086 loss_train: 274.9700\n",
      "Epoch: 087 loss_train: 269.2247\n",
      "Epoch: 088 loss_train: 264.2498\n",
      "Epoch: 089 loss_train: 266.6144\n",
      "Epoch: 090 loss_train: 268.6857\n",
      "Epoch: 091 loss_train: 261.9877\n",
      "Epoch: 092 loss_train: 262.0262\n",
      "Epoch: 093 loss_train: 264.1265\n",
      "Epoch: 094 loss_train: 261.6680\n",
      "Epoch: 095 loss_train: 257.4020\n",
      "Epoch: 096 loss_train: 260.5080\n",
      "Epoch: 097 loss_train: 258.7657\n",
      "Epoch: 098 loss_train: 255.9398\n",
      "Epoch: 099 loss_train: 256.9853\n",
      "Epoch: 100 loss_train: 256.0859\n",
      "Epoch: 101 loss_train: 254.2020\n",
      "Epoch: 102 loss_train: 252.0056\n",
      "Epoch: 103 loss_train: 251.2111\n",
      "Epoch: 104 loss_train: 252.1066\n",
      "Epoch: 105 loss_train: 250.6590\n",
      "Epoch: 106 loss_train: 249.5984\n",
      "Epoch: 107 loss_train: 250.4050\n",
      "Epoch: 108 loss_train: 249.1093\n",
      "Epoch: 109 loss_train: 248.6762\n",
      "Epoch: 110 loss_train: 247.0494\n",
      "Epoch: 111 loss_train: 246.5229\n",
      "Epoch: 112 loss_train: 245.3313\n",
      "Epoch: 113 loss_train: 245.0100\n",
      "Epoch: 114 loss_train: 245.9896\n",
      "Epoch: 115 loss_train: 244.4958\n",
      "Epoch: 116 loss_train: 243.7393\n",
      "Epoch: 117 loss_train: 243.8496\n",
      "Epoch: 118 loss_train: 243.3004\n",
      "Epoch: 119 loss_train: 242.7150\n",
      "Epoch: 120 loss_train: 243.2069\n",
      "Epoch: 121 loss_train: 241.4692\n",
      "Epoch: 122 loss_train: 241.5961\n",
      "Epoch: 123 loss_train: 242.1793\n",
      "Epoch: 124 loss_train: 240.4049\n",
      "Epoch: 125 loss_train: 240.9286\n",
      "Epoch: 126 loss_train: 240.2322\n",
      "Epoch: 127 loss_train: 240.4221\n",
      "Epoch: 128 loss_train: 238.7257\n",
      "Epoch: 129 loss_train: 239.1847\n",
      "Epoch: 130 loss_train: 237.4979\n",
      "Epoch: 131 loss_train: 236.9323\n",
      "Epoch: 132 loss_train: 237.0605\n",
      "Epoch: 133 loss_train: 235.4755\n",
      "Epoch: 134 loss_train: 236.0255\n",
      "Epoch: 135 loss_train: 234.7684\n",
      "Epoch: 136 loss_train: 234.6716\n",
      "Epoch: 137 loss_train: 234.2086\n",
      "Epoch: 138 loss_train: 233.6750\n",
      "Epoch: 139 loss_train: 232.8966\n",
      "Epoch: 140 loss_train: 231.1353\n",
      "Epoch: 141 loss_train: 231.6950\n",
      "Epoch: 142 loss_train: 229.6794\n",
      "Epoch: 143 loss_train: 228.1725\n",
      "Epoch: 144 loss_train: 228.0346\n",
      "Epoch: 145 loss_train: 226.4281\n",
      "Epoch: 146 loss_train: 226.5629\n",
      "Epoch: 147 loss_train: 225.0547\n",
      "Epoch: 148 loss_train: 223.7234\n",
      "Epoch: 149 loss_train: 222.3172\n",
      "Epoch: 150 loss_train: 222.3391\n",
      "Epoch: 151 loss_train: 220.5714\n",
      "Epoch: 152 loss_train: 218.6225\n",
      "Epoch: 153 loss_train: 218.1362\n",
      "Epoch: 154 loss_train: 216.3311\n",
      "Epoch: 155 loss_train: 215.4442\n",
      "Epoch: 156 loss_train: 213.7193\n",
      "Epoch: 157 loss_train: 213.0561\n",
      "Epoch: 158 loss_train: 209.6196\n",
      "Epoch: 159 loss_train: 8925.2373\n",
      "Epoch: 160 loss_train: 211.9498\n",
      "Epoch: 161 loss_train: 217.0647\n",
      "Epoch: 162 loss_train: 222.3956\n",
      "Epoch: 163 loss_train: 227.1257\n",
      "Epoch: 164 loss_train: 231.3324\n",
      "Epoch: 165 loss_train: 234.3711\n",
      "Epoch: 166 loss_train: 236.6044\n",
      "Epoch: 167 loss_train: 238.8232\n",
      "Epoch: 168 loss_train: 240.4818\n",
      "Epoch: 169 loss_train: 242.7938\n",
      "Epoch: 170 loss_train: 243.6197\n",
      "Epoch: 171 loss_train: 245.1112\n",
      "Epoch: 172 loss_train: 246.1806\n",
      "Epoch: 173 loss_train: 247.7786\n",
      "Epoch: 174 loss_train: 247.6538\n",
      "Epoch: 175 loss_train: 248.4075\n",
      "Epoch: 176 loss_train: 249.6433\n",
      "Epoch: 177 loss_train: 250.2978\n",
      "Epoch: 178 loss_train: 251.7199\n",
      "Epoch: 179 loss_train: 251.6744\n",
      "Epoch: 180 loss_train: 251.7773\n",
      "Epoch: 181 loss_train: 251.7967\n",
      "Epoch: 182 loss_train: 254.2331\n",
      "Epoch: 183 loss_train: 254.0224\n",
      "Epoch: 184 loss_train: 253.8512\n",
      "Epoch: 185 loss_train: 254.5992\n",
      "Epoch: 186 loss_train: 254.4957\n",
      "Epoch: 187 loss_train: 255.2285\n",
      "Epoch: 188 loss_train: 255.7271\n",
      "Epoch: 189 loss_train: 255.1732\n",
      "Epoch: 190 loss_train: 255.2703\n",
      "Epoch: 191 loss_train: 255.6363\n",
      "Epoch: 192 loss_train: 255.3844\n",
      "Epoch: 193 loss_train: 255.8033\n",
      "Epoch: 194 loss_train: 254.9518\n",
      "Epoch: 195 loss_train: 255.2749\n",
      "Epoch: 196 loss_train: 255.1332\n",
      "Epoch: 197 loss_train: 254.6216\n",
      "Epoch: 198 loss_train: 254.9496\n",
      "Epoch: 199 loss_train: 255.3070\n",
      "Epoch: 200 loss_train: 255.0203\n",
      "Epoch: 001 loss_train: 254.3434\n",
      "Epoch: 002 loss_train: 598.6369\n",
      "Epoch: 003 loss_train: 261.3264\n",
      "Epoch: 004 loss_train: 296.1697\n",
      "Epoch: 005 loss_train: 412.6790\n",
      "Epoch: 006 loss_train: 383.7733\n",
      "Epoch: 007 loss_train: 315.2337\n",
      "Epoch: 008 loss_train: 259.3921\n",
      "Epoch: 009 loss_train: 234.4051\n",
      "Epoch: 010 loss_train: 238.1823\n",
      "Epoch: 011 loss_train: 263.2251\n",
      "Epoch: 012 loss_train: 253.5413\n",
      "Epoch: 013 loss_train: 343.7458\n",
      "Epoch: 014 loss_train: 263.4921\n",
      "Epoch: 015 loss_train: 247.6035\n",
      "Epoch: 016 loss_train: 239.7726\n",
      "Epoch: 017 loss_train: 235.2453\n",
      "Epoch: 018 loss_train: 231.7547\n",
      "Epoch: 019 loss_train: 230.3086\n",
      "Epoch: 020 loss_train: 228.4658\n",
      "Epoch: 021 loss_train: 227.7371\n",
      "Epoch: 022 loss_train: 225.1056\n",
      "Epoch: 023 loss_train: 221.1268\n",
      "Epoch: 024 loss_train: 221.0437\n",
      "Epoch: 025 loss_train: 216.5338\n",
      "Epoch: 026 loss_train: 215.7046\n",
      "Epoch: 027 loss_train: 212.7254\n",
      "Epoch: 028 loss_train: 210.4457\n",
      "Epoch: 029 loss_train: 209.3238\n",
      "Epoch: 030 loss_train: 206.1039\n",
      "Epoch: 031 loss_train: 198.1845\n",
      "Epoch: 032 loss_train: 191.8000\n",
      "Epoch: 033 loss_train: 190.7592\n",
      "Epoch: 034 loss_train: 195.2104\n",
      "Epoch: 035 loss_train: 193.0937\n",
      "Epoch: 036 loss_train: 186.9981\n",
      "Epoch: 037 loss_train: 184.4086\n",
      "Epoch: 038 loss_train: 185.4630\n",
      "Epoch: 039 loss_train: 186.5582\n",
      "Epoch: 040 loss_train: 186.4952\n",
      "Epoch: 041 loss_train: 184.7916\n",
      "Epoch: 042 loss_train: 181.7066\n",
      "Epoch: 043 loss_train: 179.3642\n",
      "Epoch: 044 loss_train: 178.0800\n",
      "Epoch: 045 loss_train: 179.1143\n",
      "Epoch: 046 loss_train: 179.2286\n",
      "Epoch: 047 loss_train: 177.9529\n",
      "Epoch: 048 loss_train: 175.7966\n",
      "Epoch: 049 loss_train: 174.5074\n",
      "Epoch: 050 loss_train: 173.8926\n",
      "Epoch: 051 loss_train: 173.7958\n",
      "Epoch: 052 loss_train: 180.8912\n",
      "Epoch: 053 loss_train: 172.2090\n",
      "Epoch: 054 loss_train: 171.9238\n",
      "Epoch: 055 loss_train: 170.8024\n",
      "Epoch: 056 loss_train: 168.7032\n",
      "Epoch: 057 loss_train: 167.3565\n",
      "Epoch: 058 loss_train: 166.7401\n",
      "Epoch: 059 loss_train: 166.3172\n",
      "Epoch: 060 loss_train: 165.1915\n",
      "Epoch: 061 loss_train: 164.1581\n",
      "Epoch: 062 loss_train: 162.6558\n",
      "Epoch: 063 loss_train: 160.9780\n",
      "Epoch: 064 loss_train: 563.9720\n",
      "Epoch: 065 loss_train: 166.4744\n",
      "Epoch: 066 loss_train: 174.4370\n",
      "Epoch: 067 loss_train: 172.6328\n",
      "Epoch: 068 loss_train: 176.3685\n",
      "Epoch: 069 loss_train: 180.6778\n",
      "Epoch: 070 loss_train: 172.1184\n",
      "Epoch: 071 loss_train: 177.5606\n",
      "Epoch: 072 loss_train: 174.1621\n",
      "Epoch: 073 loss_train: 173.5315\n",
      "Epoch: 074 loss_train: 173.5338\n",
      "Epoch: 075 loss_train: 167.0827\n",
      "Epoch: 076 loss_train: 175.2207\n",
      "Epoch: 077 loss_train: 165.2393\n",
      "Epoch: 078 loss_train: 171.4027\n",
      "Epoch: 079 loss_train: 163.4194\n",
      "Epoch: 080 loss_train: 164.8751\n",
      "Epoch: 081 loss_train: 35626.7930\n",
      "Epoch: 082 loss_train: 250.8796\n",
      "Epoch: 083 loss_train: 334.5040\n",
      "Epoch: 084 loss_train: 454.2503\n",
      "Epoch: 085 loss_train: 575.5411\n",
      "Epoch: 086 loss_train: 529.8790\n",
      "Epoch: 087 loss_train: 419.9512\n",
      "Epoch: 088 loss_train: 364.5036\n",
      "Epoch: 089 loss_train: 318.3698\n",
      "Epoch: 090 loss_train: 270.7468\n",
      "Epoch: 091 loss_train: 243.7100\n",
      "Epoch: 092 loss_train: 220.2007\n",
      "Epoch: 093 loss_train: 202.4222\n",
      "Epoch: 094 loss_train: 192.7876\n",
      "Epoch: 095 loss_train: 186.9121\n",
      "Epoch: 096 loss_train: 181.8285\n",
      "Epoch: 097 loss_train: 179.8973\n",
      "Epoch: 098 loss_train: 178.0911\n",
      "Epoch: 099 loss_train: 177.0110\n",
      "Epoch: 100 loss_train: 176.2633\n",
      "Epoch: 101 loss_train: 175.7672\n",
      "Epoch: 102 loss_train: 175.7878\n",
      "Epoch: 103 loss_train: 175.4141\n",
      "Epoch: 104 loss_train: 174.9712\n",
      "Epoch: 105 loss_train: 174.7124\n",
      "Epoch: 106 loss_train: 174.4423\n",
      "Epoch: 107 loss_train: 173.9383\n",
      "Epoch: 108 loss_train: 173.6717\n",
      "Epoch: 109 loss_train: 173.2881\n",
      "Epoch: 110 loss_train: 172.7082\n",
      "Epoch: 111 loss_train: 172.0239\n",
      "Epoch: 112 loss_train: 171.6875\n",
      "Epoch: 113 loss_train: 170.6806\n",
      "Epoch: 114 loss_train: 169.2508\n",
      "Epoch: 115 loss_train: 334.1967\n",
      "Epoch: 116 loss_train: 165.4603\n",
      "Epoch: 117 loss_train: 168.7283\n",
      "Epoch: 118 loss_train: 169.2616\n",
      "Epoch: 119 loss_train: 168.9414\n",
      "Epoch: 120 loss_train: 168.8931\n",
      "Epoch: 121 loss_train: 168.6273\n",
      "Epoch: 122 loss_train: 168.6094\n",
      "Epoch: 123 loss_train: 168.4632\n",
      "Epoch: 124 loss_train: 168.2041\n",
      "Epoch: 125 loss_train: 168.1911\n",
      "Epoch: 126 loss_train: 167.5591\n",
      "Epoch: 127 loss_train: 167.5320\n",
      "Epoch: 128 loss_train: 167.2196\n",
      "Epoch: 129 loss_train: 166.9182\n",
      "Epoch: 130 loss_train: 166.6973\n",
      "Epoch: 131 loss_train: 166.4998\n",
      "Epoch: 132 loss_train: 162.5906\n",
      "Epoch: 133 loss_train: 161.8141\n",
      "Epoch: 134 loss_train: 162.0346\n",
      "Epoch: 135 loss_train: 161.4627\n",
      "Epoch: 136 loss_train: 160.5139\n",
      "Epoch: 137 loss_train: 161.5710\n",
      "Epoch: 138 loss_train: 160.0404\n",
      "Epoch: 139 loss_train: 159.3305\n",
      "Epoch: 140 loss_train: 159.8413\n",
      "Epoch: 141 loss_train: 159.6788\n",
      "Epoch: 142 loss_train: 159.2848\n",
      "Epoch: 143 loss_train: 158.4756\n",
      "Epoch: 144 loss_train: 158.9055\n",
      "Epoch: 145 loss_train: 158.9382\n",
      "Epoch: 146 loss_train: 157.8881\n",
      "Epoch: 147 loss_train: 157.2893\n",
      "Epoch: 148 loss_train: 157.4027\n",
      "Epoch: 149 loss_train: 157.4740\n",
      "Epoch: 150 loss_train: 157.8271\n",
      "Epoch: 151 loss_train: 156.7323\n",
      "Epoch: 152 loss_train: 157.1790\n",
      "Epoch: 153 loss_train: 157.0751\n",
      "Epoch: 154 loss_train: 156.4482\n",
      "Epoch: 155 loss_train: 156.5806\n",
      "Epoch: 156 loss_train: 792.1654\n",
      "Epoch: 157 loss_train: 157.3034\n",
      "Epoch: 158 loss_train: 158.5655\n",
      "Epoch: 159 loss_train: 158.9170\n",
      "Epoch: 160 loss_train: 158.5549\n",
      "Epoch: 161 loss_train: 158.4915\n",
      "Epoch: 162 loss_train: 158.2238\n",
      "Epoch: 163 loss_train: 157.9304\n",
      "Epoch: 164 loss_train: 157.9529\n",
      "Epoch: 165 loss_train: 157.8541\n",
      "Epoch: 166 loss_train: 157.9162\n",
      "Epoch: 167 loss_train: 158.1275\n",
      "Epoch: 168 loss_train: 157.8975\n",
      "Epoch: 169 loss_train: 158.2291\n",
      "Epoch: 170 loss_train: 157.4528\n",
      "Epoch: 171 loss_train: 157.2207\n",
      "Epoch: 172 loss_train: 156.9280\n",
      "Epoch: 173 loss_train: 157.2781\n",
      "Epoch: 174 loss_train: 157.3226\n",
      "Epoch: 175 loss_train: 156.6277\n",
      "Epoch: 176 loss_train: 156.7359\n",
      "Epoch: 177 loss_train: 157.5012\n",
      "Epoch: 178 loss_train: 156.7968\n",
      "Epoch: 179 loss_train: 156.9869\n",
      "Epoch: 180 loss_train: 156.9190\n",
      "Epoch: 181 loss_train: 156.8435\n",
      "Epoch: 182 loss_train: 156.5883\n",
      "Epoch: 183 loss_train: 156.5822\n",
      "Epoch: 184 loss_train: 155.8643\n",
      "Epoch: 185 loss_train: 155.9335\n",
      "Epoch: 186 loss_train: 156.0113\n",
      "Epoch: 187 loss_train: 155.9600\n",
      "Epoch: 188 loss_train: 155.7008\n",
      "Epoch: 189 loss_train: 155.3319\n",
      "Epoch: 190 loss_train: 155.0841\n",
      "Epoch: 191 loss_train: 154.7591\n",
      "Epoch: 192 loss_train: 154.7936\n",
      "Epoch: 193 loss_train: 154.9689\n",
      "Epoch: 194 loss_train: 154.6754\n",
      "Epoch: 195 loss_train: 154.0341\n",
      "Epoch: 196 loss_train: 154.5003\n",
      "Epoch: 197 loss_train: 154.0618\n",
      "Epoch: 198 loss_train: 153.7244\n",
      "Epoch: 199 loss_train: 153.4439\n",
      "Epoch: 200 loss_train: 153.7573\n",
      "Epoch: 001 loss_train: 153.7061\n",
      "Epoch: 002 loss_train: 152.3352\n",
      "Epoch: 003 loss_train: 144.5744\n",
      "Epoch: 004 loss_train: 139.7519\n",
      "Epoch: 005 loss_train: 140.3779\n",
      "Epoch: 006 loss_train: 137.9191\n",
      "Epoch: 007 loss_train: 137.2191\n",
      "Epoch: 008 loss_train: 137.0118\n",
      "Epoch: 009 loss_train: 137.3869\n",
      "Epoch: 010 loss_train: 137.0289\n",
      "Epoch: 011 loss_train: 136.5659\n",
      "Epoch: 012 loss_train: 135.6976\n",
      "Epoch: 013 loss_train: 134.5969\n",
      "Epoch: 014 loss_train: 134.0408\n",
      "Epoch: 015 loss_train: 133.0312\n",
      "Epoch: 016 loss_train: 132.9359\n",
      "Epoch: 017 loss_train: 131.7815\n",
      "Epoch: 018 loss_train: 133.3142\n",
      "Epoch: 019 loss_train: 129.2412\n",
      "Epoch: 020 loss_train: 130.6854\n",
      "Epoch: 021 loss_train: 131.1343\n",
      "Epoch: 022 loss_train: 130.3879\n",
      "Epoch: 023 loss_train: 128.9914\n",
      "Epoch: 024 loss_train: 128.8970\n",
      "Epoch: 025 loss_train: 130.2522\n",
      "Epoch: 026 loss_train: 427.7804\n",
      "Epoch: 027 loss_train: 129.9419\n",
      "Epoch: 028 loss_train: 130.1329\n",
      "Epoch: 029 loss_train: 129.5687\n",
      "Epoch: 030 loss_train: 130.7052\n",
      "Epoch: 031 loss_train: 131.2513\n",
      "Epoch: 032 loss_train: 131.8860\n",
      "Epoch: 033 loss_train: 131.1081\n",
      "Epoch: 034 loss_train: 131.5013\n",
      "Epoch: 035 loss_train: 131.1568\n",
      "Epoch: 036 loss_train: 131.6556\n",
      "Epoch: 037 loss_train: 131.1830\n",
      "Epoch: 038 loss_train: 131.5547\n",
      "Epoch: 039 loss_train: 130.9251\n",
      "Epoch: 040 loss_train: 131.5517\n",
      "Epoch: 041 loss_train: 130.5469\n",
      "Epoch: 042 loss_train: 130.1658\n",
      "Epoch: 043 loss_train: 129.7423\n",
      "Epoch: 044 loss_train: 130.5100\n",
      "Epoch: 045 loss_train: 130.1144\n",
      "Epoch: 046 loss_train: 129.7811\n",
      "Epoch: 047 loss_train: 130.0992\n",
      "Epoch: 048 loss_train: 130.6290\n",
      "Epoch: 049 loss_train: 130.8883\n",
      "Epoch: 050 loss_train: 129.7112\n",
      "Epoch: 051 loss_train: 128.7643\n",
      "Epoch: 052 loss_train: 130.1507\n",
      "Epoch: 053 loss_train: 129.9459\n",
      "Epoch: 054 loss_train: 129.2892\n",
      "Epoch: 055 loss_train: 129.3601\n",
      "Epoch: 056 loss_train: 129.8300\n",
      "Epoch: 057 loss_train: 129.3779\n",
      "Epoch: 058 loss_train: 128.8770\n",
      "Epoch: 059 loss_train: 129.3500\n",
      "Epoch: 060 loss_train: 129.0482\n",
      "Epoch: 061 loss_train: 128.4638\n",
      "Epoch: 062 loss_train: 129.0788\n",
      "Epoch: 063 loss_train: 128.5458\n",
      "Epoch: 064 loss_train: 128.7274\n",
      "Epoch: 065 loss_train: 128.6121\n",
      "Epoch: 066 loss_train: 128.3668\n",
      "Epoch: 067 loss_train: 128.4348\n",
      "Epoch: 068 loss_train: 128.4272\n",
      "Epoch: 069 loss_train: 128.1376\n",
      "Epoch: 070 loss_train: 128.5218\n",
      "Epoch: 071 loss_train: 128.1065\n",
      "Epoch: 072 loss_train: 128.7530\n",
      "Epoch: 073 loss_train: 128.1102\n",
      "Epoch: 074 loss_train: 128.5326\n",
      "Epoch: 075 loss_train: 127.8438\n",
      "Epoch: 076 loss_train: 128.1864\n",
      "Epoch: 077 loss_train: 127.9138\n",
      "Epoch: 078 loss_train: 128.4016\n",
      "Epoch: 079 loss_train: 128.1157\n",
      "Epoch: 080 loss_train: 127.3923\n",
      "Epoch: 081 loss_train: 127.3099\n",
      "Epoch: 082 loss_train: 128.1651\n",
      "Epoch: 083 loss_train: 127.9036\n",
      "Epoch: 084 loss_train: 127.8688\n",
      "Epoch: 085 loss_train: 127.8871\n",
      "Epoch: 086 loss_train: 127.1673\n",
      "Epoch: 087 loss_train: 127.8997\n",
      "Epoch: 088 loss_train: 127.7536\n",
      "Epoch: 089 loss_train: 127.4811\n",
      "Epoch: 090 loss_train: 127.4282\n",
      "Epoch: 091 loss_train: 127.3643\n",
      "Epoch: 092 loss_train: 127.4137\n",
      "Epoch: 093 loss_train: 127.0612\n",
      "Epoch: 094 loss_train: 127.0923\n",
      "Epoch: 095 loss_train: 127.4171\n",
      "Epoch: 096 loss_train: 127.0713\n",
      "Epoch: 097 loss_train: 127.1611\n",
      "Epoch: 098 loss_train: 127.4734\n",
      "Epoch: 099 loss_train: 127.4557\n",
      "Epoch: 100 loss_train: 127.1093\n",
      "Epoch: 101 loss_train: 126.8497\n",
      "Epoch: 102 loss_train: 126.7522\n",
      "Epoch: 103 loss_train: 127.0795\n",
      "Epoch: 104 loss_train: 126.7871\n",
      "Epoch: 105 loss_train: 126.4851\n",
      "Epoch: 106 loss_train: 126.7465\n",
      "Epoch: 107 loss_train: 126.7368\n",
      "Epoch: 108 loss_train: 126.3781\n",
      "Epoch: 109 loss_train: 126.3460\n",
      "Epoch: 110 loss_train: 126.9321\n",
      "Epoch: 111 loss_train: 126.4543\n",
      "Epoch: 112 loss_train: 126.2311\n",
      "Epoch: 113 loss_train: 125.8698\n",
      "Epoch: 114 loss_train: 125.7791\n",
      "Epoch: 115 loss_train: 126.4411\n",
      "Epoch: 116 loss_train: 125.8060\n",
      "Epoch: 117 loss_train: 126.3070\n",
      "Epoch: 118 loss_train: 125.9589\n",
      "Epoch: 119 loss_train: 126.0970\n",
      "Epoch: 120 loss_train: 125.7222\n",
      "Epoch: 121 loss_train: 125.8324\n",
      "Epoch: 122 loss_train: 125.5353\n",
      "Epoch: 123 loss_train: 125.4900\n",
      "Epoch: 124 loss_train: 125.6814\n",
      "Epoch: 125 loss_train: 125.6846\n",
      "Epoch: 126 loss_train: 125.4119\n",
      "Epoch: 127 loss_train: 125.2033\n",
      "Epoch: 128 loss_train: 125.2272\n",
      "Epoch: 129 loss_train: 125.2669\n",
      "Epoch: 130 loss_train: 124.7132\n",
      "Epoch: 131 loss_train: 124.8487\n",
      "Epoch: 132 loss_train: 124.2093\n",
      "Epoch: 133 loss_train: 124.6731\n",
      "Epoch: 134 loss_train: 124.5023\n",
      "Epoch: 135 loss_train: 124.8186\n",
      "Epoch: 136 loss_train: 124.6535\n",
      "Epoch: 137 loss_train: 124.0736\n",
      "Epoch: 138 loss_train: 124.8171\n",
      "Epoch: 139 loss_train: 124.1633\n",
      "Epoch: 140 loss_train: 124.2749\n",
      "Epoch: 141 loss_train: 124.6364\n",
      "Epoch: 142 loss_train: 123.7620\n",
      "Epoch: 143 loss_train: 124.0191\n",
      "Epoch: 144 loss_train: 123.3634\n",
      "Epoch: 145 loss_train: 122.9205\n",
      "Epoch: 146 loss_train: 123.7848\n",
      "Epoch: 147 loss_train: 124.0135\n",
      "Epoch: 148 loss_train: 123.1118\n",
      "Epoch: 149 loss_train: 123.5654\n",
      "Epoch: 150 loss_train: 123.0863\n",
      "Epoch: 151 loss_train: 122.8485\n",
      "Epoch: 152 loss_train: 123.5363\n",
      "Epoch: 153 loss_train: 123.2752\n",
      "Epoch: 154 loss_train: 123.0653\n",
      "Epoch: 155 loss_train: 122.4660\n",
      "Epoch: 156 loss_train: 122.7874\n",
      "Epoch: 157 loss_train: 122.0239\n",
      "Epoch: 158 loss_train: 121.9994\n",
      "Epoch: 159 loss_train: 121.3408\n",
      "Epoch: 160 loss_train: 122.2975\n",
      "Epoch: 161 loss_train: 122.2031\n",
      "Epoch: 162 loss_train: 121.5996\n",
      "Epoch: 163 loss_train: 121.4015\n",
      "Epoch: 164 loss_train: 121.5632\n",
      "Epoch: 165 loss_train: 121.2945\n",
      "Epoch: 166 loss_train: 121.5345\n",
      "Epoch: 167 loss_train: 120.8149\n",
      "Epoch: 168 loss_train: 121.1031\n",
      "Epoch: 169 loss_train: 120.6029\n",
      "Epoch: 170 loss_train: 120.6356\n",
      "Epoch: 171 loss_train: 119.9113\n",
      "Epoch: 172 loss_train: 120.7411\n",
      "Epoch: 173 loss_train: 120.2262\n",
      "Epoch: 174 loss_train: 120.1194\n",
      "Epoch: 175 loss_train: 120.2838\n",
      "Epoch: 176 loss_train: 119.8251\n",
      "Epoch: 177 loss_train: 119.6735\n",
      "Epoch: 178 loss_train: 119.4377\n",
      "Epoch: 179 loss_train: 119.9984\n",
      "Epoch: 180 loss_train: 119.8059\n",
      "Epoch: 181 loss_train: 118.2034\n",
      "Epoch: 182 loss_train: 119.2497\n",
      "Epoch: 183 loss_train: 119.1119\n",
      "Epoch: 184 loss_train: 118.4343\n",
      "Epoch: 185 loss_train: 118.3811\n",
      "Epoch: 186 loss_train: 118.1640\n",
      "Epoch: 187 loss_train: 118.3751\n",
      "Epoch: 188 loss_train: 117.6831\n",
      "Epoch: 189 loss_train: 118.1574\n",
      "Epoch: 190 loss_train: 117.6353\n",
      "Epoch: 191 loss_train: 117.4092\n",
      "Epoch: 192 loss_train: 117.5069\n",
      "Epoch: 193 loss_train: 117.4185\n",
      "Epoch: 194 loss_train: 116.8450\n",
      "Epoch: 195 loss_train: 116.4876\n",
      "Epoch: 196 loss_train: 116.1257\n",
      "Epoch: 197 loss_train: 116.5862\n",
      "Epoch: 198 loss_train: 116.1254\n",
      "Epoch: 199 loss_train: 116.0457\n",
      "Epoch: 200 loss_train: 116.1210\n",
      "Epoch: 001 loss_train: 115.9112\n",
      "Epoch: 002 loss_train: 116.5121\n",
      "Epoch: 003 loss_train: 115.5984\n",
      "Epoch: 004 loss_train: 115.7433\n",
      "Epoch: 005 loss_train: 116.2713\n",
      "Epoch: 006 loss_train: 115.7584\n",
      "Epoch: 007 loss_train: 114.8480\n",
      "Epoch: 008 loss_train: 115.3066\n",
      "Epoch: 009 loss_train: 115.0003\n",
      "Epoch: 010 loss_train: 115.8879\n",
      "Epoch: 011 loss_train: 115.6808\n",
      "Epoch: 012 loss_train: 114.8869\n",
      "Epoch: 013 loss_train: 115.2499\n",
      "Epoch: 014 loss_train: 115.6179\n",
      "Epoch: 015 loss_train: 114.8902\n",
      "Epoch: 016 loss_train: 114.7776\n",
      "Epoch: 017 loss_train: 115.2005\n",
      "Epoch: 018 loss_train: 114.8981\n",
      "Epoch: 019 loss_train: 114.8941\n",
      "Epoch: 020 loss_train: 114.8276\n",
      "Epoch: 021 loss_train: 114.1012\n",
      "Epoch: 022 loss_train: 114.5120\n",
      "Epoch: 023 loss_train: 114.2262\n",
      "Epoch: 024 loss_train: 113.6567\n",
      "Epoch: 025 loss_train: 115.0939\n",
      "Epoch: 026 loss_train: 114.5074\n",
      "Epoch: 027 loss_train: 114.7489\n",
      "Epoch: 028 loss_train: 113.7962\n",
      "Epoch: 029 loss_train: 114.6465\n",
      "Epoch: 030 loss_train: 113.7368\n",
      "Epoch: 031 loss_train: 148.4243\n",
      "Epoch: 032 loss_train: 114.1498\n",
      "Epoch: 033 loss_train: 114.1431\n",
      "Epoch: 034 loss_train: 113.5242\n",
      "Epoch: 035 loss_train: 113.0653\n",
      "Epoch: 036 loss_train: 113.6210\n",
      "Epoch: 037 loss_train: 114.1429\n",
      "Epoch: 038 loss_train: 113.3303\n",
      "Epoch: 039 loss_train: 113.7570\n",
      "Epoch: 040 loss_train: 114.0448\n",
      "Epoch: 041 loss_train: 114.0784\n",
      "Epoch: 042 loss_train: 114.0664\n",
      "Epoch: 043 loss_train: 114.1439\n",
      "Epoch: 044 loss_train: 113.7639\n",
      "Epoch: 045 loss_train: 113.8507\n",
      "Epoch: 046 loss_train: 113.2147\n",
      "Epoch: 047 loss_train: 113.9710\n",
      "Epoch: 048 loss_train: 113.6339\n",
      "Epoch: 049 loss_train: 112.6282\n",
      "Epoch: 050 loss_train: 113.6192\n",
      "Epoch: 051 loss_train: 113.5218\n",
      "Epoch: 052 loss_train: 112.7901\n",
      "Epoch: 053 loss_train: 113.1566\n",
      "Epoch: 054 loss_train: 113.6969\n",
      "Epoch: 055 loss_train: 112.9357\n",
      "Epoch: 056 loss_train: 113.1653\n",
      "Epoch: 057 loss_train: 112.7895\n",
      "Epoch: 058 loss_train: 113.3857\n",
      "Epoch: 059 loss_train: 113.1313\n",
      "Epoch: 060 loss_train: 113.0587\n",
      "Epoch: 061 loss_train: 112.9259\n",
      "Epoch: 062 loss_train: 112.8694\n",
      "Epoch: 063 loss_train: 113.1473\n",
      "Epoch: 064 loss_train: 112.6990\n",
      "Epoch: 065 loss_train: 112.2543\n",
      "Epoch: 066 loss_train: 113.3824\n",
      "Epoch: 067 loss_train: 112.7977\n",
      "Epoch: 068 loss_train: 113.2612\n",
      "Epoch: 069 loss_train: 112.2507\n",
      "Epoch: 070 loss_train: 112.7971\n",
      "Epoch: 071 loss_train: 113.2979\n",
      "Epoch: 072 loss_train: 112.7849\n",
      "Epoch: 073 loss_train: 113.1020\n",
      "Epoch: 074 loss_train: 112.8706\n",
      "Epoch: 075 loss_train: 112.2897\n",
      "Epoch: 076 loss_train: 112.9801\n",
      "Epoch: 077 loss_train: 112.7331\n",
      "Epoch: 078 loss_train: 112.3895\n",
      "Epoch: 079 loss_train: 112.3716\n",
      "Epoch: 080 loss_train: 112.5311\n",
      "Epoch: 081 loss_train: 112.3280\n",
      "Epoch: 082 loss_train: 112.3981\n",
      "Epoch: 083 loss_train: 112.7579\n",
      "Epoch: 084 loss_train: 112.6580\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-73477b2bc79c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel_glob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_glob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindex_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss_trains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/INF554/myEnv/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-85b1d758e613>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mz0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mz0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/INF554/myEnv/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/INF554/myEnv/lib64/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/INF554/myEnv/lib64/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_dim=X_train_x.shape[1]\n",
    "model_glob=MLP(n_dim,256,64,0.3)\n",
    "\n",
    "loss_trains_glob=[]\n",
    "loss = nn.MSELoss()\n",
    "lr=1e-2\n",
    "for i in range(5):\n",
    "    lr/=2\n",
    "    optimizer = optim.Adam(model_glob.parameters(), lr=lr)\n",
    "    for epoch in range(200):\n",
    "        model_glob.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glob(X_train_glob)\n",
    "        loss_train_glob = loss(output.reshape(-1), hindex_train_glob)\n",
    "        loss_trains_glob.append(loss_train_glob.item())\n",
    "        loss_train_glob.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "                'loss_train: {:.4f}'.format(loss_train_glob.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_trains_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_glob.state_dict(), \"Global/full_train_model_1.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "571ac4587ae4eeb6b02353bd76aeaaf0ceca15cf49d684242a7eb1fb5d42efb7"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('myEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
